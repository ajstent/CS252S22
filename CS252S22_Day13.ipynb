{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e00dba",
   "metadata": {},
   "source": [
    "# Normal equation\n",
    "\n",
    "So far, we have used the least squares method to *fit* functions for linear regression. \n",
    "\n",
    "If we have only two data points, we can __always__ fit a line that passes through both points:\n",
    "$$\\begin{bmatrix}1 & x_1 \\\\ 1 & x_2 \\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}c_0 \\\\ c_1\\end{bmatrix} = \\begin{bmatrix}y_1 \\\\ y_2\\end{bmatrix}$$\n",
    "\n",
    "If we have three (or more) points, we may not be able to fit a line through all three, because we have two unknowns but three equations:\n",
    "$$\\begin{bmatrix}1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}c_0 \\\\ c_1\\end{bmatrix} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3\\end{bmatrix}$$\n",
    "\n",
    "You will often see this written like:\n",
    "$$ A \\cdot \\vec{c} = \\vec{y}$$\n",
    "and the residual error $$\\vec{r} = \\vec{y} - A \\cdot \\vec{c}$$\n",
    "\n",
    "If we use least squares to solve for $\\vec{c}$, we minimize $||\\vec{r}||_2^2$ (the MSSE):\n",
    "$$||\\vec{r}||_2^2 = \\vec{r}^T\\vec{r} \\\\= (\\vec{y}-A\\vec{c})^T(\\vec{y}-A\\vec{c}) \\\\= \\vec{y}^T\\vec{y}-\\vec{y}^TA\\vec{c}-(A\\vec{c})^T\\vec{y} + (A\\vec{c})^TA\\vec{c}$$\n",
    "\n",
    "Now, $(A\\vec{c})^T = \\vec{c}^TA^T$, so\n",
    "$$ = \\vec{y}^T\\vec{y}-\\vec{y}^TA\\vec{c}-\\vec{c}^TA^T\\vec{y} + \\vec{c}^TA^TA\\vec{c} \\\\ = \\vec{y}^T\\vec{y}-2\\vec{y}^TA\\vec{c} + \\vec{c}^TA^TA\\vec{c}$$ (because $\\vec{c}^TA^T\\vec{y}$ is the same as $\\vec{y}^TA\\vec{c}$)\n",
    "\n",
    "Then we take the partial derivative with respect to $\\vec{c}$, $\\frac{\\partial}{\\partial \\vec{c}}$:\n",
    "\n",
    "$$\\vec{y}^T\\vec{y} = 2\\vec{y}^TA\\vec{c} + \\vec{c}^TA^TA\\vec{c} = 0 $$ Now, $$\\frac{\\partial}{\\partial \\vec{c}} (\\vec{y}^T\\vec{y}) = 0$$ $$\\frac{\\partial}{\\partial \\vec{c}} (2\\vec{y}^TA\\vec{c}) = 2\\vec{y}^TA = 2A^T\\vec{y}$$ $$\\frac{\\partial}{\\partial \\vec{c}} (\\vec{c}^TA^TA\\vec{c}) = 2A^TA\\vec{c} $$ so we have $$ -2\\vec{y}^TA + 2A^TA\\vec{c} = 0 \\\\ \\vec{y}^TA = A^TA\\vec{c} \\\\ A^T\\vec{y} = A^TA\\vec{c} \\\\ \\vec{c} = (A^TA)^{-1} A^T\\vec{y} $$\n",
    "\n",
    "We call $ \\vec{c} = (A^TA)^{-1} A^T\\vec{y}$ the __normal equation__.\n",
    "\n",
    "$(A^TA)^{-1}$ means to invert $A^TA$. The inverse of a matrix is another matrix which, if multiplied by the original matrix, gives you the identity matrix. \n",
    "\n",
    "Note that in some cases, $A^TA$ may not be *invertible*. Don't worry, we have other ways to fit linear regression! But if you are committed to normal equation and run into this situation, you probably have some features (variables) that are not independent of each other and you can remove them.\n",
    "\n",
    "Also note that calculating $A^TA$ is computationally expensive. Don't worry, we have other ways to fit linear regression!\n",
    "\n",
    "In the case of multiple linear regression, the solution to $ \\vec{c} = (A^TA)^{-1} A^T\\vec{y} $ is not a line, it's a plane.\n",
    "\n",
    "# Resources\n",
    "\n",
    "* http://mlwiki.org/index.php/Normal_Equation\n",
    "* https://mathworld.wolfram.com/MatrixInverse.html\n",
    "\n",
    "# Question for the class\n",
    "\n",
    "How would we write the normal equation using numpy (or numpy and scipy)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa434d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
