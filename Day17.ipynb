{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e185107",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.linalg as sp_la"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "205d74de",
   "metadata": {},
   "source": [
    "# Covariance\n",
    "\n",
    "As we know from the discussion on Wednesday, independent variables can be correlated not only with the dependent variable but also *with each other*. The degree of correlation tells us how much and in which direction they vary. Formally, the correlation of two variables $x$ and $y$ is $\\frac{(x-\\mu_x)(y-\\mu_y)}{\\sigma_x\\sigma_y}$.\n",
    "\n",
    "We can also talk about a related concept, *covariance*. The covariance of two variables $x$ and $y$ is $(x-\\mu_x)(y-\\mu_y)$.\n",
    "* The covariance of a $x$ with itself is just the variance of $x$. This is in contrast to the correlation of $x$ with itself, which is always 1.\n",
    "\n",
    "The *covariance matrix* of a data set tells us about the first order covariance relationships between all pairs of variables. The covariance matrix has the variance of each variable along its diagonal, and the remaining entries are the *covariances* of pairs of variables, ie how much they vary together. If they vary together, then they are related to each other - some information is shared between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215a517",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* If the covariance is close to 0, then what is true of the pair of features?\n",
    "* If the covariance is big, then what is true of the pair of features?\n",
    "* Correlations can be positive or negative; what about covariances?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3e5d95e",
   "metadata": {},
   "source": [
    "We can calculate covariance using matrix multiplication:\n",
    "* First, center the data: $A_c = A - \\mu_A$\n",
    "* Then, calculate $C$: $C = \\frac{1}{N-1} A_c^TA_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69849196",
   "metadata": {},
   "source": [
    "Let's look at some toy examples (h/t Stephanie Taylor!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc240a5",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "These two variables co-vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46693777",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3,0], [1.1, 2.1, 3, 0.5]]).T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, center the data\n",
    "def centerData(data):\n",
    "    return data - data.mean(axis=0)\n",
    "\n",
    "# Then, calculate C\n",
    "def covarMatrix(data):\n",
    "    centered = centerData(data)\n",
    "    return (centered.T@centered) / (len(centered)-1) # or centered.shape[0] - 1\n",
    "\n",
    "print(covarMatrix(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92da1cd2",
   "metadata": {},
   "source": [
    "*What do we observe about the variance of X? of Y? What about the covariance?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b6b48f7",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "This is just like before by the values of $Y$ are now negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b855c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3,0], [-1.1, -2.1, -3, -0.5]]).T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data and calculate C\n",
    "print(covarMatrix(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d6b2799",
   "metadata": {},
   "source": [
    "*What do we observe about the variance of X? of Y? What about the covariance?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20b33084",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "\n",
    "The values of $X$ and $Y$ are now random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea344ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([np.random.standard_normal(4), np.random.standard_normal(4)]).T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1099060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data and calculate C\n",
    "print(covarMatrix(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4aea489c",
   "metadata": {},
   "source": [
    "*What do we observe about the variance of X? of Y? What about the covariance?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ddd26c5",
   "metadata": {},
   "source": [
    "## On our data\n",
    "\n",
    "### Load the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ea4cde7",
   "metadata": {},
   "source": [
    "Same code as Wednesday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be our columns\n",
    "columns = [\"price\", \"year\", \"manufacturer\", \"model\", \"condition\", \"fuel\", \"odometer\", \"title_status\", \"transmission\"]\n",
    "# this will contain our converters\n",
    "colValues = {}\n",
    "\n",
    "# first we load our data as strings so we can define the converters\n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), skip_header=1, dtype=str, encoding='utf-8'))  \n",
    "\n",
    "# make a list of the unique values in each column of our data\n",
    "for colIndex in range(data.shape[1]):\n",
    "    colValues[colIndex] = np.unique(data[:, colIndex]).tolist()\n",
    "\n",
    "# fix up some of these ones we know are ordered\n",
    "colValues[columns.index('condition')] = ['new', 'like new', 'excellent', 'good', 'fair', 'salvage']\n",
    "colValues[columns.index('title_status')] = ['clean', 'lien', 'rebuilt', 'salvage', 'parts only', 'missing']\n",
    "\n",
    "# map values to their indices in the list of unique values\n",
    "def converter(x, colIndex):\n",
    "    return colValues[colIndex].index(x)\n",
    "\n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), converters={3: lambda x: converter(x, 2), 4: lambda x: converter(x, 3), 5: lambda x: converter(x, 4), 7: lambda x: converter(x,5), 9: lambda x: converter(x, 7), 11: lambda x: converter(x, 8)}, skip_header=1, dtype=int, encoding='utf-8'))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0e6428",
   "metadata": {},
   "source": [
    "### Covariance matrix: raw data\n",
    "\n",
    "Center the data and calculate C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data and calculate C\n",
    "C = covarMatrix(data)\n",
    "\n",
    "# I want to see it better!\n",
    "print(pd.DataFrame(C))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83e5ac95",
   "metadata": {},
   "source": [
    "Well, that table is hard to look at. Let's plot the covariance matrix.\n",
    "\n",
    "In an ideal world, your covariance matrix will have a lot of different colors in it. That's a dataset you can use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8192c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "sns.heatmap(pd.DataFrame(C), annot=False, cmap='PuOr')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51ee8fcf",
   "metadata": {},
   "source": [
    "*What do we observe?*\n",
    "\n",
    "### Covariance matrix: min-max global\n",
    "\n",
    "Now let's try:\n",
    "1. min-max global our data\n",
    "2. center and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec074421",
   "metadata": {},
   "outputs": [],
   "source": [
    "homogenizedData = np.append(data, np.array([np.ones(data.shape[0], dtype=int)]).T, axis=1)\n",
    "\n",
    "# subtract the global minimum from each datapoint\n",
    "translateTransform = np.eye(homogenizedData.shape[1], dtype=float)\n",
    "for i in range(data.shape[1]):\n",
    "    translateTransform[i, -1] = -data.min()\n",
    "\n",
    "# divide by the global range\n",
    "scaleTransform = np.eye(homogenizedData.shape[1])\n",
    "for i in range(data.shape[1]):\n",
    "    scaleTransform[i, i] = 1/(data.max()-data.min())\n",
    "\n",
    "totalTransform = scaleTransform@translateTransform\n",
    "transformedData = (totalTransform@homogenizedData.T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data and calculate C\n",
    "C = covarMatrix(transformedData)\n",
    "\n",
    "# Let's look at the covariance matrix\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "sns.heatmap(pd.DataFrame(C), annot=False, cmap='PuOr')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c58d1088",
   "metadata": {},
   "source": [
    "*What do we observe?*\n",
    "\n",
    "### Covariance matrix: min-max local\n",
    "\n",
    "Now let's try:\n",
    "1. min-max local our data\n",
    "2. center and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translateTransform = np.eye(homogenizedData.shape[1], dtype=float)\n",
    "for i in range(data.shape[1]):\n",
    "    translateTransform[i, -1] = -data[:, i].min()\n",
    "\n",
    "scaleTransform = np.eye(homogenizedData.shape[1], dtype=float)\n",
    "for i in range(data.shape[1]):\n",
    "    scaleTransform[i, i] = 1/(data[:, i].max()-data[:, i].min())\n",
    "\n",
    "totalTransform = scaleTransform@translateTransform\n",
    "\n",
    "transformedData = (totalTransform @ homogenizedData.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data and calculate C\n",
    "C = covarMatrix(transformedData)\n",
    "\n",
    "# Let's look at the covariance matrix\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "sns.heatmap(pd.DataFrame(C), annot=False, cmap='PuOr')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13f06507",
   "metadata": {},
   "source": [
    "*What do we observe?*\n",
    "\n",
    "### Covariance matrix: z-score\n",
    "\n",
    "Now let's try:\n",
    "1. z-score our data (subtract the mean and divide by the standard deviation; this also centers the data)\n",
    "2. (center and) covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translateTransform = np.eye(homogenizedData.shape[1], dtype=float)\n",
    "for i in range(data.shape[1]):\n",
    "    translateTransform[i, -1] = -data[:, i].mean()\n",
    "\n",
    "scaleTransform = np.eye(homogenizedData.shape[1], dtype=float)\n",
    "for i in range(data.shape[1]):\n",
    "    scaleTransform[i, i] = 1/data[:, i].std()\n",
    "\n",
    "totalTransform = scaleTransform@translateTransform\n",
    "\n",
    "transformedData = (totalTransform @ homogenizedData.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data and calculate C\n",
    "C = covarMatrix(transformedData)\n",
    "\n",
    "# Let's look at the covariance matrix\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "sns.heatmap(pd.DataFrame(C), annot=False, cmap='PuOr')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82a1db0e",
   "metadata": {},
   "source": [
    "*What do we observe?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a1a09cd",
   "metadata": {},
   "source": [
    "## Warning\n",
    "\n",
    "This is a thing that will calculate the covariance matrix for you, but you *may not use it* for your project: np.cov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe3d8959",
   "metadata": {},
   "source": [
    "# The curse of dimensionality\n",
    "\n",
    "When data is very multidimensional, everything becomes slower and more complicated:\n",
    "* How can you look at your data? There's too much there!\n",
    "* How can you normalize your data? Without being able to look at it, what normalizations would you know to apply?\n",
    "* How can you fit a regression to your data? Which variables would you choose?\n",
    "\n",
    "In fact, data in a high dimensional space just is different:\n",
    "* there are more extreme values. In a 2 dimensional unit square, the probability that a random point is within 0.001 of the border is 0.004, but in a 10000 dimensional unit hypercube, the probability is > 0.999.\n",
    "* the distances between points are bigger. In a 2 dimensional unit square, the average distance between two points is about 0.5. In a 3 dimensional unit cube, it's 0.66. But in a 10000 dimensional unit hypercube, it's about 408.25.\n",
    "\n",
    "Here's the thing though: generally, only a small fraction of variables in a data set suffice to model the whole. For example, very often many features do covary. So we can use covariance information to *project* our data to a much smaller dimensional space and analyze the data in that space instead. The technique we will use for that is *principal component analysis* (PCA)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
