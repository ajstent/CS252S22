{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e185107",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "## Review\n",
    "\n",
    "1. Find some data!\n",
    "2. Load and look at your data\n",
    "  * thing1\n",
    "  * thing2\n",
    "3. Consider cleaning, transforming and/or normalizing your data\n",
    "  * thing1\n",
    "  * thing2\n",
    "  * thing3\n",
    "  * thing4\n",
    "4. Look at your data some more and consider feature selection, feature engineering, dimensionality reduction\n",
    "  * *correlations*\n",
    "  * *covariance matrix*\n",
    "  * *PCA*\n",
    "5. Model\n",
    "  * thing1 (in three variations!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "219e0a18",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Today we will keep working with the set of Craigslist listings for used cars.\n",
    "\n",
    "Same code as Wednesday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b91651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy.linalg as sp_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be our columns\n",
    "columns = [\"price\", \"year\", \"manufacturer\", \"model\", \"condition\", \"fuel\", \"odometer\", \"title_status\", \"transmission\"]\n",
    "# this will contain our converters\n",
    "colValues = {}\n",
    "\n",
    "# first we load our data as strings so we can define the converters\n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), skip_header=1, dtype=str, encoding='utf-8'))  \n",
    "\n",
    "# make a list of the unique values in each column of our data\n",
    "for colIndex in range(data.shape[1]):\n",
    "    colValues[colIndex] = np.unique(data[:, colIndex]).tolist()\n",
    "    print(colIndex, colValues[colIndex])\n",
    "\n",
    "# fix up some of these ones we know are ordered\n",
    "colValues[columns.index('condition')] = ['new', 'like new', 'excellent', 'good', 'fair', 'salvage']\n",
    "colValues[columns.index('title_status')] = ['clean', 'lien', 'rebuilt', 'salvage', 'parts only', 'missing']\n",
    "\n",
    "# map values to their indices in the list of unique values\n",
    "def converter(x, colIndex):\n",
    "    return colValues[colIndex].index(x)\n",
    "\n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), converters={3: lambda x: converter(x, 2), 4: lambda x: converter(x, 3), 5: lambda x: converter(x, 4), 7: lambda x: converter(x,5), 9: lambda x: converter(x, 7), 11: lambda x: converter(x, 8)}, skip_header=1, dtype=int, encoding='utf-8'))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e373672",
   "metadata": {},
   "source": [
    "Let's calculate *correlations* between price and the other variables. (Remind me what correlation values vary between?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31477b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(columns)):\n",
    "    print(columns[i], np.corrcoef(data[:, 0], data[:, i], rowvar=True)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d74de",
   "metadata": {},
   "source": [
    "# Covariance\n",
    "\n",
    "As we can see from the correlation analysis above, features (variables) in a data set may be related to each other.\n",
    "\n",
    "The *covariance matrix* of a data set tells us about the first order relationships between different features. If $A$ is the matrix corresponding to our data set of $N$ data points for each of which we have $M$ features, and $\\bar{A_i}$ is the mean of the $i$th feature, then covariance matrix $C$ is:\n",
    "    $$C_{i,j} = \\sum_{k=1}^N \\frac{(A_{k,i} - \\bar{A_i})(A_{k,j} - \\bar{A_j})}{N-1}$$\n",
    "    \n",
    "The covariance matrix has the variance of each feature along its diagonal, and the remaining entries are the *covariances* of pairs of features, ie how much they vary together. If they vary together, then they are related to each other - some information is shared between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215a517",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* If the covariance is close to 0, then what is true of the pair of features?\n",
    "* If the covariance is big, then what is true of the pair of features?\n",
    "* Correlations can be positive or negative; what about covariances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5d95e",
   "metadata": {},
   "source": [
    "We can calculate covariance using matrix multiplication:\n",
    "* First, center the data: $A_c = A - \\bar{A}$\n",
    "* Then, calculate $C$: $C = \\frac{1}{N-1} A_c^TA_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69849196",
   "metadata": {},
   "source": [
    "Let's look at some toy examples (h/t Stephanie Taylor!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc240a5",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "These two variables co-vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46693777",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3,0], [1.1, 2.1, 3, 0.5]]).T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, center the data\n",
    "def centerData(data):\n",
    "    return data - data.mean(axis=0)\n",
    "\n",
    "# Then, calculate C\n",
    "def covarMatrix(data):\n",
    "    centered = centerData(data)\n",
    "    return (centered.T@centered) / (len(centered)-1) # or centered.shape[0] - 1\n",
    "\n",
    "print(centerData(A))\n",
    "print(covarMatrix(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92da1cd2",
   "metadata": {},
   "source": [
    "*What do we observe about the variance of X? of Y? What about the covariance?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b6b48f7",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "This is just like before by the values of $Y$ are now negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b855c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3,0], [-1.1, -2.1, -3, -0.5]]).T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, center the data\n",
    "print(centerData(A))\n",
    "\n",
    "# Then, calculate C\n",
    "print(covarMatrix(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d6b2799",
   "metadata": {},
   "source": [
    "*What do we observe about the variance of X? of Y? What about the covariance?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20b33084",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "\n",
    "The values of $X$ and $Y$ are now random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea344ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([np.random.standard_normal(4), np.random.standard_normal(4)]).T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1099060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, center the data\n",
    "print(centerData(A))\n",
    "\n",
    "# Then, calculate C\n",
    "print(covarMatrix(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4aea489c",
   "metadata": {},
   "source": [
    "*What do we observe about the variance of X? of Y? What about the covariance?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ddd26c5",
   "metadata": {},
   "source": [
    "## On our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = covarMatrix(data)\n",
    "\n",
    "# I want to see it better!\n",
    "print(pd.DataFrame(C))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51ee8fcf",
   "metadata": {},
   "source": [
    "*What do we observe?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a1a09cd",
   "metadata": {},
   "source": [
    "## Warning\n",
    "\n",
    "This is a thing that will calculate the covariance matrix for you, but you *may not use it* for your project: np.cov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe3d8959",
   "metadata": {},
   "source": [
    "# The curse of dimensionality\n",
    "\n",
    "When data is very multidimensional, everything becomes slower and more complicated:\n",
    "* How can you look at your data? There's too much there!\n",
    "* How can you normalize your data? Without being able to look at it, what normalizations would you know to apply?\n",
    "* How can you fit a regression to your data? Which variables would you choose?\n",
    "\n",
    "In fact, data in a high dimensional space just is different:\n",
    "* there are more extreme values. In a 2 dimensional unit square, the probability that a random point is within 0.001 of the border is 0.004, but in a 10000 dimensional unit hypercube, the probability is > 0.999.\n",
    "* the distances between points are bigger. In a 2 dimensional unit square, the average distance between two points is about 0.5. In a 3 dimensional unit cube, it's 0.66. But in a 10000 dimensional unit hypercube, it's about 408.25.\n",
    "\n",
    "Here's the thing though: generally, only a small fraction of variables in a data set suffice to model the whole. For example, very often many features do covary. So we can use covariance information to *project* our data to a much smaller dimensional space and analyze the data in that space instead. The technique we will use for that is *principal component analysis* (PCA)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
