{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c680f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import utilityfunctions as uf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e774a1",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "The steps to calculate PCA are:\n",
    "* (If appropriate) normalize the variables to be in the range 0-1\n",
    "* Center the data\n",
    "* Compute the covariance matrix\n",
    "* Compute the eigenvectors and eigenvalues; the eigenvectors tell us the direction of variance, and the eigenvalues tell us the amount of variance\n",
    "* Get an ordering over the eigenvalues\n",
    "* Sort the eigenvalues and eigenvectors accordingly\n",
    "* Compute the proportional variance (how much bigger?) accounted for by each principal component\n",
    "* Compute the cumulative sum of the proportional variance (tells us how many eigenvectors we need to explain a desired amount of variance)\n",
    "* Examine the principal components. Select v' of them.\n",
    "* Project the data into PCA space\n",
    "* Reconstruct the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc6e1d",
   "metadata": {},
   "source": [
    "## Today\n",
    "\n",
    "Today, we are going to look at what happens when we preprocess the data in different ways before fitting a PCA.\n",
    "\n",
    "So we will look again at the summary statistics for the data.\n",
    "\n",
    "We will preprocess the data in four ways:\n",
    "* center only\n",
    "* minmax global\n",
    "* minmax local\n",
    "* zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things we need from Day 19\n",
    "\n",
    "# Let's split off the labels\n",
    "def split(data, ycol):\n",
    "    y = data[:, ycol]\n",
    "    xfirst = data[:, 0:ycol]\n",
    "    xsecond = data[:, ycol+1:data.shape[1]]\n",
    "    return (np.hstack((xfirst, xsecond)), y)\n",
    "\n",
    "# center\n",
    "def center(data):\n",
    "    centered = data - np.mean(data, axis=0)\n",
    "    return centered\n",
    "\n",
    "# preprocess\n",
    "def preprocess(data, minmax=False, local=False, zscore=False):\n",
    "    if minmax == True and zscore == True:\n",
    "        print(\"Nope, won't do that!\")\n",
    "        return data\n",
    "    elif minmax == True:\n",
    "        if local == False:\n",
    "            data = uf.minmaxGlobal(data)\n",
    "        else:\n",
    "            data = uf.minmaxLocal(data)\n",
    "        return center(data)\n",
    "    elif zscore == True:\n",
    "        return uf.zScore(data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26aa891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is most of the code from Day 19 in one function; it fits a PCA and prints out all kinds of things along the way\n",
    "def pca_with_plots(data):\n",
    "    # covariance\n",
    "    covariance_matrix = (data.T @ data) / (data.shape[0] - 1)\n",
    "    print(\"covariance matrix\")\n",
    "    print(covariance_matrix.shape)\n",
    "\n",
    "    # Let's look at the covariance matrix\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    sns.heatmap(pd.DataFrame(covariance_matrix), xticklabels=columns, yticklabels=columns, annot=False, cmap='PuOr')\n",
    "    plt.show()\n",
    "\n",
    "    # svd\n",
    "    (evals, evectors) = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # sort\n",
    "    evals_order = np.argsort(evals)[::-1]\n",
    "    evals_sorted = evals[evals_order]\n",
    "    evectors_sorted = evectors[:, evals_order]\n",
    "\n",
    "    # proportional variance\n",
    "    evals_sum = np.sum(evals_sorted)\n",
    "    proportional_vars = [e / evals_sum for e in evals_sorted]\n",
    "\n",
    "    # cumulative sum of proportional variance\n",
    "    cumulative_sum = np.cumsum(proportional_vars)\n",
    "\n",
    "    # Let's look at the proportional variance\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(len(proportional_vars)), proportional_vars, alpha=0.5, align='center',\n",
    "            label='Proportional variance')\n",
    "    plt.ylabel('Proportional variance ratio')\n",
    "    plt.xlabel('Ranked Principal Components')\n",
    "    plt.title(\"Scree Graph\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(cumulative_sum)\n",
    "    ax1.set_ylim([0,1.0])\n",
    "    ax1.set_xlabel('Number of Principal Components')\n",
    "    ax1.set_ylabel('Cumulative explained variance')\n",
    "    ax1.set_title('Elbow Plot')\n",
    "    plt.show()\n",
    "    \n",
    "    return evals_sorted, evectors_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68deb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature','chorus_hit','sections','popularity']\n",
    "data = np.array(np.genfromtxt('data/spotify_dataset.csv', delimiter=',', skip_header=1, usecols=(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17), dtype=float, encoding='utf-8'))  \n",
    "\n",
    "print(data.shape)\n",
    "print(uf.getSummaryStatistics(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188749d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_data = center(data)\n",
    "print(\"centered data\")\n",
    "print(centered_data.shape)\n",
    "print(pd.DataFrame(uf.getSummaryStatistics(centered_data)))\n",
    "\n",
    "evals_sorted, evectors_sorted = pca_with_plots(centered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05023b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxed_data = preprocess(data, minmax=True, local=False, zscore=False)\n",
    "print(\"minmaxed data\")\n",
    "print(minmaxed_data.shape)\n",
    "print(pd.DataFrame(uf.getSummaryStatistics(minmaxed_data)))\n",
    "\n",
    "evals_sorted, evectors_sorted = pca_with_plots(minmaxed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d415e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxed_data = preprocess(data, minmax=True, local=True, zscore=False)\n",
    "print(\"minmaxed data\")\n",
    "print(minmaxed_data.shape)\n",
    "print(pd.DataFrame(uf.getSummaryStatistics(minmaxed_data)))\n",
    "\n",
    "evals_sorted, evectors_sorted = pca_with_plots(minmaxed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_data = preprocess(data, minmax=False, zscore=True)\n",
    "print(\"zscored data\")\n",
    "print(zscore_data.shape)\n",
    "print(pd.DataFrame(uf.getSummaryStatistics(zscore_data)))\n",
    "\n",
    "evals_sorted, evectors_sorted = pca_with_plots(zscore_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2af3e",
   "metadata": {},
   "source": [
    "Resources:\n",
    "* https://plotly.com/python/pca-visualization/\n",
    "* https://wendynavarrete.com/principal-component-analysis-with-numpy/\n",
    "* https://dev.to/akaame/implementing-simple-pca-using-numpy-3k0a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
