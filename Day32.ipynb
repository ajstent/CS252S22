{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fee62f89",
   "metadata": {},
   "source": [
    "# The One Goal for Today\n",
    "\n",
    "To understand ROC curves and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, RocCurveDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b45ff9b",
   "metadata": {},
   "source": [
    "I am going to go back to the spam/ham dataset from Wednesday so that we can get a better understanding of ROC curves.\n",
    "\n",
    "## I. Load and Look at our data\n",
    "\n",
    "Let's load and __look at our data__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(np.genfromtxt('data/SMSSpamCollection', delimiter='\\t', encoding='utf-8', dtype=str))  \n",
    "print(data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9aeed082",
   "metadata": {},
   "source": [
    "## II. Split the data\n",
    "\n",
    "Let's split the data into train and test. \n",
    "\n",
    "When we check by printing shapes and unique values, does everything look okay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = np.split(data, [int(.8 * len(data))])\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(np.unique(train_data[:, 0], return_counts=True))\n",
    "print(np.unique(test_data[:, 0], return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df312415",
   "metadata": {},
   "source": [
    "## III. Preprocess the data\n",
    "\n",
    "On Wednesday we tokenized the data and extracted counts for each token for each class ourselves.\n",
    "\n",
    "Today I'm going to use two scikit-learn utilities:\n",
    "\n",
    "CountVectorizer - will tokenize and count\n",
    "LabelEncoder - will map the string labels to ints\n",
    "\n",
    "As on Wednesday, I use *only the training data* to extract my token vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb157178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tokenizes and makes the count table\n",
    "vectorizer = CountVectorizer(lowercase=True, analyzer='word', max_features=1000)\n",
    "vectorizer.fit(iter(train_data[:, 1]))\n",
    "\n",
    "# this encodes the training, dev and test data using the count table\n",
    "# We have to use np.asarray because sklearn 1.0 doesn't want matrices for naive Bayes\n",
    "train_processed = np.asarray(vectorizer.transform(iter(train_data[:, 1])).todense())\n",
    "test_processed = np.asarray(vectorizer.transform(iter(test_data[:, 1])).todense())\n",
    "\n",
    "# this encodes the labels into numbers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data[:, 0])\n",
    "train_labels = encoder.transform(train_data[:, 0])\n",
    "test_labels = encoder.transform(test_data[:, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da7da5cf",
   "metadata": {},
   "source": [
    "## IV. Fit, Predict and Score\n",
    "\n",
    "Today I'm going to compare the performance of several scikit-learn Naive Bayes alternatives on this dataset. These variations on Naive Bayes model different *probability distributions* over the training data, rather than using the likelihoods and priors directly.\n",
    "\n",
    "Although we aren't using our own, hand-written Naive Bayes, you can see that the pattern is the same:\n",
    "1. Fit\n",
    "2. Predict\n",
    "3. Score\n",
    "\n",
    "With respect to \"score\", you'll see we are calculating:\n",
    "* precision\n",
    "* recall\n",
    "* F1\n",
    "\n",
    "*per class*. \n",
    "\n",
    "Let's make a different version of the confusion matrix focusing on just the 'spam' class, which is the class we really want to do well on:\n",
    "\n",
    "| | Predict not in 'spam' | Predict in 'spam' | Rates |\n",
    "| -- | --- | --- | -- |\n",
    "| Actual not in 'spam' | TN | FP | FPR = FP/(FP+TN) |\n",
    "| Actual in 'spam' | FN | TP | TPR = TP/(TP+FN) |\n",
    "\n",
    "With a table like this, we can calculate:\n",
    "* Precision (how many of those we guessed were 'spam' were actually 'spam'?): TP / (TP + FP)\n",
    "* Recall (how many actual 'spam' did we guess were 'spam'?): TP / (TP + FN)\n",
    "\n",
    "Then, to get an assessment of recall and precision together, we can calculate F1: (2\\*Precision\\*Recall)/(Precision+Recall)\n",
    "\n",
    "Today, we depict the confusion matrix using a heatmap, for variety.\n",
    "\n",
    "Q: *What is a situation in which you might prefer to know P/R/F1 instead of accuracy, or where accuracy might be misleading?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_processed, train_labels)\n",
    "pred = nb.predict(test_processed)\n",
    "print(classification_report(test_labels, pred, target_names=encoder.classes_))\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "sns.heatmap(confusion_matrix(test_labels, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0f5e5f3",
   "metadata": {},
   "source": [
    "## ROC Curve; AUC\n",
    "\n",
    "We can also plot the **ROC curve** and calculate **AUC**.\n",
    "* ROC - receiver operating characteristic curve, constructed by plotting the TPR against the FPR\n",
    "* AUC - area under the ROC curve\n",
    "\n",
    "Here is a good introduction to these: https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/\n",
    "\n",
    "We will plot the ROC curve using sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617459c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(test_labels, nb.predict_proba(test_processed)[:, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc2d5637",
   "metadata": {},
   "source": [
    "Let's take a closer look at those predictions. We will call predict_proba instead of predict. This will give us the *probabilities* the naive Bayes classifier uses to determine the best class. So for each data point, we will have the probability of ham and the probability of spam. \n",
    "\n",
    "Q: *Are these the prior, likelihood, normalization or posterior probabilities?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_with_probs = nb.predict_proba(test_processed)\n",
    "print(pred_with_probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a65a8aad",
   "metadata": {},
   "source": [
    "By default, we take the label with the maximum probability, right? However, you can see that one way we could \"adjust\" the classifier is to set a threshold for making a decision; so for example, it's not spam unless the probability of spam is at least 60%.\n",
    "\n",
    "Q: *What would be the effect of setting that 60% threshold?*\n",
    "\n",
    "The ROC curve plots the FPR (x axis) against the TPR (y axis). \n",
    "\n",
    "Q: *What do FPR and TPR stand for, again?*\n",
    "\n",
    "So the ROC curve illustrates the thresholds that are available to us, and the consequences if we choose a different threshold.\n",
    "\n",
    "Now, AUC is the area under that curve.\n",
    "\n",
    "Q: *If AUC is 1, then what can we say about the model?*\n",
    "\n",
    "Q: *If AUC is 0, then what can we say about the model?*\n",
    "\n",
    "Quite often, you will see a ROC curve plotted against a diagonal line as in \n",
    "\n",
    "![this example from Wikipedia](https://upload.wikimedia.org/wikipedia/commons/d/de/ROC_curve.svg)\n",
    "\n",
    "Q: *What does the diagonal line denote?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(train_processed, train_labels)\n",
    "pred = nb.predict(test_processed)\n",
    "print(classification_report(test_labels, pred, target_names=encoder.classes_))\n",
    "sns.heatmap(confusion_matrix(test_labels, pred))\n",
    "RocCurveDisplay.from_predictions(test_labels, nb.predict_proba(test_processed)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "nb.fit(train_processed, train_labels)\n",
    "pred = nb.predict(test_processed)\n",
    "print(classification_report(test_labels, pred, target_names=encoder.classes_))\n",
    "sns.heatmap(confusion_matrix(test_labels, pred))\n",
    "RocCurveDisplay.from_predictions(test_labels, nb.predict_proba(test_processed)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6379664",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(train_processed, train_labels)\n",
    "pred = nb.predict(test_processed)\n",
    "print(classification_report(test_labels, pred, target_names=encoder.classes_))\n",
    "sns.heatmap(confusion_matrix(test_labels, pred))\n",
    "RocCurveDisplay.from_predictions(test_labels, nb.predict_proba(test_processed)[:, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eae74196",
   "metadata": {},
   "source": [
    "Q. *Looking at these ROC curves and AUC scores, which model would you choose? Why? Does your choice correspond to the model with highest accuracy?*\n",
    "\n",
    "You *can* plot ROC curves for multiclass classifiers, but you have to do it one label at a time, either:\n",
    "* one versus rest - assume this class is \"positive\" and all the other datapoints are \"negative\"\n",
    "* one versus one - compare pairs of labels\n",
    "\n",
    "## V. Resources\n",
    "\n",
    "* https://towardsdatascience.com/roc-and-auc-how-to-evaluate-machine-learning-models-in-no-time-fb2304c83a7f\n",
    "* https://towardsdatascience.com/interpreting-roc-curve-and-roc-auc-for-classification-evaluation-28ec3983f077\n",
    "* https://towardsdatascience.com/multiclass-classification-evaluation-with-roc-curves-and-roc-auc-294fd4617e3a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
