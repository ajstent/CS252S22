{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35832fe7",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "An artificial neural network (ANN, or just NN) is:\n",
    "* a __graph__ where\n",
    "* the __nodes__ process inputs (from the \"outside world\" or from other nodes)\n",
    "  * each node processes inputs according to an __activation function__, which is typically nonlinear (eg a Gaussian)\n",
    "* the __edges__ indicate the flow of data between nodes\n",
    "  * each edge has a __weight__\n",
    "  \n",
    "When the ANN is trained, the hyperparameters of the activation function and the edge weights are fit to the training data.\n",
    "\n",
    "A neural network is typically trained using a MSSE loss function and gradient descent of some type.\n",
    "\n",
    "There are different types of ANN. For example:\n",
    "* feedforward - the information flow in this type of NN is always from input to output\n",
    "* recurrent - a node may feed *back into itself*\n",
    "\n",
    "In a neural network, nodes that read directly from the outside world are typically in an \"input layer\"; those that write directly to the outside world are in an \"output layer\"; and all the rest are in one or more \"hidden layers\". For example, in DALL-E there are 64 self-attention layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a67e0e",
   "metadata": {},
   "source": [
    "## Let's look at one node in a neural network\n",
    "\n",
    "Node $j$ in a neural network takes inputs $I_j$ and produces a single output $O_j$, by calculating $f(I_j)$. \n",
    "\n",
    "Typically, $I_j = \\sum_i^{N-1} w_{ij}O_i$, where $w_{ij}$ is the weight on the edge from node $i$ to node $j$.\n",
    "\n",
    "Each node typically has a *bias term*, a constant value, which is fit during training just like the edge weights. In the sum above, $O_0$ would be the bias term. \n",
    "\n",
    "The function $f$ is the activation function. It could be any type of function; for example, linear, or ReLU, or Gaussian. For more, see the resources below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a22bf1",
   "metadata": {},
   "source": [
    "## Let's play with neural networks\n",
    "\n",
    "Open https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5e9c8",
   "metadata": {},
   "source": [
    "# Radial Basis Functions\n",
    "\n",
    "A radial basis function is \"a real-valued function whose value depends only on the distance between the input and some fixed point\" (https://en.wikipedia.org/wiki/Radial_basis_function).\n",
    "\n",
    "What might look like that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12feb31e",
   "metadata": {},
   "source": [
    "# Radial Basis Function Networks\n",
    "\n",
    "A RBF network is a feedforward network with one hidden layer. Each node in the hidden layer represents a prototype, or prototypical point, fit from the training data. Any new point (eg in the test data) that is close enough to the prototype will activate that node.\n",
    "\n",
    "The input layer in a RBF will have one node for each dimension in the input data.\n",
    "\n",
    "The output layer will have one node for each class into which the data may be classified. A well-trained RBF will activate only one node in the output layer for any input data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabffb4",
   "metadata": {},
   "source": [
    "## Training a RBF\n",
    "\n",
    "Training a RBF consists of:\n",
    "* Finding prototypes\n",
    "* Selecting the activation function for the hidden nodes\n",
    "* Selecting the activation function for the output nodes\n",
    "* Setting the weights for the edges and biases\n",
    "\n",
    "To find prototypes we can select training data points at random, but it will work better if we use one of the analysis methods we already know.\n",
    "\n",
    "A typical activation function for the hidden nodes is the Gaussian, so something like $exp \\left - \\frac{||\\vec{d}-\\vec{\\mu_j}||^2}{2\\delta_j^2 + \\epsilon}$, where $\\vec{d}$ is the data point, $\\vec{\\mu_j}$ is the prototype, $\\delta_j$ is the hidden unit's standard deviation, $\\epsilon$ is a small constant and $||.||^2$ is the squared Euclidean distance.\n",
    "\n",
    "A good activation function for the output nodes is $w_{bias,k} + \\sum_{j=1}^{N_p}(w_{j,k}H_j)$ where $w_{bias,k}$ is the weight from the bias node (which has a value of 1) to the $k$th output node, $N_p$ is the number of prototypes (hidden nodes), $w_{j,k}$ is the weight on the edge from the $j$th hidden node to the $k$th output node and $H_j$ is the activation level (output) of the $j$th hidden node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f5a2b",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "* https://playground.tensorflow.org\n",
    "* https://mathworld.wolfram.com/RadialFunction.html\n",
    "* http://www.ideal.ece.utexas.edu/papers/agogino_1999ijcnn.ps.gz\n",
    "* https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c\n",
    "* https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
