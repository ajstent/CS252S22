{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db20836",
   "metadata": {},
   "source": [
    "# RBF Networks with Iris Data\n",
    "\n",
    "\n",
    "Review from Monday:\n",
    "\n",
    "Training a RBF consists of:\n",
    "* Finding prototypes\n",
    "* Selecting the activation function for the hidden nodes\n",
    "* Selecting the activation function for the output nodes\n",
    "* Setting the weights for the edges and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a175dc86",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2a9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def typeConverter(x):\n",
    "    values = ['setosa', 'versicolor', 'virginica']\n",
    "    return float(values.index(x))\n",
    "\n",
    "\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "iris = np.array(np.genfromtxt('../data/iris.csv', delimiter=',', converters={4: typeConverter}, skip_header=2, dtype=float, encoding='utf-8'))  \n",
    "print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e9b4c",
   "metadata": {},
   "source": [
    "# Look at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStatistics(data):\n",
    "    \"Get the max, min, mean, var for each variable in the data.\"\n",
    "    return pd.DataFrame(np.array([data.max(axis=0), data.min(axis=0), data.mean(axis=0), data.var(axis=0)]))\n",
    "\n",
    "def getShapeType(data):\n",
    "    \"Get the shape and type of the data.\"\n",
    "    return (data.shape, data.dtype)\n",
    "\n",
    "print(getSummaryStatistics(iris))\n",
    "getShapeType(iris)\n",
    "\n",
    "df = pd.DataFrame(iris, columns=columns)\n",
    "\n",
    "sns.pairplot(df, y_vars = [\"class\"], kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e1293",
   "metadata": {},
   "source": [
    "## What kind of analysis are we going to do?\n",
    "\n",
    "Regression, clustering, classification?\n",
    "\n",
    "If supervised, which is our dependent variable?\n",
    "\n",
    "If we have a dependent variable, how many possible values does it have? What will this number correspond to in the RBF network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are we doing this?\n",
    "np.random.shuffle(iris)\n",
    "\n",
    "# Why are we doing this?\n",
    "train_data, dev_data, test_data = np.split(iris, [int(.8 * len(iris)), int(.9 * len(iris))])\n",
    "print(\"train\", \"\\n\", getSummaryStatistics(train_data), np.unique(train_data[:, -1]))\n",
    "print(\"dev\", \"\\n\", getSummaryStatistics(dev_data), np.unique(dev_data[:, -1]))\n",
    "print(\"test\", \"\\n\", getSummaryStatistics(test_data), np.unique(test_data[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a7b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split off the y variables\n",
    "\n",
    "train_data, train_y = train_data[:, :-1], train_data[:, -1]\n",
    "dev_data, dev_y = dev_data[:, :-1], dev_data[:, -1]\n",
    "test_data, test_y = test_data[:, :-1], test_data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd73d8",
   "metadata": {},
   "source": [
    "## Does the data need to be cleaned?\n",
    "\n",
    "Are there missing or erroneous values? \n",
    "\n",
    "Do we need to fix the types of some of the variables?\n",
    "\n",
    "## Does it need to be normalized?\n",
    "\n",
    "Is the range of one or more values clearly out of line with the rest?\n",
    "\n",
    "## Consider transformation\n",
    "\n",
    "Would PCA help?\n",
    "* if we had a thousand independent variables, probably, but in this case no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee65252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogenizeData(data):\n",
    "    return np.append(data, np.array([np.ones(data.shape[0], dtype=float)]).T, axis=1)\n",
    "   \n",
    "def zScore(data, translateTransform=None, scaleTransform=None):\n",
    "    \"z score.\"\n",
    "    homogenizedData = np.append(data, np.array([np.ones(data.shape[0], dtype=float)]).T, axis=1)\n",
    "    if translateTransform is None:\n",
    "        translateTransform = np.eye(homogenizedData.shape[1])\n",
    "        for i in range(homogenizedData.shape[1]):\n",
    "            translateTransform[i, homogenizedData.shape[1]-1] = -homogenizedData[:, i].mean()\n",
    "    if scaleTransform is None:\n",
    "        diagonal = [1 / homogenizedData[:, i].std() if homogenizedData[:, i].std() != 0 else 1 for i in range(homogenizedData.shape[1])]\n",
    "        scaleTransform = np.eye(homogenizedData.shape[1], dtype=float) * diagonal\n",
    "    data = (scaleTransform@translateTransform@homogenizedData.T).T\n",
    "    return translateTransform, scaleTransform, data[:, :data.shape[1]-1]\n",
    "\n",
    "translateTransform, scaleTransform, train_data_transformed = zScore(train_data)\n",
    "print(getSummaryStatistics(train_data_transformed))\n",
    "getShapeType(train_data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d7cfc",
   "metadata": {},
   "source": [
    "# Find Prototypes\n",
    "\n",
    "To do this, we use kmeans. I am going to use the scikit-learn implementation; you should use your own for the project.\n",
    "\n",
    "Why would we not just have the number of prototypes be equal to the number of classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertia_by_k = []\n",
    "\n",
    "for k in range(2, 17):\n",
    "    print(k)\n",
    "    km = KMeans(n_clusters=k, random_state=0).fit(train_data)\n",
    "    inertia_by_k.append([k, km.inertia_])\n",
    "\n",
    "inertia_by_k = np.array(inertia_by_k)\n",
    "print(inertia_by_k)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(inertia_by_k[:, 0], inertia_by_k[:, 1])\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbc06c",
   "metadata": {},
   "source": [
    "So, what value will we choose for k? What will this number correspond to in the RBF network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c617009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the centroid for each hiddent node - each prototype\n",
    "k = 11\n",
    "\n",
    "km = KMeans(n_clusters=k, random_state=0).fit(train_data)\n",
    "\n",
    "print(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5cd661",
   "metadata": {},
   "source": [
    "# Define the Activation Function for the Hidden Nodes\n",
    "\n",
    "Recall that a typical activation function for the hidden nodes is the Gaussian, so something like $exp \\left( - \\frac{||\\vec{d}-\\vec{\\mu_j}||^2}{2\\delta_j^2 + \\epsilon} \\right)$, where $\\vec{d}$ is the data point, $\\vec{\\mu_j}$ is the prototype, $\\delta_j$ is the hidden unit's standard deviation, $\\epsilon$ is a small constant and $||.||^2$ is the squared Euclidean distance.\n",
    "\n",
    "Let's take a good look at this activation function. \n",
    "* What is in the numerator? Why look, it's the distance! Why would we not just use the distance itself as the activation function? \n",
    "* What is the function of $\\delta_j$?\n",
    "* Why do we have $\\epsilon$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9510186",
   "metadata": {},
   "source": [
    "# What Will We Do When We Get a New Data Point?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306056a",
   "metadata": {},
   "source": [
    "At this point, we have defined:\n",
    "* The input layer (ish)\n",
    "* The hidden layer\n",
    "\n",
    "For a new data point, we will:\n",
    "1. encode it using the same zscoring we did on train - not defining a new zscoring. IE use mean and stdev from the *training data*\n",
    "2. send it to each of the hidden layer nodes (so the weights from the input layer to the hidden layer are all 1)\n",
    "3. each hidden layer node will calculate its activation for this data point\n",
    "\n",
    "On Monday we will define the output layer, and explain how it relates to another analysis method we already know well, linear regression. We will then show how we can *also use RBF networks for regression*!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fc967",
   "metadata": {},
   "source": [
    "# Let's Process the Dev Data Through the Hidden Layer with Matrix Math\n",
    "\n",
    "So to process a set of data points, eg the dev data, I'm going to:\n",
    "1. \"encode\" - input layer - subtract mean of training data and divide by stdev of training data. Take a look at zScore above; it *already does all this with matrix multiplication*! Remember, a zScoring is just a translation and scaling.\n",
    "2. calculate activations of hidden layer nodes. Take a look at the activation function. Inside the exponent, it has a numerator and a denominator. The denominator operates as a scaling, and you know how to do that. The numerator includes a translation (see the minus?) and then squares it (and you know how to do that!). And the exponentiation can be done via broadcasting.\n",
    "\n",
    "Because you should implement the activation function above yourselves, I'm instead going to implement this stupid activation function just to show you the matrix math:\n",
    "$exp \\left( - \\frac{||\\vec{d}-\\vec{\\mu_j}||}{3} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba143005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other thing you need is for each of these, the activation function\n",
    "# I am going to implement a _stupid activation function_ so that you can implement the right one yourselves for project 7\n",
    "def calculateActivations(data, centroids):\n",
    "    \"I repeat, do not use this activation function directly. This one is exp(-distance / 3); yours is exp(-distance^2 / (2*radius + epsilon))\"\n",
    "    # You can easily fiddle with this numerator to make it calculate the square of the distance\n",
    "    numerator = -np.linalg.norm(data-centroids[:,np.newaxis], axis = 2).T\n",
    "    # The construction of your denominator will be a little more complex than this; the diagonals will be centroid/prototype-specific\n",
    "    denominator = np.eye(centroids.shape[0], dtype=float) * 1/3\n",
    "    print(numerator.shape, denominator.shape)\n",
    "    return np.exp((denominator@numerator.T).T)\n",
    "\n",
    "train_calcs = calculateActivations(train_data_transformed, km.cluster_centers_)\n",
    "print(train_calcs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523fa09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to normalize the dev data using the scale and translate from the train data\n",
    "_, _, dev_data_transformed = zScore(dev_data, translateTransform, scaleTransform)\n",
    "print(dev_data_transformed.shape)\n",
    "dev_calcs = calculateActivations(dev_data_transformed, km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d205b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(train_calcs, open(\"../hidden_node_weights_train.pkl\",'wb'))\n",
    "pkl.dump(train_y, open(\"../labels_train.pkl\",'wb'))\n",
    "pkl.dump(dev_calcs, open(\"../hidden_node_weights_dev.pkl\",'wb'))\n",
    "pkl.dump(dev_y, open(\"../labels_dev.pkl\",'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
