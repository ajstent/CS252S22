{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e16737ff",
   "metadata": {},
   "source": [
    "# The One Goal for Today\n",
    "\n",
    "To implement and train a from input to hidden of a RBF network using python.\n",
    "\n",
    "# RBF Networks with the car logo data\n",
    "\n",
    "Review: training a RBF consists of:\n",
    "* Finding prototypes\n",
    "* Selecting the activation function for the hidden nodes\n",
    "* Selecting the activation function for the output nodes\n",
    "* Setting (or fitting) the weights for the edges and biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ba73819",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We will keep using the car logo dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(np.genfromtxt('data/logos.csv', delimiter=',', dtype=int))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6593f0",
   "metadata": {},
   "source": [
    "# Look at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStatistics(data):\n",
    "    \"Get the max, min, mean, var for each variable in the data.\"\n",
    "    return pd.DataFrame(np.array([data.max(axis=0), data.min(axis=0), data.mean(axis=0), data.var(axis=0)]))\n",
    "\n",
    "def getShapeType(data):\n",
    "    \"Get the shape and type of the data.\"\n",
    "    return (data.shape, data.dtype)\n",
    "\n",
    "print(getSummaryStatistics(data))\n",
    "getShapeType(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d109eb",
   "metadata": {},
   "source": [
    "## What kind of analysis are we going to do?\n",
    "\n",
    "Regression, clustering, classification?\n",
    "\n",
    "If supervised, which is our dependent variable?\n",
    "\n",
    "If we have a dependent variable, how many possible values does it have? What will this number correspond to in the RBF network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3cf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are we doing this?\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Why are we doing this?\n",
    "train_data, dev_data, test_data = np.split(data, [int(.8 * len(data)), int(.9 * len(data))])\n",
    "print(getSummaryStatistics(train_data))\n",
    "print(getSummaryStatistics(dev_data))\n",
    "print(getSummaryStatistics(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22318350",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[:, -1]\n",
    "x_train = train_data[:, 0:-1]\n",
    "y_dev = dev_data[:, -1]\n",
    "x_dev = dev_data[:, 0:-1]\n",
    "y_test = test_data[:, -1]\n",
    "x_test = test_data[:, 0:-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eb8149c",
   "metadata": {},
   "source": [
    "## Does the data need to be cleaned?\n",
    "\n",
    "Are there missing or erroneous values? \n",
    "\n",
    "Do we need to fix the types of some of the variables?\n",
    "\n",
    "## Does it need to be normalized?\n",
    "\n",
    "Is the range of one or more values clearly out of line with the rest?\n",
    "\n",
    "## Consider transformation\n",
    "\n",
    "Would PCA help? Yes, probably! We will reuse the code from day 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pca(data):\n",
    "    # center data\n",
    "    centered_data = data - np.mean(data, axis=0)\n",
    "    # covariance matrix\n",
    "    covariance_matrix = (centered_data.T @ centered_data) / (data.shape[0] - 1)\n",
    "    # singular value decomposition\n",
    "    evals, evectors = scipy.linalg.eigh(covariance_matrix)\n",
    "    # sort eigenvals, eigenvecs\n",
    "    order = np.argsort(evals)[::-1]\n",
    "    eigenvals_sorted = evals[order]\n",
    "    eigenvecs_sorted = evectors[:, order]\n",
    "    return centered_data, covariance_matrix, eigenvals_sorted, eigenvecs_sorted\n",
    "\n",
    "def plot_covariance_matrix(covariance_matrix):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    sns.heatmap(pd.DataFrame(covariance_matrix), annot=False, cmap='PuOr')\n",
    "    plt.show()\n",
    "\n",
    "def plot_eigenvectors(eigenvecs_sorted):\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    sns.heatmap(pd.DataFrame(eigenvecs_sorted[:, 0:21].T), \n",
    "                annot=False, cmap='coolwarm',\n",
    "               vmin=-0.5,vmax=0.5)\n",
    "    plt.ylabel(\"Ranked Eigenvalue\")\n",
    "    plt.xlabel(\"Eigenvector Components\")\n",
    "    plt.show()\n",
    "\n",
    "def get_proportional_variances(eigenvals_sorted):\n",
    "    sum = np.sum(eigenvals_sorted)\n",
    "    proportional_variances = np.array([eigenvalue / sum for eigenvalue in eigenvals_sorted])\n",
    "    cumulative_sum = np.cumsum(proportional_variances)\n",
    "    return proportional_variances, cumulative_sum\n",
    "\n",
    "def scree_graph(proportional_variances):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(len(proportional_variances)), proportional_variances, alpha=0.5, align='center',\n",
    "            label='Proportional variance')\n",
    "    plt.ylabel('Proportional variance ratio')\n",
    "    plt.xlabel('Ranked Principal Components')\n",
    "    plt.title(\"Scree Graph\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def elbow_plot(cumulative_sum):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(cumulative_sum)\n",
    "    ax1.set_ylim([0,1.0])\n",
    "    ax1.set_xlabel('Number of Principal Components')\n",
    "    ax1.set_ylabel('Cumulative explained variance')\n",
    "    ax1.set_title('Elbow Plot')\n",
    "    plt.show()\n",
    "\n",
    "def fit_pca(centered_data, eigenvecs_sorted, number_to_keep):\n",
    "    v = eigenvecs_sorted[:, :number_to_keep]\n",
    "    projected_data = centered_data@v\n",
    "    return projected_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc494688",
   "metadata": {},
   "source": [
    "As we know from day 30, we will keep 200 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8033e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_train, covariance_matrix, eigenvals_sorted, eigenvecs_sorted = prep_pca(x_train)\n",
    "projected_train = fit_pca(centered_train, eigenvecs_sorted, 200)\n",
    "centered_dev = x_dev - np.mean(x_train, axis=0)\n",
    "projected_dev = fit_pca(centered_dev, eigenvecs_sorted, 200)\n",
    "centered_test = x_test - np.mean(x_train, axis=0)\n",
    "projected_test = fit_pca(centered_test, eigenvecs_sorted, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e59f1d",
   "metadata": {},
   "source": [
    "# Find Prototypes\n",
    "\n",
    "To do this, we use kmeans. I am going to use the scikit-learn implementation; you should use your own for the project.\n",
    "\n",
    "Why would we not just have the number of prototypes be equal to the number of classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ee397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertia_by_k = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "    print(k)\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_init = 10).fit(projected_train)\n",
    "    inertia_by_k.append([k, km.inertia_])\n",
    "\n",
    "inertia_by_k = np.array(inertia_by_k)\n",
    "print(inertia_by_k)\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(inertia_by_k[:, 0], inertia_by_k[:, 1])\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe8c2a",
   "metadata": {},
   "source": [
    "So, what value will we choose for k? What will this number correspond to in the RBF network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21240a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 14\n",
    "\n",
    "km = KMeans(n_clusters=k, random_state=0, n_init=10).fit(projected_train)\n",
    "\n",
    "print(km.cluster_centers_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c6a4c3e",
   "metadata": {},
   "source": [
    "# Define the Activation Function for the Hidden Nodes\n",
    "\n",
    "Recall that a typical activation function for the hidden nodes is the Gaussian, so something like $exp \\left( - \\frac{||\\vec{d}-\\vec{\\mu_j}||^2}{2\\delta_j^2 + \\epsilon} \\right)$, where $\\vec{d}$ is the data point, $\\vec{\\mu_j}$ is the prototype, $\\delta_j$ is the hidden unit's standard deviation, $\\epsilon$ is a small constant and $||.||^2$ is the squared Euclidean distance.\n",
    "\n",
    "Let's take a good look at this activation function. \n",
    "* *What is in the numerator? Why look, it's the distance! Why would we not just use the distance itself as the activation function?* \n",
    "* *What is the function of $\\delta_j$?*\n",
    "* *Why do we have $\\epsilon$?*\n",
    "\n",
    "In the cell below. you should define a function that will return the activations for the hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce42224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e68e75f",
   "metadata": {},
   "source": [
    "# What Will We Do When We Get a New Data Point?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d92e155d",
   "metadata": {},
   "source": [
    "At this point, we have defined:\n",
    "* The input layer (ish)\n",
    "* The hidden layer\n",
    "\n",
    "For a new data point, we will:\n",
    "1. encode it using the same method we used for train, including any summary statistics **on the training data**\n",
    "2. \"send it\" to each of the hidden layer nodes (so the weights from the input layer to the hidden layer are all 1) - this is really just a matrix multiply!\n",
    "3. each hidden layer node will calculate its activation for this data point - more matrix multiply!\n",
    "\n",
    "Next class session we will define the output layer, and explain how it relates to another analysis method we already know well, linear regression. We will then show how we can *also RBF networks for classification AND regression*!\n",
    "\n",
    "To facilitate that, let's save our processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55af2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hidden_node_weights_train.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_calcs, f)\n",
    "with open(\"labels_train.pkl\", \"wb\") as f:\n",
    "    pkl.dump(train_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf68e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
