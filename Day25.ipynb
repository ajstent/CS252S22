{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The One Goal for Today\n",
    "\n",
    "Understand different ways to evaluate models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating regression models\n",
    "\n",
    "*What are three types of regression model we know how to fit?*\n",
    "\n",
    "*How do we evaluate the performance of a regression model?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating clustering models\n",
    "\n",
    "*What is one type of clustering method we know how to fit?*\n",
    "\n",
    "Clustering is unsupervised; this makes evaluation harder because there is no *ground truth* (there are no *labels*). \n",
    "\n",
    "One thing we can calculate without labels is the **silhouette coefficient**. The silhouette coefficient is calculated as:\n",
    "$$SC = 1/N \\sum_i^N \\frac{b_i-a_i}{max(a_i,b_i)}$$\n",
    "\n",
    "where $a_i$ is the average distance between the $ith$ datapoint and all other datapoints in its cluster, and $b_i$ is the average distance between the $ith$ datapoint and all datapoints in its next nearest cluster.\n",
    "\n",
    "For more evaluation metrics for clustering, including ones that require you to obtain labels for some data points, see:\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating binary classification models\n",
    "\n",
    "Our Craigslist car dataset includes listings for two car manufacturers: Hyundai and Kia. Let's image that we train a kNN classifier to distinguish Hyundais (H) from Kias (K). Let's imagine these are the results for ten of the datapoints in the test data:\n",
    "\n",
    "| Item | $y$ | $\\hat{y}$ |\n",
    "| ---- | --- | -------- |\n",
    "|  0   |  H  |   H |\n",
    "|  1   |  H  |   H |\n",
    "|  2   |  H  |   K |\n",
    "|  3   |  H  |   H |\n",
    "|  4   |  H  |   K |\n",
    "|  5   |  K  |   H |\n",
    "|  6   |  K  |   K |\n",
    "|  7   |  K  |   K |\n",
    "|  8   |  K  |   K |\n",
    "|  9   |  K  |   K |\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "For k-nearest neighbors, so far we have evaluated using **accuracy**: the percentage of data points for which the predicted class is the same as the actual class. \n",
    "\n",
    "*Using the Craigslist result table, what is the accuracy of our model?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "Accuracy is a nice simple metric, but it doesn't help us understand *which* data points are being misclassified, which might be important for improving the model or deciding whether to deploy the model. For example, if a model for car logo identification works great on every manufacturer other than Kia, then maybe we focus on getting better data for Kias. Or if a model for determining which students to admit to Colby does a good job for white and Asian students, but a terrible job for Black students, maybe we *do not deploy that model*. What *can* help us dig deeper into the performance of a model is a **confusion matrix**.\n",
    "\n",
    "For binary classification (two labels), pick one class to be 'positive' and the other 'negative'; then a confusion matrix looks like:\n",
    "\n",
    "| Total population = P+N | Predict positive | Predict negative | \n",
    "| -- | --- | --- |\n",
    "| **Actual positive** (P) | TP | FN | \n",
    "| **Actual negative** (N) | FP | TN | \n",
    "\n",
    "*Using the Craigslist results table, what is the confusion matrix for our model?*\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPR, FPR, Precision, Recall, F\n",
    "\n",
    "Once we have a confusion matrix we can calculate interesting things from it, including:\n",
    "1. **True Positive Rate (TPR)**: TPR = TP/(TP+FN) (this is also called Recall, R)\n",
    "2. **False Positive Rate (FPR)**: FPR = FP/(FP+TN)\n",
    "3. **Accuracy** (!): ACC = (TP+TN)/(P+N)\n",
    "4. **Precision (P)**: P = TP/(TP+FP)\n",
    "5. **F1**: F1 = 2x((PxR)/(P+R)) (we call this F1 because you could pick a number other than 2 and get a different F)\n",
    "\n",
    "*Using the confusion matrix you created, what are the TPR (R), FPR, Accuracy, P and F1 for our model?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "All those great metrics for binary classifiers do have multiclass equivalents, but they require a small mental leap.\n",
    "\n",
    "To create a confusion matrix for a multiclass classifier, we have to think of it as a combination (or *ensemble*) of binary classifiers, either:\n",
    "* One-vs-rest (one-vs-all) - one binary classifier for each class, with the positive examples being data points in the class and the negative examples being data points in any other class. \n",
    "* One-vs-one - one binary classifier for each pair of classes.\n",
    "\n",
    "Note: depending on the ML algorithm, we don't have to actually *fit* a bunch of binary classifiers, we just have to *imagine that we did*. For example, for kNN we only fit one model regardless of the number of classes.\n",
    "\n",
    "I like one-vs-rest, for a reason that will become clear in a minute.\n",
    "\n",
    "Our car logo dataset has 34 car logos in it.\n",
    "\n",
    "Questions:\n",
    "1. *For 34 classes, how many classifiers would we (mentally) fit for one-vs-rest?*\n",
    "2. *For 34 classes, how many classifiers would we (mentally) fit for one-vs-one?*\n",
    "\n",
    "Once we have (mentally) fit binary classifiers, we can create one confusion matrix per model. Then we can calculate all the metrics for each class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a stab at writing the code to get a confusion matrix and calculate TPR, FPR, P, Accuracy and F1 from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# this captures the Craigslist results table; we will use 0 for H and 1 for K\n",
    "y = np.array([0,0,0,0,0,1,1,1,1,1])\n",
    "yhat = np.array([0,0,1,0,1,0,1,1,1,1])\n",
    "\n",
    "def confusion_matrix(y, yhat):\n",
    "    ??\n",
    "\n",
    "def true_positive_rate(confusion_matrix):\n",
    "    ??\n",
    "\n",
    "def false_positive_rate(confusion_matrix):\n",
    "    ??\n",
    "\n",
    "def true_positive_rate(confusion_matrix):\n",
    "    ??\n",
    "\n",
    "def precision(confusion_matrix):\n",
    "    ??\n",
    "\n",
    "def recall(confusion_matrix):\n",
    "    ??\n",
    "\n",
    "def f1(confusion_matrix):\n",
    "    ??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
