{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy.linalg as sp_la\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16737ff",
   "metadata": {},
   "source": [
    "# Overall Review <a class=\"anchor\" id=\"review\"></a>\n",
    "\n",
    "\n",
    "Today we are going to load a dataset and use it to review each type of data analysis method we have looked at this semester.\n",
    "\n",
    "*NB* I guarantee there are bugs in the code, and quite likely errors or gaps in the descriptions. Comments welcome!\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [Prepare Your Data](#prepData)\n",
    "    * [Load Your Data](#loadData)\n",
    "    * [Look At Your Data](#lookData)\n",
    "    * [Clean Your Data](#cleanData)\n",
    "    * [Split Your Data](#splitData)\n",
    "    * [Transform/Normalize Your Data](#normalizeData)\n",
    "    * [Reduce The Dimensionality of Your Data](#pcaData)\n",
    "    * [Summary](#summaryData)\n",
    "* [Model](#model)\n",
    "    * [Distance Metrics](#distances)\n",
    "    * [Regression](#regression)\n",
    "      * [Types](#regressionTypes)\n",
    "      * [Distance Metrics](#regressionDistance)\n",
    "      * [Loss Function](#regressionLoss)\n",
    "      * [Methods](#regressionMethods)\n",
    "        * [Least Squares](#leastSquares)\n",
    "        * [Normal Equation](#normalEquation)\n",
    "        * [QR Decomposition](#qrDecomposition)\n",
    "        * [Gradient Descent](#gradientDescent)\n",
    "        * [Stepwise](#stepwiseRegression)\n",
    "        * [RBF Networks](#regressionRBF)\n",
    "      * [Evaluation and Visualization](#regressionEvaluation)\n",
    "      * [Hyperparameters](#regressionHyperparameters)\n",
    "    * [Clustering](#clustering)\n",
    "      * [Distance Metrics](#clusteringDistance)\n",
    "      * [Loss Function](#clusteringLoss)\n",
    "      * [Methods](#clusteringMethods)\n",
    "        * [K Means](#kmeans)\n",
    "      * [Evaluation and Visualization](#clusteringEvaluation)\n",
    "      * [Hyperparameters](#clusteringHyperparameters)\n",
    "      * [Example](#clusteringExample)\n",
    "    * [Classification](#classification)\n",
    "      * [Distance Metrics](#classificationDistance)\n",
    "      * [Loss Function](#classificationgLoss)\n",
    "      * [Methods](#classificationMethods)\n",
    "        * [K Nearest Neighbors](#knn)\n",
    "        * [Naive Bayes](#nb)\n",
    "        * [RBF Networks](#classificationRBF)\n",
    "      * [Evaluation and Visualization](#classificationEvaluation)\n",
    "      * [Hyperparameters](#classificationHyperparameters)\n",
    "      * [Probability Distributions](#classificationProbabilityDistributions)\n",
    "      * [Example](#classificationExample)\n",
    "  * [RBF Networks](#rbfNetworks)\n",
    "    * [Example For Regression](#rbfRegression)\n",
    "    * [Example For Classification](#rbfClassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cb32d",
   "metadata": {},
   "source": [
    "# Prepare Your Data <a class=\"anchor\" id=\"prepData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba73819",
   "metadata": {},
   "source": [
    "## Load Your Data <a class=\"anchor\" id=\"loadData\"></a>\n",
    "\n",
    "* Where does the data come from?\n",
    "* Are there any ethical concerns with using this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6593f0",
   "metadata": {},
   "source": [
    "## Look At Your Data <a class=\"anchor\" id=\"lookData\"></a>\n",
    "\n",
    "* How many data points are there?\n",
    "* How many variables are there?\n",
    "* What is the type of each variable?\n",
    "* Are the variables independent of each other?\n",
    "* Is there a value for each variable for each data point?\n",
    "* Do the values make sense?\n",
    "\n",
    "Visualizations useful for looking at your data include:\n",
    "* histograms\n",
    "* scatter plots\n",
    "* pair plots\n",
    "\n",
    "It's important that visualizations be:\n",
    "* clear\n",
    "* color blind friendly\n",
    "* labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStatistics(data):\n",
    "    \"Get the max, min, mean, var for each variable in the data.\"\n",
    "    return pd.DataFrame(np.array([data.max(axis=0), data.min(axis=0), data.mean(axis=0), data.var(axis=0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d109eb",
   "metadata": {},
   "source": [
    "## If There Are Missing/Erroneous/Mistyped Values, Clean Your Data <a class=\"anchor\" id=\"cleanData\"></a>\n",
    "\n",
    "If your data is *unordered*:\n",
    "* You can fill in missing values using the mean or median (quantitative variable) or mode (qualitative variable)\n",
    "\n",
    "If your data is *ordered*:\n",
    "* Use interpolation or fill-forward\n",
    "\n",
    "You can also *delete* data points with missing or erroneous values, but be careful.\n",
    "\n",
    "See https://liberalarts.utexas.edu/prc/_files/cs/Missing-Data.pdf for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f2800",
   "metadata": {},
   "source": [
    "## If You Intend to Perform Regression or Classification Experiments, Split Your Data <a class=\"anchor\" id=\"splitData\"></a>\n",
    "\n",
    "* Into train, dev, test\n",
    "* And identify and split off the dependent variable\n",
    "* Make sure that the (labels and) data distributions are the same across train, dev and test\n",
    "  * You may have to use stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3cf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random shuffle\n",
    "def randomSplit(data, depVar, indVars):\n",
    "    np.random.shuffle(data)\n",
    "    train, dev, test = np.split(data, [int(.8 * len(data)), int(.9 * len(data))])\n",
    "    train, trainY = train[np.ix_(np.arange(train.shape[0]), indVars)], train[:, depVar]\n",
    "    dev, devY = dev[np.ix_(np.arange(dev.shape[0]), indVars)], dev[:, depVar]\n",
    "    test, testY = test[np.ix_(np.arange(test.shape[0]), indVars)], test[:, depVar]\n",
    "    return train, dev, test, trainY, devY, testY\n",
    "\n",
    "# Stratified sampling\n",
    "def stratifiedSplit(data, depVar, indVars, numberValsToKeep=-1):\n",
    "    if numberValsToKeep > -1:\n",
    "        byCategory = [data[data[:,-1]==k] for k in np.unique(data[:,-1])][0:numberValsToKeep]\n",
    "    else:\n",
    "        byCategory = [data[data[:,-1]==k] for k in np.unique(data[:,-1])]\n",
    "    print(len(byCategory))\n",
    "\n",
    "    trainFeats = []\n",
    "    devFeats = []\n",
    "    testFeats = []\n",
    "    trainYs = []\n",
    "    devYs = []\n",
    "    testYs = []\n",
    "    for category in byCategory:\n",
    "        train, dev, test = np.split(category, [int(.8 * len(category)), int(.9 * len(category))])\n",
    "        train, trainY = train[np.ix_(np.arange(train.shape[0]), indVars)], train[:, depVar]\n",
    "        dev, devY = dev[np.ix_(np.arange(dev.shape[0]), indVars)], dev[:, depVar]\n",
    "        test, testY = test[np.ix_(np.arange(test.shape[0]), indVars)], test[:, depVar]\n",
    "        trainFeats.append(train)\n",
    "        devFeats.append(dev)\n",
    "        testFeats.append(test)\n",
    "        trainYs.append(trainY)\n",
    "        devYs.append(devY)\n",
    "        testYs.append(testY)\n",
    "    return np.vstack(trainFeats), np.vstack(devFeats), np.vstack(testFeats), np.concatenate(trainYs), np.concatenate(devYs), np.concatenate(testYs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8149c",
   "metadata": {},
   "source": [
    "## Consider Transforming/Normalizing the Data <a class=\"anchor\" id=\"normalizeData\"></a>\n",
    "\n",
    "You might want to transform your data:\n",
    "* in order to visualize it better / more easily - scale, rotate, min-max, zscore\n",
    "* in order to remove the outsize impact of some variables having larger range than others - min-max, zscore\n",
    "* in order to center the data for PCA - translate\n",
    "\n",
    "Types of transformation include:\n",
    "* translation: in 3d, x by $\\alpha$, y by $\\beta$, z by $\\gamma$:\n",
    "  * you need to add a homogenous coordinate to your data - a trailing column of 1s\n",
    "  * the transformation matrix looks like $$\\begin{pmatrix} 1 & 0 & 0 & \\alpha \\\\ 0 & 1 & 0 & \\beta \\\\ 0 & 0 & 1 & \\gamma \\end{pmatrix}$$\n",
    "* scaling: in 3d, x by $\\delta$, y by $\\zeta$, z by $\\kappa$\n",
    "  * the transformation matrix looks like $$\\begin{pmatrix} \\delta & 0 & 0  \\\\ 0 & \\zeta & 0 \\\\ 0 & 0 & \\kappa \\end{pmatrix}$$\n",
    "* rotation: \n",
    "  * in 2d, by $\\theta$ radians: $$\\begin{pmatrix} cos(\\theta) & -sin(\\theta) \\\\ sin(\\theta) & cos(\\theta)  \\end{pmatrix}$$\n",
    "  * in 3d:\n",
    "    * around the x-axis, by $\\theta$ radians: $$\\begin{pmatrix} 1 & 0 & 0  \\\\ 0 & cos(\\theta) & -sin(\\theta) \\\\ 0 & sin(\\theta) & cos(\\theta)  \\end{pmatrix}$$\n",
    "    * around the y-axis, by $\\theta$ radians: $$\\begin{pmatrix} cos(\\theta) & 0 & sin(\\theta)  \\\\ 0 & 1 & 0 \\\\ -sin(\\theta) & 0 & cos(\\theta) \\end{pmatrix}$$\n",
    "    * around the z-axis, by $\\theta$ radians: $$\\begin{pmatrix} cos(\\theta) & -sin(\\theta) & 0 \\\\ sin(\\theta) & cos(\\theta) & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n",
    "\n",
    "and these combinations of translation + scaling:\n",
    "* min-max global: divide each data point by (max-min) \n",
    "* min-max local: divide each data point by (max-min) *for that feature*\n",
    "* zscoring: subtract the per-feature mean from each data point and divide by the per-feature standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogenizeData(data):\n",
    "    return np.append(data, np.array([np.ones(data.shape[0], dtype=float)]).T, axis=1)\n",
    "   \n",
    "# yaw\n",
    "def rotateTransformX(x):\n",
    "    return np.array([1, 0, 0, 0, np.cos(np.radians(x)), -np.sin(np.radians(x)), 0, np.sin(np.radians(x)), np.cos(np.radians(x))]).reshape(3, 3)\n",
    "\n",
    "# pitch\n",
    "def rotateTransformY(y):\n",
    "    return np.array([np.cos(np.radians(y)), 0, np.sin(np.radians(y)), 0, 1, 0, -np.sin(np.radians(y)), 0, np.cos(np.radians(y))]).reshape(3, 3)\n",
    "\n",
    "# roll\n",
    "def rotateTransformZ(z):\n",
    "    return np.array([np.cos(np.radians(z)), -np.sin(np.radians(z)), 0, np.sin(np.radians(z)), np.cos(np.radians(z)), 0, 0, 0, 1]).reshape(3, 3)\n",
    "\n",
    "def minmaxGlobal(data):\n",
    "    \"Global max-min normalization.\"\n",
    "    scaleTransform = np.eye(data.shape[1], data.shape[1])\n",
    "    for i in range(data.shape[1]):\n",
    "        scaleTransform[i, i] = 1 / (data.max() - data.min())\n",
    "    return (scaleTransform@data.T).T\n",
    "\n",
    "def minmaxLocal(data):\n",
    "    \"Local max-min normalization.\"\n",
    "    scaleTransform = np.eye(data.shape[1], data.shape[1])\n",
    "    for i in range(data.shape[1]):\n",
    "        if data[:, i].max() - data[:, i].min() != 0:\n",
    "            scaleTransform[i, i] = 1 / (data[:, i].max() - data[:, i].min())\n",
    "    return (scaleTransform@data.T).T\n",
    "\n",
    "def zScore(data, translateTransform=None, scaleTransform=None):\n",
    "    \"z score.\"\n",
    "    homogenizedData = np.append(data, np.array([np.ones(data.shape[0], dtype=float)]).T, axis=1)\n",
    "    if translateTransform is None:\n",
    "        translateTransform = np.eye(homogenizedData.shape[1])\n",
    "        for i in range(homogenizedData.shape[1]):\n",
    "            translateTransform[i, homogenizedData.shape[1]-1] = -homogenizedData[:, i].mean()\n",
    "    if scaleTransform is None:\n",
    "        diagonal = [1 / homogenizedData[:, i].std() if homogenizedData[:, i].std() != 0 else 1 for i in range(homogenizedData.shape[1])]\n",
    "        scaleTransform = np.eye(homogenizedData.shape[1], dtype=float) * diagonal\n",
    "    data = (scaleTransform@translateTransform@homogenizedData.T).T\n",
    "    return translateTransform, scaleTransform, data[:, :data.shape[1]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8ae19",
   "metadata": {},
   "source": [
    "## Consider Dimensionality Reduction <a class=\"anchor\" id=\"pcaData\"></a>\n",
    "\n",
    "For dimensionality reduction we use principal components analysis (PCA). \n",
    "\n",
    "A PCA model is fit on top of the *covariance matrix* for a dataset. The covariance matrix tells us about the first order relationships between different features. If $A$ is the matrix corresponding to our data set of $N$ data points for each of which we have $M$ features, and $\\bar{A_i}$ is the mean of the $i$th feature, then covariance matrix $C$ is:\n",
    "    $$C_{i,j} = \\sum_{k=1}^N \\frac{(A_{k,i} - \\bar{A_i})(A_{k,j} - \\bar{A_j})}{N-1}$$\n",
    "    \n",
    "The covariance matrix has the variance of each feature along its diagonal, and the remaining entries are the *covariances* of pairs of features, ie how much they vary together. If they vary together, then they are related to each other - some information is shared between them.\n",
    "\n",
    "The steps to fit a PCA are:\n",
    "* (If appropriate) normalize the variables to be in the range 0-1\n",
    "* Center the data\n",
    "* Compute the covariance matrix\n",
    "* Compute the eigenvectors and eigenvalues; the eigenvectors tell us the direction of variance, and the eigenvalues tell us the amount of variance\n",
    "* Get an ordering over the eigenvalues\n",
    "* Sort the eigenvalues and eigenvectors accordingly\n",
    "* Compute the proportional variance (how much bigger?) accounted for by each principal component\n",
    "* Compute the cumulative sum of the proportional variance (tells us how many eigenvectors we need to explain a desired amount of variance)\n",
    "* Examine the principal components. Select v' of them.\n",
    "* Project the data into PCA space\n",
    "* Reconstruct the data\n",
    "\n",
    "Visualizations helpful for examining the covariance matrix are:\n",
    "* heatmap\n",
    "\n",
    "Visualizations useful for examining the output of a PCA to determine how many principal components to keep are:\n",
    "* elbow plot\n",
    "* scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, centered=False, plot=False):\n",
    "        self.eigenvalues = None\n",
    "        self.eigenvectors = None\n",
    "        self.principalComponents = 0\n",
    "        self.centered = centered\n",
    "        self.plot = plot\n",
    "    \n",
    "    def fit(self, data):\n",
    "        # center\n",
    "        if not self.centered:\n",
    "            data = data - np.mean(data, axis=0)\n",
    "\n",
    "        # covariance\n",
    "        covarianceMatrix = (data.T @ data) / (data.shape[0] - 1)\n",
    "        if self.plot:\n",
    "            sns.heatmap(pd.DataFrame(covarianceMatrix), annot=False, cmap='PuOr')\n",
    "            plt.title(\"Covariance Matrix\")\n",
    "            plt.show()\n",
    "\n",
    "        # svd\n",
    "        (evals, evectors) = np.linalg.eig(covarianceMatrix)\n",
    "\n",
    "        # sort\n",
    "        evalsOrder = np.argsort(evals)[::-1]\n",
    "        self.eigenvalues = evals[evalsOrder]\n",
    "        self.eigenvectors = evectors[:, evalsOrder]\n",
    "\n",
    "        # proportional variance\n",
    "        evalsSum = np.sum(self.eigenvalues)\n",
    "        proportionalVars = [e / evalsSum for e in self.eigenvalues]\n",
    "\n",
    "        # cumulative sum of proportional variance\n",
    "        cumulativeSum = np.cumsum(proportionalVars)\n",
    "\n",
    "        if self.plot:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.bar(range(len(proportionalVars)), proportionalVars, alpha=0.5, align='center',\n",
    "                    label='Proportional variance')\n",
    "            plt.xticks(list(range(len(proportionalVars))))\n",
    "            plt.ylabel('Proportional variance ratio')\n",
    "            plt.xlabel('Ranked Principal Components')\n",
    "            plt.title(\"Scree Graph\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(range(len(cumulativeSum)), cumulativeSum)\n",
    "            plt.ylim((0,1.1))\n",
    "            plt.xlabel('Number of Principal Components')\n",
    "            plt.ylabel('Cumulative explained variance')\n",
    "            plt.title('Elbow Plot')\n",
    "            plt.show()\n",
    "\n",
    "    def project(self, data, numberOfComponents):\n",
    "        self.principalComponents = numberOfComponents\n",
    "        # center\n",
    "        if not self.centered:\n",
    "            data = data - np.mean(data, axis=0)\n",
    "        v = self.eigenvectors[:, :numberOfComponents]\n",
    "        projected = data@v\n",
    "        return projected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1daea54",
   "metadata": {},
   "source": [
    "## All Together <a class=\"anchor\" id=\"summaryData\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb34932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLabel(label):\n",
    "    labels = {'SEKER': 0, 'BARBUNYA': 1, 'BOMBAY': 2, 'CALI': 3, 'HOROZ': 4, 'SIRA': 5, 'DERMASON': 6}\n",
    "    return float(labels[str(label)])\n",
    "\n",
    "def prepData(type=\"regression\", depVar=0, fractionToKeep=1.0):\n",
    "    # Load the data\n",
    "    print(\"\\nLoad the data\")\n",
    "    columns = ['area', 'perimeter', 'major_axis_length', 'minor_axis_length', 'aspect_ratio', 'eccentricity', 'convex_area', 'equivalent_diameter', 'extent', 'solidity', 'roundness', 'compactness', 'shapefactor1', 'shapefactor2', 'shapefactor3', 'shapefactor4', 'class']\n",
    "    # This dataset comes from https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset and the data sheet is there\n",
    "    data = np.array(np.genfromtxt('data/Dry_Bean_Dataset.arff', delimiter=',', converters={16: convertLabel}, skip_header=25, dtype=float, encoding='utf-8')) \n",
    "\n",
    "    # Look at the data\n",
    "    print(\"\\nInspect the data\")\n",
    "    print(\"data shape\\n\", data.shape, \"\\ndata type\\n\", data.dtype)\n",
    "    print(\"data max, min, mean, var\\n\", getSummaryStatistics(data))\n",
    "    sns.pairplot(pd.DataFrame(data, columns=columns), y_vars = [\"class\"], x_vars = ['area', 'perimeter', 'major_axis_length', 'minor_axis_length', 'aspect_ratio'], kind = \"scatter\")\n",
    "    sns.pairplot(pd.DataFrame(data, columns=columns), y_vars = [\"class\"], x_vars = ['eccentricity', 'convex_area', 'equivalent_diameter', 'extent', 'solidity'], kind = \"scatter\")\n",
    "    sns.pairplot(pd.DataFrame(data, columns=columns), y_vars = [\"class\"], x_vars = ['roundness', 'compactness', 'shapefactor1', 'shapefactor2', 'shapefactor3', 'shapefactor4'], kind = \"scatter\")\n",
    "    plt.show()\n",
    "\n",
    "    # Clean the data\n",
    "    print(\"\\nClean the data\")\n",
    "    ## No missing values\n",
    "\n",
    "    if depVar > -1:\n",
    "        # Set the dependent and independent variables\n",
    "        indVars = list(range(data.shape[1]))\n",
    "        indVars.pop(depVar)\n",
    "        depName = columns.pop(depVar)\n",
    "        # Split the data, reducing it if necessary\n",
    "        print(\"\\nSplit the data, dependent variable \", depVar, \", to keep \", fractionToKeep)\n",
    "        train, dev, test, trainY, devY, testY = stratifiedSplit(data, depVar, indVars, numberValsToKeep=fractionToKeep)\n",
    "        print(\"training data shape\", \"\\n\", train.shape, \"\\ntraining data max, min, mean, var\\n\", getSummaryStatistics(train))\n",
    "        print(\"\\ndev data shape\", \"\\n\", dev.shape, \"\\ndev data max, min, mean, var\\n\", getSummaryStatistics(dev))\n",
    "        print(\"\\ntest data shape\", \"\\n\", test.shape, \"\\ntest data max, min, mean, var\\n\", getSummaryStatistics(test))\n",
    "        # Transform the data\n",
    "        print(\"\\nTransform the data\")\n",
    "        translateTransform, scaleTransform, trainTransformed = zScore(train)\n",
    "        print(\"training data shape\", \"\\n\", trainTransformed.shape, \"\\ntraining data max, min, mean, var\\n\", getSummaryStatistics(trainTransformed))\n",
    "        _, _, devTransformed = zScore(dev, translateTransform=translateTransform, scaleTransform=scaleTransform)\n",
    "        print(\"\\ndev data shape\", \"\\n\", devTransformed.shape, \"\\ndev data max, min, mean, var\\n\", getSummaryStatistics(devTransformed))\n",
    "        _, _, testTransformed = zScore(test, translateTransform=translateTransform, scaleTransform=scaleTransform)\n",
    "        print(\"\\ntest data shape\", \"\\n\", testTransformed.shape, \"\\ntest data max, min, mean, var\\n\", getSummaryStatistics(testTransformed))\n",
    "        return trainTransformed, devTransformed, testTransformed, trainY, devY, testY\n",
    "    else:\n",
    "        # Reduce the data if necessary\n",
    "        if fractionToKeep < 1.0:\n",
    "            print(\"\\nReducing the data by\", (1.0-fractionToKeep))\n",
    "            data, _ = np.split(data, [int(fractionToKeep*len(data))])\n",
    "        # Transform the data\n",
    "        print(\"\\nTransform the data\")\n",
    "        print(\"data shape\", \"\\n\", data.shape, \"\\ndata max, min, mean, var\\n\", getSummaryStatistics(data))\n",
    "        translateTransform, scaleTransform, transformed = zScore(data)\n",
    "        print(\"data shape\", \"\\n\", transformed.shape, \"\\ndata max, min, mean, var\\n\", getSummaryStatistics(transformed))\n",
    "        return transformed\n",
    "    \n",
    "def projectData(data, principalComponents, *args):\n",
    "    # Dimensionality reduction\n",
    "    print(\"\\nDo PCA on the data, keep principal components\")\n",
    "    pca = PCA(centered=True, plot=True)\n",
    "    pca.fit(data)\n",
    "    projected = pca.project(data, principalComponents)\n",
    "    return projected, [pca.project(arg, principalComponents) for arg in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e59f1d",
   "metadata": {},
   "source": [
    "[Go back to the top](#review)\n",
    "\n",
    "\n",
    "# Model <a class=\"anchor\" id=\"model\"></a>\n",
    "\n",
    "There are several basic types of ML model:\n",
    "* regression\n",
    "* clustering\n",
    "* classification\n",
    "\n",
    "Data is a good fit for:\n",
    "* regression - if the dependent variable is *ordered* - quantitative continuous, quantitative discrete or (at a stretch) qualitative ordinal\n",
    "* clustering - if you have no dependent variable, but want to understand the structure of your data better\n",
    "* classification - if the dependent variable is *qualitative*\n",
    "\n",
    "No matter what modeling approach you use, you will want to define:\n",
    "* a distance metric - between data points\n",
    "* a loss function - something to optimize (typically a loss function is defined in terms of the distance metric)\n",
    "* a way to evaluate, examine or visualize the model output\n",
    "\n",
    "Different model architectures may have additional *hyperparameters*.\n",
    "\n",
    "## Distance Metrics <a class=\"anchor\" id=\"distances\"></a>\n",
    "\n",
    "Commonly used distance metrics are:\n",
    "* Euclidean (L2 norm): $d(\\vec{a}, \\vec{b}) = \\sqrt{\\sum_{i=1}^m (a_i - b_i)^2}$\n",
    "  * We write this as $||\\vec{a} - \\vec{b}||_2$.\n",
    "* Manhattan (L1 norm): $d(\\vec{a}, \\vec{b}) = \\sum_{i=1}^m |a_i - b_i|$\n",
    "  * We write this as $||\\vec{a} - \\vec{b}||_1$.\n",
    "* Chebyshev distance (L$^\\infty$ norm): $d(\\vec{a}, \\vec{b}) = max_i |a_i - b_i|$\n",
    "  * We write this as $||\\vec{a}, \\vec{b}||_\\infty$.\n",
    "* Minkowski distance (L$^p$ norm):  $d(\\vec{a}) = \\left[\n",
    "\\sum_{i=1}^m |a_i|^p\n",
    "\\right]^{1/p}$\n",
    "  * We write this as $||\\vec{a}||_p$.\n",
    "  \n",
    "However, there are a lot more than this: Hamming distance, cosine distance, Levenshtein distance...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeafb4d",
   "metadata": {},
   "source": [
    "## Regression <a class=\"anchor\" id=\"regression\"></a>\n",
    "\n",
    "### Types <a class=\"anchor\" id=\"regressionTypes\"></a>\n",
    "\n",
    "We know three types of regression:\n",
    "* Linear regression - fit a line through 2d data\n",
    "* Multiple linear regression - fit a plane through 3d+ data\n",
    "* Polynomial regression - fit a curve through 2d data\n",
    "  * If you want to do multivariate polynomial regression in pure python, check out https://github.com/sigvaldm/localreg\n",
    "\n",
    "### Distance Metric  <a class=\"anchor\" id=\"regressionDistance\"></a>\n",
    "\n",
    "The standard metric used for regression is Euclidean distance.\n",
    "\n",
    "### Loss Function  <a class=\"anchor\" id=\"regressionLoss\"></a>\n",
    "\n",
    "The loss function is MSSE (mean sum squared error): $$MSSE = 1 / N \\sum_{i=1}^N (r_i)^2 = 1/N \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$. The Euclidean distance is embedded in there.\n",
    "\n",
    "### Methods  <a class=\"anchor\" id=\"regressionMethods\"></a>\n",
    "\n",
    "We know six methods for fitting a regression. All these methods minimize MSSE.\n",
    "\n",
    "#### Least squares <a class=\"anchor\" id=\"leastSquares\"></a>\n",
    "\n",
    "$\\vec{y} = b + m x_i = (1, X_i) \\cdot (b, m)$, or $\\vec{y} = A \\cdot \\vec{c}$\n",
    "\n",
    "This can be computationally expensive to calculate.\n",
    "\n",
    "#### Normal equation <a class=\"anchor\" id=\"normalEquation\"></a>\n",
    "\n",
    "The normal equation is $ \\vec{c} = (A^TA)^{-1} A^T\\vec{y}$.\n",
    "\n",
    "If $A^TA$ is not invertible we can't use the normal equation. Use one of the other methods, or remove some variables that are not independent of each other.\n",
    "\n",
    "Calculating $A^TA$ is computationally expensive.\n",
    "\n",
    "#### QR decomposition <a class=\"anchor\" id=\"qrDecomposition\"></a>\n",
    "\n",
    "$R \\vec{c} = Q^T \\vec{y}  $, where $A = QR$\n",
    "* $Q$ is orthonormal, which means that $Q^TQ = I$, which means that $Q^T = Q^{-1}$.\n",
    "* We calculate $Q$ using Gram-Schmidt decomposition.\n",
    "* $R$ is upper triangle, so we can use back substitution to solve.\n",
    "\n",
    "If $A$ does not have linearly independent columns, QR decomposition won't work.\n",
    "\n",
    "#### Gradient descent <a class=\"anchor\" id=\"gradientDescent\"></a>\n",
    "\n",
    "Unlike the other methods, which are analytical, this is an approximation method.\n",
    "\n",
    "This method is an iterative step by step method. We take a step, figure out if we are going towards the *minimum loss* and if so we take another step. \n",
    "* At first we pick random values for $m$ and $b$ (our slope and intercept).\n",
    "* At each step we:\n",
    "  1. calculate the partial derivative of $MSSE$ with respect to $m$ and to $b$, and plug in $x$, $y$, $m$ and $b$.\n",
    "    * partial derivative of $MSSE$ ($p$) with respect to $m$: $$\\frac{\\partial p}{\\partial m} = 1/N \\sum_{i=1}^N \\frac{\\partial }{\\partial m} (y_i - \\hat{y}_i)^2 = 1/N \\sum_{i=1}^N \\frac{\\partial }{\\partial m} (y_i - (m x_i + b))^2 = 1 / N \\sum_{i=1}^N -2 x_i (y_i - (m x_i + b)) = -2 / N \\sum_{i=1}^N x_i (y_i - \\hat{y}_i)$$\n",
    "    * partial derivative of $MSSE$ ($p$) with respect to $b$: $$\\frac{\\partial p}{\\partial b} = 1/N \\sum_{i=1}^N \\frac{\\partial }{\\partial b} (y_i - \\hat{y}_i)^2 = 1/N \\sum_{i=1}^N \\frac{\\partial }{\\partial b} (y_i - (m x_i + b))^2 = 1 / N \\sum_{i=1}^N -2 (y_i - (m x_i + b)) = -2 / N \\sum_{i=1}^N (y_i - \\hat{y}_i)$$\n",
    "  2. update $m$ and $b$ as follows:\n",
    "    * $m = m - lr * \\frac{\\partial p}{\\partial m}$\n",
    "    * $b = b - lr * \\frac{\\partial p}{\\partial b}$\n",
    "    where $lr$ is a *learning rate* set, hopefully, big enough that we don't have to step forever, and small enough that we don't overshoot our goal (the minimum loss).\n",
    "* We stop after a preset number of steps (*epochs*) or after the change in loss stops getting smaller.\n",
    "\n",
    "#### Stepwise regression <a class=\"anchor\" id=\"stepwiseRegression\"></a>\n",
    "\n",
    "This method can be used with the others. It is useful when you have many features and you want to know which ones work.\n",
    "\n",
    "For stepwise regression, we use ${R^2}_{adj} = 1 - \\frac{(1-R^2)(N-1)}{N-k-1}$\n",
    "where $N$ is the number of variables, and $k$ is the number of variables in $A$.\n",
    "\n",
    "Stepwise regression works like this:\n",
    "1. Initialize $A$ to be just the leading column of 1s (because we know we will have an intercept).\n",
    "2. Then while the improvements in ${R^2}_{adj}$ are > 0 and there remain independent variables not yet added:\n",
    "  * calculate a regression using $A$ and each variable not yet in $A$, and \n",
    "  * add the one with the highest ${R^2}_{adj}$ to $A$.\n",
    "  \n",
    "#### RBF networks <a class=\"anchor\" id=\"regressionRBF\"></a>\n",
    "\n",
    "See RBF networks, below.\n",
    "\n",
    "### Evaluation and Visualization  <a class=\"anchor\" id=\"regressionEvaluation\"></a>\n",
    "\n",
    "The evaluation metric is $R^2$: $R^2 = 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat{y})^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2} = 1 - \\frac{r_i^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2}$. This measures the goodness of fit of the regression.\n",
    "\n",
    "### Hyperparameters  <a class=\"anchor\" id=\"regressionHyperparameters\"></a>\n",
    "\n",
    "The hyperparameter for regression is the polynomial degree of the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.slopesIntercept = None\n",
    "    \n",
    "    def fitlstsq(self, data, y):\n",
    "        \"Fit a linear regression using least squares.\"\n",
    "        # We add a column of 1s for the intercept\n",
    "        A = np.hstack((np.array([np.ones(data.shape[0])]).T, data))\n",
    "        # This is the regression coefficients that were fit, plus some other results\n",
    "        self.slopesIntercept, _, _, _ = sp_la.lstsq(A, y)\n",
    "\n",
    "    def fitnorm(self, data, y):\n",
    "        \"Fit a linear regression using the normal equation.\"\n",
    "        # We add a column of 1s for the intercept\n",
    "        A = np.hstack((np.array([np.ones(data.shape[0])]).T, data))\n",
    "        # This is the regression coefficients that were fit, plus some other results\n",
    "        self.slopesIntercept = sp_la.inv(A.T.dot(A)).dot(A.T).dot(y)\n",
    "\n",
    "    def fitqr(self, data, y):\n",
    "        \"Fit a linear regression using QR decomposition.\"\n",
    "        # We add a column of 1s for the intercept\n",
    "        A = np.hstack((np.array([np.ones(data.shape[0])]).T, data))\n",
    "        # This is the regression coefficients that were fit, plus some other results\n",
    "        Q, R = sp_la.qr(A)\n",
    "        self.slopesIntercept = sp_la.solve_triangular(R, Q.T.dot(y))\n",
    "\n",
    "    def gradient_descent(self, data, y, lr, epochs):\n",
    "        \"Fit a linear regression using gradient descent.\"\n",
    "        # initialize m and b\n",
    "        rng = default_rng()\n",
    "        # possibly this should not be fixed at 2\n",
    "        self.slopesIntercept = rng.standard_normal(2)\n",
    "        A = np.hstack((np.array([np.ones(data.shape[0])]).T, data))\n",
    "        for i in range(epochs):\n",
    "            yhat = np.dot(A, self.slopesIntercept)\n",
    "            if self.msse(y, yhat) < 0.00001:\n",
    "                return\n",
    "            # fill in the partial derivatives\n",
    "            dpdm = (-2/n) * np.sum(x * (y - yhat))\n",
    "            dpdb = (-2/n) * np.sum(y - yhat)\n",
    "            # update c\n",
    "            self.slopesIntercept = self.slopesIntercept - np.array([lr * dpdm, lr * dpdb])\n",
    "    \n",
    "    def msse(self, y, yhat):\n",
    "        \"Calculate MSSE.\"\n",
    "        if len(y) != len(yhat):\n",
    "            print(\"Need y and yhat to be the same length!\")\n",
    "            return 0\n",
    "        return (1 / len(y)) * (((y - yhat())**2).sum())\n",
    "\n",
    "    def rsquared(self, y, yhat):\n",
    "        \"Calculate R^2.\"\n",
    "        if len(y) != len(yhat):\n",
    "            print(\"Need y and yhat to be the same length!\")\n",
    "            return 0\n",
    "        return 1 - (((y - yhat)**2).sum() / ((y - y.mean())**2).sum())\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"Given a linear regression function, predict on new data.\"\n",
    "        # We add a column of 1s for the intercept\n",
    "        A = np.hstack((np.array([np.ones(data.shape[0])]).T, data))\n",
    "        return np.dot(A, self.slopesIntercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression:\n",
    "    \"Polynomial regression with one independent variable only.\"\n",
    "    def __init__(self):\n",
    "        self.poly = 0\n",
    "        self.slopesIntercept = None\n",
    "    \n",
    "    def makePoly(self, data):\n",
    "        A = np.zeros([data.shape[0], self.poly+1])\n",
    "        for i in range(0, self.poly+1):\n",
    "            A[:, i] = np.squeeze(data**i)\n",
    "        print(getSummaryStatistics(A))\n",
    "        print(getShapeType(A))\n",
    "        return A\n",
    "\n",
    "    def fitlstsq(self, data, y, poly):\n",
    "        self.poly = poly\n",
    "        # We add a column of 1s for the intercept\n",
    "        A = makePoly(data)\n",
    "        # This is the regression coefficients that were fit, plus some other results\n",
    "        self.slopesIntercept, _, _, _ = sp_la.lstsq(A, y)\n",
    "\n",
    "    def predict(self, data):\n",
    "        # We add a column of 1s for the intercept\n",
    "        A = makePoly(data, self.poly)\n",
    "        return np.dot(A, self.slopesIntercept)\n",
    "    \n",
    "    def rsquared(self, y, yhat):\n",
    "        \"Calculate R^2.\"\n",
    "        if len(y) != len(yhat):\n",
    "            print(\"Need y and yhat to be the same length!\")\n",
    "            return 0\n",
    "        return 1 - (((y - yhat)**2).sum() / ((y - y.mean())**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753788fa",
   "metadata": {},
   "source": [
    "[Go back to the top](#review)\n",
    "\n",
    "### A Worked Example <a class=\"anchor\" id=\"regressionExample\"></a>\n",
    "\n",
    "Why would we not fit a regression on the raw data?\n",
    "\n",
    "Which is better, fitting a linear regression on the transformed data or on the projected data?\n",
    "\n",
    "Which is more efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTrain, transformedDev, transformedTest, trainY, devY, testY = prepData(type=\"regression\", depVar=0, fractionToKeep=-1)\n",
    "projectedTrain, projectedDevTest = projectData(transformedTrain, 8, transformedDev, transformedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Linear regression on the transformed data\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fitlstsq(transformedTrain, trainY)\n",
    "print(lr.rsquared(lr.predict(transformedDev), devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Linear regression on the dimensionality-reduced, projected data\n",
    "\n",
    "lr.fitlstsq(projectedTrain, trainY)\n",
    "print(lr.rsquared(lr.predict(projectedDevTest[0]), devY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5923b90",
   "metadata": {},
   "source": [
    "[Go back to the top](#review)\n",
    "\n",
    "## Clustering <a class=\"anchor\" id=\"clustering\"></a>\n",
    "\n",
    "Clustering algorithms separate a data set into groups, or clusters, that are near each other (similar to each other) using a distance metric we choose or define.\n",
    "\n",
    "We can use clustering methods for data exploration and for data compression.\n",
    "\n",
    "### Distance Metric <a class=\"anchor\" id=\"clusteringDistance\"></a>\n",
    "\n",
    "If you have used PCA to do dimensionality reduction on your data, you should use Euclidean distance.\n",
    "\n",
    "Otherwise, you should choose a distance metric relevant to your data. \n",
    "\n",
    "### Loss Function  <a class=\"anchor\" id=\"clusteringLoss\"></a>\n",
    "\n",
    "For k-means clustering, we minimize *inertia*:\n",
    "$inertia = 1/N \\sum_{j=1}^N d(\\vec{x_j}, \\vec{m_{\\vec{x_j}}})^2$, where $\\vec{m_{x_j}}$ is the centroid of the cluster that $x_j$ is currently assigned to, and $d$ is your chosen distance metric.\n",
    "\n",
    "### Methods  <a class=\"anchor\" id=\"clusteringMethods\"></a>\n",
    "\n",
    "We know one method for clustering.\n",
    "\n",
    "For a list of lots of clustering algorithms, see https://scikit-learn.org/stable/modules/clustering.html.\n",
    "\n",
    "#### K-means clustering <a class=\"anchor\" id=\"kmeans\"></a>\n",
    "\n",
    "To make this computationally efficient, we calculate an approximate solution by iteration:\n",
    "1. Pick initial centroids.\n",
    "  * You may pick them at random.\n",
    "  * Or you may use k-means++:\n",
    "    1. Choose a single random data point as the first centroid, $\\vec{c_1}$.\n",
    "    2. Repeat til $k$\n",
    "      1. Calculate the distance between each data point $\\vec{x_i}$ and its nearest previously chosen centroid $\\vec{c_i}$.\n",
    "      2. Pick the next centroid according to $p(\\vec{x_i}) = \\frac{d(\\vec{x_i}, \\vec{c_i})}{\\sum_{j=1}^N d(\\vec{x_j}, \\vec{c_i})}$. (In python, you can get the index of this next centroid using index = np.random.choice([0, 1, ..., N], p=$p(x_i)$). You can also square the distances in the numerator and denominator above, to spread the distances out further.)\n",
    "2. For step in range(max_steps):\n",
    "  1. Assign each point to its closest centroid.\n",
    "  2. Pick new centroids using the members in each cluster. If the centroids don't change, then return.\n",
    "  \n",
    "This method is guaranteed to converge as at each round the inertia will decrease or stay the same, and if it stays the same you halt.\n",
    "\n",
    "It is important to normalize your data before k-means clustering, or the variable with the widest range will dominate.\n",
    "\n",
    "### Evaluation and Visualization  <a class=\"anchor\" id=\"clusteringEvaluation\"></a>\n",
    "\n",
    "An elbow plot of inertia over k is useful for choosing k.\n",
    "\n",
    "A clustering is really useful insofar as it tells you something interesting about your data. However, there are ways of assessing the \"goodness\" of a clustering, including:\n",
    "* Daviesâ€“Bouldin index\n",
    "* Dunn index\n",
    "* Silhouette coefficient\n",
    "\n",
    "### Hyperparameters  <a class=\"anchor\" id=\"clusteringHyperparameters\"></a>\n",
    "\n",
    "The hyperparameters for k-means clustering include:\n",
    "* the distance metric\n",
    "* k\n",
    "* the method for initializing centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176464eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self):\n",
    "        self.centroids = None\n",
    "        self.clusters = None\n",
    "        self.k = None\n",
    "\n",
    "    # Let's define a distance metric; which one is this??\n",
    "    def distance(self, a, b):\n",
    "        subtracted = a-b\n",
    "        return np.sqrt(np.dot(subtracted.T, subtracted))\n",
    "\n",
    "    # Let's define a function to calculate the distance from each data point to each centroid\n",
    "    def getDistances(self, item):\n",
    "        return [self.distance(item, centroid) for centroid in self.centroids]\n",
    "\n",
    "    # Let's define a function to update cluster assignments given a set of centroids\n",
    "    def updateClusters(self, data):\n",
    "        self.clusters = [np.argmin(self.getDistances(item)) for item in data]\n",
    "\n",
    "    # Let's define a function to update the centroids\n",
    "    def updateCentroids(self, data):\n",
    "        withClusters = np.hstack((data, np.array([self.clusters]).T))\n",
    "        indices = np.argsort(withClusters[:, -1])\n",
    "        withClustersSorted = withClusters[indices]\n",
    "        byCluster = np.array_split(withClustersSorted, np.where(np.diff(withClustersSorted[:, -1])!=0)[0]+1)\n",
    "        self.centroids = np.array([np.mean(cluster[:, :-1], axis=0) for cluster in byCluster])\n",
    "\n",
    "    # Let's define a function to measure the inertia\n",
    "    def inertia(self, data):\n",
    "        sumSquares = 0\n",
    "        for i in range(len(data)):\n",
    "            sumSquares += self.distance(data[i], self.centroids[self.clusters[i]])**2\n",
    "        return sumSquares / len(data)\n",
    "    \n",
    "    def pickInitialCentroids(self, data, k, random=True):\n",
    "        if random:\n",
    "            self.centroids = np.array([data[x] for x in np.random.choice(np.arange(len(data)), size=k, replace=False)])\n",
    "        else:\n",
    "            self.centroids = [data[np.random.randint(len(data))]]\n",
    "            while len(self.centroids) < k:\n",
    "                leastDistance = [np.min(self.getDistances(datum))**2 for datum in data]\n",
    "                p = leastDistance / np.sum(leastDistance)\n",
    "                self.centroids.append(data[np.random.choice(np.arange(len(leastDistance)),p=p)])\n",
    "                    \n",
    "    def fitExplore(self, data, minK, maxK, byK, random=True):\n",
    "        inertiaByK = []\n",
    "        for k in range(minK, maxK, byK):\n",
    "            print(k)\n",
    "            thisInertia = self.fit(data, k, random=random)\n",
    "            inertiaByK.append([k, thisInertia])\n",
    "        fig = plt.figure(figsize=(6,4))\n",
    "        inertiaByK = np.array(inertiaByK)\n",
    "        plt.plot(inertiaByK[:, 0], inertiaByK[:, 1])\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.title('Elbow Plot')\n",
    "        plt.show()\n",
    "            \n",
    "    def fit(self, data, k, random=False):\n",
    "        self.k = k\n",
    "        self.pickInitialCentroids(data, k, random=random)\n",
    "        self.updateClusters(data)\n",
    "        thisInertia = self.inertia(data)\n",
    "        lastInertia = thisInertia + 1\n",
    "        while abs(lastInertia - thisInertia) > 0.01:\n",
    "            lastInertia = thisInertia\n",
    "            self.updateCentroids(data)\n",
    "            self.updateClusters(data)\n",
    "            thisInertia = self.inertia(data)\n",
    "        return thisInertia\n",
    "    \n",
    "    def plot(self, data, labels=None):\n",
    "        if labels is not None:\n",
    "            sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=self.clusters, style=labels)\n",
    "            plt.show()\n",
    "        else:\n",
    "            sns.scatterplot(data=data, x=data[:, 0], y=data[:, 1], hue=self.clusters)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd3aab",
   "metadata": {},
   "source": [
    "[Go back to the top](#review)\n",
    "\n",
    "## A Worked Example <a class=\"anchor\" id=\"clusteringExample\"></a>\n",
    "\n",
    "Why would we not fit a k-means clustering on the raw data?\n",
    "\n",
    "Which is better, fitting a k-means clustering on the transformed data or on the projected data?\n",
    "\n",
    "Which is more efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a082b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedData = prepData(type=\"clustering\", depVar=-1, fractionToKeep=.24)\n",
    "projectedData, _ = projectData(transformedData, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47271d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Clustering on the transformed data, choose k\n",
    "\n",
    "km = KMeans()\n",
    "km.fitExplore(transformedData, 2, 40, 2, random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Clustering on the transformed data, best k\n",
    "\n",
    "km.fit(transformedData, 16, random=False)\n",
    "km.plot(transformedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fa16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Clustering on the PCA projected data, choose k\n",
    "\n",
    "km = KMeans()\n",
    "km.fitExplore(projectedData, 2, 40, 2, random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Clustering on the PCA projected data, best k\n",
    "\n",
    "km.fit(projectedData, 10, random=False)\n",
    "km.plot(projectedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03afa8f",
   "metadata": {},
   "source": [
    "[Go back to the top](#review)\n",
    "\n",
    "# Classification <a class=\"anchor\" id=\"classification\"></a>\n",
    "\n",
    "Classification algorithms fit a prediction function that can be used to assign a new data point to one of a fixed set of classes.\n",
    "\n",
    "We know two types of classification:\n",
    "* binary - there are only two classes\n",
    "* multiclass - there are three or more classes\n",
    "\n",
    "### Distance Metric <a class=\"anchor\" id=\"classificationDistance\"></a>\n",
    "\n",
    "The distance metric is typically Euclidean distance (but it need not be).\n",
    "\n",
    "### Loss Function <a class=\"anchor\" id=\"classificationLoss\"></a>\n",
    "\n",
    "The loss function varies by classification model architecture (method).\n",
    "\n",
    "### Methods <a class=\"anchor\" id=\"classificationMethods\"></a>\n",
    "\n",
    "We know three methods for classification.\n",
    "\n",
    "#### K-nearest neighbors <a class=\"anchor\" id=\"knn\"></a>\n",
    "\n",
    "K-nearest neighbors (KNNs) is a very very simple supervised ML algorithm.\n",
    "* fit - it just stores all the training data!\n",
    "* predict - it finds the data points in the training data that are closest to the data point to be classified, and takes a majority vote of their labels.\n",
    "\n",
    "Loss function for k-nearest-neighbors:\n",
    "* None. kNN doesn't optimize anything, just stores the training data.\n",
    "\n",
    "Hyperparameters for k-nearest-neighbors:\n",
    "* distance metric\n",
    "* k\n",
    "* whether to use k-means as preprocessing and then calculate: a) nearest centroid and b) k-nearest-neighbors on elements of cluster defined by nearest centroid\n",
    "\n",
    "Visualizations that may be helpful:\n",
    "* elbow plot for choosing k\n",
    "\n",
    "#### Naive Bayes <a class=\"anchor\" id=\"nb\"></a>\n",
    "\n",
    "Naive Bayes relies on Bayes rule: $P(Y|X) = \\frac{P(Y)*P(X|Y)}{P(X)}$.\n",
    "* $P(Y|X)$ is the *posterior*\n",
    "* $P(Y)$ is the *prior*; we approximate this using $\\frac{|Y|}{\\sum_Y |Y|}$\n",
    "* $P(X|Y)$ is the *likelihood*; we approximate this using $\\frac{|X\\&Y|}{|Y|}$\n",
    "* $P(X)$ is the *evidence* (or *normalization*); we don't need to calculate or approximate this in Naive Bayes since in Naive Bayes we are comparing the posteriors for multiple outcomes and regardless of outcome the denominator is the same\n",
    "\n",
    "The reason we call it \"naive\" is because we assume that each independent variable is independent of each other independent variable, so we can use the chain rule to calculate $P(Y|X\\&Z)$: $P(Y|X\\&Z) = P(Y|X)*P(Y|Z)$.\n",
    "\n",
    "To fit a Naive Bayes model:\n",
    "* Walk over the data and compute a counts table. Each row will correspond to one value for the dependent variable, and each column to one value for an independent variable. Be sure and include row and column totals.\n",
    "* Walk over the table and calculate:\n",
    "  * the likelihoods: dividing each value by the total for that row (for that outcome)\n",
    "  * the priors: dividing the total for each row by the total number of data points\n",
    "\n",
    "To predict, for each possible variable of the dependent variable, look up the relevant precalculated likelihood and prior and multiply them. Return the value corresponding to the most likely posterior.\n",
    "\n",
    "Loss function for Naive Bayes:\n",
    "* None. Basic Naive Bayes doesn't optimize anything, just calculates frequencies over the training data.\n",
    "\n",
    "Hyperparameters for Naive Bayes:\n",
    "* Whether or not to smooth the counts of the independent variables to account for the possibility of seeing some new value for an independent variable in the dev or test data. We used Laplace smoothing: for $x_i \\in X$:\n",
    "  * instead of $P(Y|x_i) \\sim \\frac{|x_i \\& Y|}{|Y|}$, \n",
    "  * we use $P(Y|x_i) \\sim \\frac{|x_i \\& Y|+1}{|Y| + |x_i|}$. \n",
    "* Whether to store raw likelihoods/priors or logs of likelihoods/priors (useful if there are many values for independent variables).\n",
    "* Whether to use the counts directly, or to use a probability distribution (e.g. Gaussian, Multinomial, Bernoulli) over the data.\n",
    "\n",
    "Visualizations that may be helpful:\n",
    "* the counts and likelihoods tables\n",
    "* confusion matrix (per class)\n",
    "* ROC curve\n",
    "\n",
    "#### RBF networks <a class=\"anchor\" id=\"classificationRBF\"></a>\n",
    "\n",
    "See RBF networks, below.\n",
    "\n",
    "### Evaluation and Visualization <a class=\"anchor\" id=\"classificationEvaluation\"></a>\n",
    "\n",
    "For classification, metrics we may report include:\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* F1\n",
    "\n",
    "We often display (or plot, as a heat map) a confusion matrix. A confusion matrix shows the true positives, true negatives, false positives and false negatives comparing two classes:\n",
    "* for binary classification, there are only two classes\n",
    "* for multiclass classification, confusion matrices may be constructed in one of two ways: one versus others (per class), or one versus one (pairs of classes)\n",
    "\n",
    "Once we have calculated the confusion matrix, we can easily calculate the true positive rate and false negative rate, and then plot the receiver operating characteristic (ROC) curve and calculate the area under that curve (AUC).\n",
    "\n",
    "### Hyperparameters <a class=\"anchor\" id=\"classificationHyperparameters\"></a>\n",
    "\n",
    "The hyperparameters for classification vary by method; see above.\n",
    "\n",
    "#### Probability Distributions <a class=\"anchor\" id=\"classificationProbabilityDistributions\"></a>\n",
    "\n",
    "It's important to be able to recognize and think about which probability distribution matches your data.\n",
    "* Gaussian (normal) - probability density function is $f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$, where $\\mu$ is the mean and $\\sigma$ is the standard deviation of the data (https://mathworld.wolfram.com/NormalDistribution.html).\n",
    "![normal distribution](https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg)\n",
    "* Bernoulli - probability density function is $p^n(1-p)^{1-n}$ where $p$ is the probability that an outcome $n=1$ occurs and $1-p$ is the probability that outcome $n=0$ occurs (https://mathworld.wolfram.com/BernoulliDistribution.html).\n",
    "* Binomial - probability density function is $P_p(n|N) = \\binom{N}{n} p^n q^{N-n} = \\frac{N!}{n!(N-n)!}p^n(1-p)^{N-n}$, where $n$ is the number of successes out of $N$ trials, and the result of each trial is true with probability $p$ and false with probability $q=1-p$, and $\\binom{N}{n}$ is a binomial coefficient (https://mathworld.wolfram.com/BinomialDistribution.html).\n",
    "![normal vs binomial distributions](https://upload.wikimedia.org/wikipedia/commons/b/b7/Binomial_Distribution.svg)\n",
    "* Multinomial - the extension of the binomial distribution to the case of multiple random variables (https://mathworld.wolfram.com/MultinomialDistribution.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, elements):\n",
    "        return np.array([self.predictOne(element) for element in elements])\n",
    "    \n",
    "    def accuracy(self, y, yhat):\n",
    "        assert len(y) == len(yhat)\n",
    "        diffs = y == yhat\n",
    "        vals, counts = np.unique(diffs, return_counts=True)\n",
    "        return (counts[np.where(vals == True)] / (np.sum(counts)))[0]\n",
    "    \n",
    "    def confusionMatrix(self, y, yhat):\n",
    "        \"Thanks to https://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python\"\n",
    "        labelMappings = dict(zip(np.unique(self.labels), range(len(np.unique(self.labels))))) \n",
    "        assert len(y) == len(yhat)\n",
    "        result = np.zeros((len(labelMappings), len(labelMappings)))\n",
    "        for i in range(len(y)):\n",
    "            result[labelMappings[y[i]]][labelMappings[yhat[i]]] += 1\n",
    "        return result\n",
    "    \n",
    "    def aucRoc(self, y, yhat):\n",
    "        \"Only works for binary classification\"\n",
    "        auc = metrics.roc_auc_score(y, yhat, multi_class=\"ovr\")\n",
    "        FPR, TPR, thresholds = metrics.roc_curve(y, yhat)\n",
    "        plt.figure(figsize=(10, 8), dpi=100)\n",
    "        plt.axis('scaled')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.title(\"AUC & ROC Curve\")\n",
    "        plt.plot(FPR, TPR, 'g')\n",
    "        plt.fill_between(FPR, TPR, facecolor='lightgreen', alpha=0.7)\n",
    "        plt.text(0.95, 0.05, 'AUC = %0.4f' % auc, ha='right', fontsize=12, weight='bold', color='blue')\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.show()\n",
    "\n",
    "    def score(self, elements, labels):\n",
    "        predictedLabels = self.predict(elements)\n",
    "        return self.accuracy(labels, predictedLabels), self.confusionMatrix(labels, predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433463af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(Classifier):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.labels = None\n",
    "        self.k = 2\n",
    "        \n",
    "    def fitExplore(self, train, trainY, dev, devY, minK, maxK, byK):\n",
    "        accuracyByK = []\n",
    "        for k in range(minK, maxK, byK):\n",
    "            print(k)\n",
    "            self.fit(train, trainY, k)\n",
    "            devYhat = self.predict(dev)\n",
    "            accuracyByK.append([k, self.accuracy(devY, devYhat)])\n",
    "        fig = plt.figure(figsize=(6,4))\n",
    "        accuracyByK = np.array(accuracyByK)\n",
    "        plt.plot(accuracyByK[:, 0], accuracyByK[:, 1])\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Elbow Plot')\n",
    "        plt.show()\n",
    "\n",
    "    # Let's define fit for kNN\n",
    "    def fit(self, data, labels, k, distanceMetric=None):\n",
    "        self.model = data\n",
    "        self.labels = labels\n",
    "        assert len(self.model) == len(self.labels)\n",
    "        self.k = k\n",
    "        if distanceMetric != None:\n",
    "            self.distanceMetric = distanceMetric\n",
    "        else:\n",
    "        # Let's define a distance metric; which one is this??\n",
    "            self.distanceMetric = lambda a, b: np.sqrt(np.dot((a-b).T, a-b))\n",
    "\n",
    "    # Let's define predict for kNN\n",
    "    def predictOne(self, element):\n",
    "        neighborsByDistance = np.argpartition([self.distanceMetric(element, datapoint) for datapoint in self.model], self.k)\n",
    "        neighborLabels = [self.labels[neighborsByDistance[x]] for x in range(self.k)]\n",
    "        vals, counts = np.unique(neighborLabels, return_counts=True)\n",
    "        return vals[np.argwhere(counts == np.max(counts))][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310441d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesQualitative(Classifier):\n",
    "    \"This only works for data where the independent variables are qualitative.\"\n",
    "    def __init__(self):\n",
    "        self.likelihoods = None\n",
    "        self.priors = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def fit(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.likelihoods = []\n",
    "        data = np.insert(data, data.shape[1], labels, axis=1)\n",
    "        cats = sorted(np.unique(labels))\n",
    "        for cat in cats:\n",
    "            likelihoodsCat = []\n",
    "            for feature in range(data.shape[1]-1):\n",
    "                values = sorted(np.unique(data[:, feature]))\n",
    "                print(\"cat\", cat, \"values\", values)\n",
    "                for value in values:\n",
    "                    print(value, len(data[np.where((data[:, feature] == value) & (data[:, -1] == cat))]), len(data[np.where(data[:, -1] == cat)]))\n",
    "                    likelihoodsCat.append(np.log(len(data[np.where((data[:, feature] == value) & (data[:, -1] == cat))]) + 1) / (len(data[np.where(data[:, -1] == cat)]) + len(values)))\n",
    "            self.likelihoods.append(likelihoodsCat)\n",
    "        self.priors = [np.log(len(data[np.where(data[:, -1] == cat)]) / len(data)) for cat in cats]\n",
    "        print(self.likelihoods)\n",
    "        print(self.priors)\n",
    "\n",
    "    def predictOne(self, datum):\n",
    "        res = []\n",
    "        for j, cat in enumerate(self.likelihoods):\n",
    "            product = 1\n",
    "            for i, value in enumerate(datum):\n",
    "                try:\n",
    "                    likelihood = cat[i*value]\n",
    "                except:\n",
    "                    print(\"feature value \" + value + \" is not in likelihoods, returning most frequent category!\")\n",
    "                    likelihood = 1\n",
    "                product = product*likelihood\n",
    "            res.append(self.priors[j]*product)\n",
    "        return np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68421bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesQuantitative(Classifier):\n",
    "    \"This only works for data where the independent variables are quantitative.\"\n",
    "    def __init__(self):\n",
    "        self.likelihoods = None\n",
    "        self.priors = None\n",
    "        self.labels = None\n",
    "        \n",
    "    def fit(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.likelihoods = []\n",
    "        data = np.insert(data, data.shape[1], labels, axis=1)\n",
    "        cats = sorted(np.unique(labels))\n",
    "        for cat in cats:\n",
    "            likelihoodsCat = []\n",
    "            for feature in range(data.shape[1]):\n",
    "                subData = data[np.where(data[:, -1] == cat)][:, feature]\n",
    "                likelihoodsCat.append([subData.mean(), subData.std()])\n",
    "            self.likelihoods.append(likelihoodsCat)\n",
    "        self.priors = [len(data[np.where(data[:, -1] == cat)]) / len(data) for cat in cats]\n",
    "\n",
    "    def normalPDF(self, x, mean, stdev):\n",
    "        exp = math.exp(-(1/2)*(math.pow((x-mean)/stdev,2)))\n",
    "        return (1 / (math.sqrt(2*math.pi) * stdev)) * exp\n",
    "\n",
    "    def predictOne(self, datum):\n",
    "        res = []\n",
    "        for j, cat in enumerate(self.likelihoods):\n",
    "            product = 1\n",
    "            for i, value in enumerate(datum):\n",
    "                product = product*self.normalPDF(value, cat[i][0], cat[i][1])\n",
    "            res.append(self.priors[j]*product)\n",
    "        return np.argmax(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3671227",
   "metadata": {},
   "source": [
    "[Go back to the top](#review)\n",
    "\n",
    "## A Worked Example <a class=\"anchor\" id=\"classificationExample\"></a>\n",
    "\n",
    "Why would we not fit a classification model on the raw data?\n",
    "\n",
    "Which is better, fitting a model on the transformed data or on the projected data?\n",
    "\n",
    "Which classification method is \"better\"? Which is more time efficient? Which is more space efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTrain, transformedDev, transformedTest, trainY, devY, testY = prepData(type=\"classification\", depVar=16, fractionToKeep=2)\n",
    "projectedTrain, projectedDevTest = projectData(transformedTrain, 5, transformedDev, transformedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit a kNN to the transformed train data, choose k using dev data; this is hyperparameter tuning\n",
    "\n",
    "knn = KNN()\n",
    "knn.fitExplore(transformedTrain, trainY, transformedDev, devY, 2, 40, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02208570",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit a kNN to the transformed train data, best k, test using test data\n",
    "\n",
    "knn.fit(transformedTrain, trainY, 4)\n",
    "testYhat = knn.predict(transformedTest)\n",
    "print(knn.accuracy(testYhat, testY))\n",
    "print(knn.confusionMatrix(testYhat, testY))\n",
    "knn.aucRoc(testYhat, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4629f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit a kNN to the PCA projected train data, choose k using dev data; this is hyperparameter tuning\n",
    "\n",
    "knn = KNN()\n",
    "knn.fitExplore(projectedTrain, trainY, projectedDevTest[0], devY, 2, 40, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit a kNN to the PCA projected train data, best k, test using test data\n",
    "\n",
    "knn.fit(projectedTrain, trainY, 4)\n",
    "test_yhat = knn.predict(projectedDevTest[1])\n",
    "print(knn.accuracy(testYhat, testY))\n",
    "print(knn.confusionMatrix(testYhat, testY))\n",
    "knn.aucRoc(testYhat, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit a naive Bayes model to the transformed train data, test using test data\n",
    "\n",
    "print(transformedTrain.shape)\n",
    "nb = NaiveBayesQuantitative()\n",
    "nb.fit(transformedTrain, trainY)\n",
    "test_yhat = nb.predict(transformedTest)\n",
    "print(nb.accuracy(testYhat, testY))\n",
    "print(nb.confusionMatrix(testYhat, testY))\n",
    "nb.aucRoc(testYhat, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de243dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit a naive Bayes model to the PCA projected train data, test using test data\n",
    "\n",
    "print(projectedTrain.shape)\n",
    "nb = NaiveBayesQuantitative()\n",
    "nb.fit(projectedTrain, trainY)\n",
    "test_yhat = nb.predict(projectedDevTest[1])\n",
    "print(nb.accuracy(testYhat, testY))\n",
    "print(nb.confusionMatrix(testYhat, testY))\n",
    "nb.aucRoc(testYhat, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a4c3e",
   "metadata": {},
   "source": [
    "[Go back to the top](#review)\n",
    "\n",
    "# RBF Networks <a class=\"anchor\" id=\"rbfNetworks\"></a>\n",
    "\n",
    "A radial basis function network is a neural network that uses a radial basis function (kernel) as the activation function on hidden layer nodes. A radial basis function is \"a real-valued function whose value depends only on the distance between the input and some fixed point\" (https://en.wikipedia.org/wiki/Radial_basis_function).\n",
    "\n",
    "Training a RBF consists of:\n",
    "* Finding prototypes\n",
    "* Selecting the activation function for the hidden nodes\n",
    "  * Any radial basis function will do; we use a Gaussian, $exp \\left( - \\frac{||\\vec{d}-\\vec{\\mu_j}||^2}{2\\sigma_j^2 + \\epsilon} \\right)$, where $\\vec{d}$ is the data point, $\\vec{\\mu_j}$ is the prototype, $\\sigma_j$ is the hidden unit's standard deviation, $\\epsilon$ is a small constant and $||.||^2$ is the squared Euclidean distance. We calculate $\\sigma_j$ as the average distance from any training data point in the $j$th cluster to the centroid of the $j$th cluster. \n",
    "* Selecting the activation function for the output nodes\n",
    "  * We use a simple weighted sum of the activations of the hidden nodes\n",
    "* Setting the weights for the edges and biases\n",
    "\n",
    "We use:\n",
    "* kmeans to find prototypes\n",
    "* linear regression to fit the edge weights from the hidden layer to the output layer\n",
    "\n",
    "We can use RBF networks for:\n",
    "* classification - by taking the absolute rounded value of the argmax of the outputs from the output layer (one output node per class)\n",
    "* regression - by taking the output from the one node in the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFNetwork:\n",
    "    def __init__(self, type='regression'):\n",
    "        self.prototypes = None\n",
    "        self.clusters = None\n",
    "        self.epsilon = 1e-8\n",
    "        self.prototypeStandardDeviations = None\n",
    "        self.regression = None\n",
    "        self.type = type\n",
    "        \n",
    "    def explorePrototypes(self, data, minK, maxK, byK):\n",
    "        km = KMeans()\n",
    "        km.fitExplore(data, minK, maxK, byK, random=False)\n",
    "        \n",
    "    def fitPrototypes(self, data, k, epsilon=1e-8):\n",
    "        km = KMeans()\n",
    "        km.fit(data, k, random=False)\n",
    "        self.prototypes = km.centroids\n",
    "        self.clusters = km.clusters\n",
    "        self.calculatePrototypeStandardDeviations(data, epsilon=epsilon)\n",
    "\n",
    "    def calculateActivation(self, datum, prototypeIndex):\n",
    "        return np.exp(-np.sum(np.square(datum-self.prototypes[prototypeIndex])) / (2*self.prototypeStandardDeviations[prototypeIndex]**2 + self.epsilon))\n",
    "    \n",
    "    def calculateActivations(self, datum):\n",
    "        return np.array([self.calculateActivation(datum, prototypeIndex) for prototypeIndex in range(len(self.prototypes))])\n",
    "    \n",
    "    def calculateStandardDeviation(self, cluster, prototypeIndex):\n",
    "        cluster = cluster[:, :-1]\n",
    "        return (1/len(cluster))*np.sum([np.linalg.norm(datum-self.prototypes[prototypeIndex]) for datum in cluster])\n",
    "            \n",
    "    def calculatePrototypeStandardDeviations(self, data, epsilon=1e-8):\n",
    "        self.epsilon = epsilon\n",
    "        with_clusters = np.hstack((data, np.array([self.clusters]).T))\n",
    "        indices = np.argsort(with_clusters[:, -1])\n",
    "        with_clusters_sorted = with_clusters[indices]\n",
    "        self.prototypeStandardDeviations = [self.calculateStandardDeviation(x, i) for i, x in enumerate(np.array_split(with_clusters_sorted, np.where(np.diff(with_clusters_sorted[:, -1])!=0)[0]+1))]\n",
    "                        \n",
    "    def fitClassification(self, data, labels):\n",
    "        allYs = []\n",
    "        for value in np.unique(labels):\n",
    "            allYs.append([1 if x == value else 0 for x in labels])\n",
    "        y = np.vstack(allYs).T\n",
    "        self.regression = LinearRegression()\n",
    "        self.regression.fitlstsq(data, y)\n",
    "                        \n",
    "    def fitRegression(self, data, labels):\n",
    "        self.regression = LinearRegression()\n",
    "        self.regression.fitlstsq(data, labels)\n",
    "                        \n",
    "    def fitOutputNodes(self, data, labels):\n",
    "        activations = np.array([self.calculateActivations(datum) for datum in data])\n",
    "        if self.type == 'regression':\n",
    "            self.fitRegression(activations, labels)\n",
    "        else:\n",
    "            self.fitClassification(activations, labels)  \n",
    "      \n",
    "    def fit(self, data, labels, k, epsilon=1e-8):\n",
    "        self.fitPrototypes(data, k, epsilon=epsilon)\n",
    "        self.fitOutputNodes(data, labels)\n",
    "\n",
    "    def predict(self, data):\n",
    "        activations = np.array([self.calculateActivations(datum) for datum in data])\n",
    "        predictions = self.regression.predict(activations)\n",
    "        if self.type == 'regression':\n",
    "            return predictions\n",
    "        else:\n",
    "            return [np.argmax(x) for x in predictions]\n",
    "\n",
    "    def accuracy(self, y, yhat):\n",
    "        return np.sum([1 if y[i]==yhat[i] else 0 for i in range(len(y))]) / len(y)\n",
    "\n",
    "    def rsquared(self, y, yhat):\n",
    "        return 1 - (((y - yhat)**2).sum() / ((y - y.mean())**2).sum())\n",
    " \n",
    "    def score(self, y, yhat):\n",
    "        if self.type == 'regression':\n",
    "            return self.rsquared(y, yhat)\n",
    "        else:\n",
    "            return self.accuracy(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68e75f",
   "metadata": {},
   "source": [
    "## A Worked Example for Regression <a class=\"anchor\" id=\"rbfRegression\"></a>\n",
    "\n",
    "Which is better, fitting a model on the transformed data or on the projected data?\n",
    "\n",
    "Which is more accurate, linear regression (see earlier) or regression via RBF network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTrain, transformedDev, transformedTest, trainY, devY, testY = prepData(type=\"regression\", depVar=0, fractionToKeep=-1)\n",
    "projectedTrain, projectedDevTest = projectData(transformedTrain, 8, transformedDev, transformedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get the number of prototypes; this is hyperparameter tuning\n",
    "\n",
    "rbf = RBFNetwork(type=\"regression\")\n",
    "rbf.explorePrototypes(transformedTrain, 2, 40, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f00d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rbf.fit(transformedTrain, trainY, 36)\n",
    "yhat = rbf.predict(transformedDev)\n",
    "rbf.score(devY, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get the number of prototypes; this is hyperparameter tuning\n",
    "\n",
    "rbf = RBFNetwork(type=\"regression\")\n",
    "rbf.explorePrototypes(projectedTrain, 2, 40, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rbf.fit(projectedTrain, trainY, 36)\n",
    "yhat = rbf.predict(projectedDevTest[0])\n",
    "rbf.score(devY, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b7bd4d",
   "metadata": {},
   "source": [
    "## A Worked Example for Classification <a class=\"anchor\" id=\"rbfClassification\"></a>\n",
    "\n",
    "Which is better, fitting a model on the transformed data or on the projected data?\n",
    "\n",
    "Which is most accurate, kNN classification, Naive Bayes classification, or classification via RBF network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0968e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTrain, transformedDev, transformedTest, trainY, devY, testY = prepData(type=\"classification\", depVar=16, fractionToKeep=2)\n",
    "projectedTrain, projectedDevTest = projectData(transformedTrain, 5, transformedDev, transformedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d438cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get the number of prototypes; this is hyperparameter tuning\n",
    "\n",
    "rbf = RBFNetwork(type=\"classification\")\n",
    "rbf.explorePrototypes(transformedTrain, 2, 40, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rbf.fit(transformedTrain, trainY, 36)\n",
    "yhat = rbf.predict(transformedDev)\n",
    "rbf.score(devY, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get the number of prototypes; this is hyperparameter tuning\n",
    "\n",
    "rbf = RBFNetwork(type=\"classification\")\n",
    "rbf.explorePrototypes(projectedTrain, 2, 40, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rbf.fit(projectedTrain, trainY, 36)\n",
    "yhat = rbf.predict(projectedDevTest[0])\n",
    "rbf.score(devY, yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
