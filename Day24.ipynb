{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5e78268",
   "metadata": {},
   "source": [
    "# The One Goal For Today\n",
    "\n",
    "To understand k nearest neighbors as a machine learning algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "482c7d82",
   "metadata": {},
   "source": [
    "# Let's talk about machine learning\n",
    "\n",
    "The phrase \"machine learning\" refers to any method for approximating a solution to a problem for which we don't have an analytical solution (an algorithmic solution) through examining data. The basic taxonomy of machine learning approaches is depicted below:\n",
    "\n",
    "![ML algorithms](https://blogs.sas.com/content/subconsciousmusings/files/2017/04/machine-learning-cheet-sheet-2.png)\n",
    "\n",
    "*Image from https://blogs.sas.com/content/subconsciousmusings/*\n",
    "\n",
    "However, this diagram does not include a third major class of ML algorithm, reinforcement learning, which has been used (among many other applications!) to develop ChatGPT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f4a8497",
   "metadata": {},
   "source": [
    "When we talk about machine learning, we talk about:\n",
    "* *fitting* (or *training*) a *prediction function*, or *model*, to\n",
    "* *training* data, experimenting with various\n",
    "* *hyperparameters* related to the *model architecture* using held-out\n",
    "* *development* data, so that the resulting model generalizes well, making good *predictions* on held-out\n",
    "* *test* (or *evaluation*) data\n",
    "\n",
    "The goal of unsupervised learning is to uncover latent structure or patterns in the data. An example of unsupervised learning is k-means clustering. The goal of supervised learning is to learn to match the labels (or answers, or ground truth, or dependent variable) in the data. An example of supervised learning is regression. \n",
    "\n",
    "We will spend the rest of the semester looking at three supervised ML algorithms:\n",
    "* k-nearest neighbors\n",
    "* naive Bayes\n",
    "* radial basis function networks (a type of neural network)\n",
    "\n",
    "If you want to investigate ML further, here is a great python library for ML:\n",
    "* scikit-learn (sklearn): https://scikit-learn.org/stable/index.html\n",
    "(you will use sklearn for Lab 6)\n",
    "\n",
    "Sklearn uses a pattern; for each ML algorithm, there is a fit function (for training), a predict function (for inference or testing), and a score function (for evaluation).\n",
    "\n",
    "We evaluate models / prediction functions using [any number of metrics](https://scikit-learn.org/stable/modules/model_evaluation.html). A commonly used one for supervised machine learning is:\n",
    "* accuracy - what percent of the data points were classified correctly?\n",
    "\n",
    "Of course, accuracy is just one number. To get a clearer understanding, we can construct a\n",
    "* confusion matrix\n",
    "\n",
    "which has the classes (the labels) along rows and columns, and in each cell indicates the number of data points classified as *row* that are truly in class *column*. \n",
    "\n",
    "We will look more at confusion matrices later this week."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "416d7d61",
   "metadata": {},
   "source": [
    "# K-nearest neighbors\n",
    "\n",
    "K-nearest neighbors (KNNs) is a very very simple supervised ML algorithm.\n",
    "* fit - it just stores all the training data!\n",
    "* predict - it finds the data points in the training data that are closest to the data point to be classified, and takes a majority vote of their labels\n",
    "\n",
    "*closest to* means it needs a distance function.\n",
    "\n",
    "Let's implement k-nearest neighbors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utilityfunctions as uf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d364fb0b",
   "metadata": {},
   "source": [
    "Your tasks:\n",
    "1. Complete any line that has a ??\n",
    "2. Comment every line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56377191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over euclidean distance from Friday\n",
    "def distance(a, b):\n",
    "    ??\n",
    "\n",
    "# \"fits\" a model to the data\n",
    "def fit_knn(data, labels, k):\n",
    "    assert len(data) == len(labels)\n",
    "    # \"store\" or return the model which is the combination of data, labels and k\n",
    "    # see predict_one_knn for what it should look like\n",
    "    ??\n",
    "\n",
    "# predict the label for one datapoint\n",
    "def predict_one_knn(element, model):\n",
    "    training_data = model[0]\n",
    "    labels = model[1]\n",
    "    k = model[2]\n",
    "    # let's look up this argpartition thing\n",
    "    neighbors_by_distance = np.argpartition([distance(element, datapoint) for datapoint in training_data], k)\n",
    "    print(\"neighbors by distance: \", neighbors_by_distance)\n",
    "    neighbor_labels = [labels[neighbors_by_distance[x]] for x in range(k)]\n",
    "    vals, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "    print(\"neighbor labels by counts: \", vals, counts)\n",
    "    return vals[np.argwhere(counts == np.max(counts))][0,0]\n",
    "\n",
    "# predict the label for a set of data points\n",
    "def predict_knn(data, model):\n",
    "    return np.array([predict_one_knn(datapoint, model) for datapoint in data])\n",
    "    \n",
    "# calculate accuracy given actual labels y and predicted labels yhat\n",
    "def accuracy(y, yhat):\n",
    "    assert len(y) == len(yhat)\n",
    "    diffs = ??\n",
    "    vals, counts = np.unique(diffs, return_counts=True)\n",
    "    return (counts[np.where(vals == True)] / (np.sum(counts)))[0]\n",
    "\n",
    "# score a model using test data\n",
    "def score(model, testing_data, test_labels):\n",
    "    predicted_labels = ??\n",
    "    return accuracy(test_labels, predicted_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e7b3967",
   "metadata": {},
   "source": [
    "## Let's get some data!\n",
    "\n",
    "The data set we wil be analyzing is a dataset of car logos from https://github.com/GeneralBlockchain/vehicle-logos-dataset. I converted each logo to greyscale and downscaled them to a consistent size. I also converted the dependent variable (manufacturer name) to an int; it is the last column.\n",
    "\n",
    "We last used this dataset on day 19.\n",
    "\n",
    "Your tasks:\n",
    "1. Explain why we shuffle the data before splitting into train and test.\n",
    "2. Explain why we split one of the variables off from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015105c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = np.array(np.genfromtxt('data/logos.csv', delimiter=',', dtype=int))  \n",
    "\n",
    "# shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# split the data into train and test\n",
    "(train, test) = np.split(data, [int(len(data) / 10 * 8)])\n",
    "print(\"train, test: \", train.shape, test.shape)\n",
    "\n",
    "# separate the dependent variable from the independent variables\n",
    "y_train = train[:, -1]\n",
    "x_train = train[:, 0:-1]\n",
    "y_test = test[:, -1]\n",
    "x_test = test[:, 0:-1]\n",
    "print(\"train, test without labels: \",x_train.shape, x_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d229134",
   "metadata": {},
   "source": [
    "## Let's train and evaluate a kNN model on this data!\n",
    "\n",
    "Your task is to fill in the lines that contain ??.\n",
    "\n",
    "First, we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# use a k of 3\n",
    "model = ??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2607e6cb",
   "metadata": {},
   "source": [
    "Then, we test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# call score\n",
    "??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ecec23f",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "\n",
    "*What are some of the hyperparameters related to kNN?*\n",
    "\n",
    "*How would you pick good values for them?*\n",
    "\n",
    "kNN is expensive in terms of computational cost and memory. You can address each of these costs using methods *we already know*.\n",
    "* *To make it faster, what can you do?*\n",
    "* *To make it less memory intensive, what can you do?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
