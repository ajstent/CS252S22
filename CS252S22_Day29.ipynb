{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45ff9b",
   "metadata": {},
   "source": [
    "# Naive Bayes for Text, Multi-Class\n",
    "\n",
    "Text data, *without preprocessing*, is qualitative data. Let's use Naive Bayes to classify some text data! Today's data  has more than two classes, so this is multi-class classification rather than binary classification. \n",
    "\n",
    "I'm going to be using the news dataset from [here](https://data.world/elenadata/vox-articles). Side note: this data set was released for a workshop in 2017 that I co-organized!\n",
    "\n",
    "## I. Load and Look at our data\n",
    "\n",
    "Let's load and __look at our data__. Where is the dependent variable?\n",
    "\n",
    "This data is big, so I zipped it. Let's look at the first five lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('data/dsjVoxArticles.zip') as z:\n",
    "    with z.open('dsjVoxArticles.tsv', 'r') as tsv:\n",
    "        lines = [next(tsv) for x in range(5)]\n",
    "        print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a961d",
   "metadata": {},
   "source": [
    "For efficiency, I'm going to ignore the article bodies and just use the titles. (They would need quite a bit of preprocessing anyway since they contain markup.) So I want the first and third fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc38d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with ZipFile('data/dsjVoxArticles.zip') as z:\n",
    "    with z.open('dsjVoxArticles.tsv', 'r') as tsv:\n",
    "        for line in tsv:\n",
    "            cols = line.decode('utf-8').strip().split('\\t')[:3]\n",
    "            data.append([cols[0], cols[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac87e3d",
   "metadata": {},
   "source": [
    "Let's make this into a numpy array and take a look.\n",
    "* How many data points?\n",
    "* How many classes?\n",
    "* What are the classes, anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad1c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "print(data.shape)\n",
    "print(np.unique(data[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91f9eb",
   "metadata": {},
   "source": [
    "Well, that's too many classes, and some of them are super specific. Let's just take five pretty generic classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = data[np.where(np.isin(data[:, 1], ['Business & Finance', 'Health Care', 'Science & Health', 'Politics & Policy', 'Criminal Justice']))]\n",
    "np.random.shuffle(reduced_data)\n",
    "print(reduced_data.shape)\n",
    "print(np.unique(reduced_data[:, 1], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeed082",
   "metadata": {},
   "source": [
    "## II. Split the data\n",
    "\n",
    "Let's split the data into train, dev and test. \n",
    "\n",
    "When we check by printing shapes and unique values, does everything look okay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d9135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data, test_data = np.split(reduced_data, [int(.8 * len(reduced_data)), int(.9 * len(reduced_data))])\n",
    "print(train_data.shape, dev_data.shape, test_data.shape)\n",
    "print(np.unique(train_data[:, 1]), np.unique(dev_data[:, 1]), np.unique(test_data[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df312415",
   "metadata": {},
   "source": [
    "## III. Preprocess the data\n",
    "\n",
    "On Monday we tokenized the data and extracted counts for each token for each class ourselves.\n",
    "\n",
    "Today I'm going to use two scikit-learn utilities:\n",
    "* CountVectorizer - will tokenize and count\n",
    "* LabelEncoder - will map the string labels to ints\n",
    "\n",
    "As on Monday, I use *only the training data* to extract my token vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb157178",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, analyzer='word', max_features=1000)\n",
    "\n",
    "vectorizer.fit(iter(train_data[:, 0]))\n",
    "# We have to use np.asarray because sklearn 1.0 doesn't want matrices for naive Bayes\n",
    "train_processed = np.asarray(vectorizer.transform(iter(train_data[:, 0])).todense())\n",
    "dev_processed = np.asarray(vectorizer.transform(iter(dev_data[:, 0])).todense())\n",
    "test_processed = np.asarray(vectorizer.transform(iter(test_data[:, 0])).todense())\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_data[:, 1])\n",
    "train_labels = encoder.transform(train_data[:, 1])\n",
    "dev_labels = encoder.transform(dev_data[:, 1])\n",
    "test_labels = encoder.transform(test_data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7da5cf",
   "metadata": {},
   "source": [
    "## IV. Fit, Predict and Score\n",
    "\n",
    "Today I'm going to compare the performance of several scikit-learn Naive Bayes alternatives on this dataset. If you recall from last week, these variations on Naive Bayes model different *probability distributions* over the training data, rather than using the likelihoods and priors directly.\n",
    "\n",
    "Although we aren't using our own, hand-written Naive Bayes, you can see that the pattern is the same:\n",
    "1. Fit\n",
    "2. Predict\n",
    "3. Score\n",
    "\n",
    "With respect to \"score\", you'll see we are calculating:\n",
    "* precision\n",
    "* recall\n",
    "* F1\n",
    "\n",
    "*per class*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(train_processed, train_labels)\n",
    "pred = nb.predict(dev_processed)\n",
    "print(classification_report(dev_labels, pred, target_names=encoder.classes_))\n",
    "print(confusion_matrix(dev_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(train_processed, train_labels)\n",
    "pred = nb.predict(dev_processed)\n",
    "print(classification_report(dev_labels, pred, target_names=encoder.classes_))\n",
    "print(confusion_matrix(dev_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "nb.fit(train_processed, train_labels)\n",
    "pred = nb.predict(dev_processed)\n",
    "print(classification_report(dev_labels, pred, target_names=encoder.classes_))\n",
    "print(confusion_matrix(dev_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6379664",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(train_processed, train_labels)\n",
    "pred = nb.predict(dev_processed)\n",
    "print(classification_report(dev_labels, pred, target_names=encoder.classes_))\n",
    "print(confusion_matrix(dev_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae74196",
   "metadata": {},
   "source": [
    "## V. Questions\n",
    "\n",
    "1. What are the definitions for precision, recall and F1, and how do they relate to the confusion matrix?\n",
    "2. What do \"macro avg\" and \"weighted avg\" mean?\n",
    "3. Which variant of Naive Bayes works the best on this data?\n",
    "4. Is there a class that is consistently miscategorized regardless of method?\n",
    "5. Which metric or way of analyzing the results makes the most sense to you? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
