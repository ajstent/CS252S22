{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "639f9fcb",
   "metadata": {},
   "source": [
    "# The One Goal for Today\n",
    "\n",
    "To implement a Naive Bayes model using python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71f10b3a",
   "metadata": {},
   "source": [
    "# Bayes' Rule reviewed \n",
    "\n",
    "$P(Y|X) = \\frac{P(Y)*P(X|Y)}{P(X)}$. \n",
    "* $P(Y|X)$ is the *posterior*\n",
    "* $P(Y)$ is the *prior*\n",
    "* $P(X|Y)$ is the *likelihood*\n",
    "* $P(X)$ is the *evidence* (or *normalization*)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "857c4a9b",
   "metadata": {},
   "source": [
    "# Fitting and predicting using Naive Bayes\n",
    "\n",
    "Let's imagine I have an *infinite* bag of M&Ms. Some of them are peanut, some raisin and some chocolate. I only like peanut M&Ms. The thing is, I'm in a dark room so I can only guess the type of the M&M by feel. I repeatedly reach into the bag, grab one M&M and then either eat it or do not eat it. Unlike Wednesday, you record the length of the M&M as well as the type. Because numpy won't let me have arrays of heterogeneous types, I'm going to express length and width as 's', 'm' or 'l'. This gives the dataset below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64ce0c4f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| Type | Length | Outcome |\n",
    "| ----- | ---- | --- |\n",
    "| peanut M&M (N) | l | ate it (E) |\n",
    "| peanut M&M (N) | l | ate it (E) |\n",
    "| peanut M&M (N) | l | ate it (E) |\n",
    "| raisin M&M (R) | m | did not eat it (D) |\n",
    "| raisin M&M (R) | l | ate it (E) |\n",
    "| chocolate M&M (C) | s | did not eat it (D) |\n",
    "| peanut M&M (N) | m | did not eat it (D) |\n",
    "| chocolate M&M (C) | s | did not eat it (D) |\n",
    "| raisin M&M (R) | s | did not eat it (D) |\n",
    "| raisin M&M (R) | s | did not eat it (D) |\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70007ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60098a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    ['N', 'l', 'E'], \n",
    "    ['N', 'l', 'E'], \n",
    "    ['N', 'l', 'E'], \n",
    "    ['R', 'm', 'D'], \n",
    "    ['R', 'l', 'E'], \n",
    "    ['C', 's', 'D'], \n",
    "    ['N', 'm', 'D'], \n",
    "    ['C', 's', 'D'], \n",
    "    ['R', 's', 'D'], \n",
    "    ['R', 's', 'D']])\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2948b",
   "metadata": {},
   "source": [
    "Side note: we typically convert qualitative values to numbers for machine learning; it enables us to use all the power of numpy, at some cost in readability to humans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77064e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features to numbers\n",
    "\n",
    "def convert(data):\n",
    "    mappings = []\n",
    "    for col in range(data.shape[1]):\n",
    "        values = sorted(np.unique(data[:, col]))\n",
    "        mappings.append(values)\n",
    "        for row in range(data.shape[0]):\n",
    "            data[row, col] = values.index(data[row, col])\n",
    "    return data.astype(int), mappings\n",
    "\n",
    "def convert_one(datum, mappings):\n",
    "    return [mappings[i].index(x) for i, x in enumerate(datum)]\n",
    "\n",
    "data, mappings = convert(data)\n",
    "print(data, \"\\n\", mappings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "588bee68",
   "metadata": {},
   "source": [
    "We want to fit a model that will tell us the probability that I ate it (or did not eat it) given the type of candy it is *and* its length. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e69cd989",
   "metadata": {},
   "source": [
    "## Conditional independence\n",
    "\n",
    "When we fit a Naive Bayes model across multiple independent features, we assume those features are *conditionally independent*, given $Y$. In other words, the effect of the value of a feature  on the label is independent of the values of other features.\n",
    "\n",
    "The Naive Bayes formula in this case, where $X = {x_1, x_2, ...}$ looks like:\n",
    "$P(Y|X) = argmax_Y P(Y)*P(X|Y) = argmax_Y P(Y)*\\prod_i{P(x_i|Y)}$.\n",
    "\n",
    "We can do the product of $P(x_i|Y)$ because of this *conditional independence*. This *conditional independence* assumption is the \"naive\" in Naive Bayes.\n",
    "\n",
    "Looking at the data, is the length feature *really* independent of the type feature?\n",
    "* How many large, medium and small peanut M&Ms are there?\n",
    "* How many large, medium and small raisin M&Ms are there?\n",
    "* How many large, medium and small regular M&Ms are there?\n",
    "\n",
    "Even if the independent variables aren't really conditionally independent, Naive Bayes is still a surprisingly strong baseline model for many tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ffabd86",
   "metadata": {},
   "source": [
    "## Missing feature values\n",
    "\n",
    "Let's make a table of counts, like before:\n",
    "\n",
    "| Y | N | C | R | l | m | s |\n",
    "| -------- | ------- | ---------- | ------------ | -- | -- | -- |\n",
    "| E         | 3 | 0 | 1 | 4 | 0 | 0 |\n",
    "| D         | 1 | 2 | 3 | 0 | 2 | 4 |\n",
    "| Total     | 4 | 2 | 4 | 4 | 2 | 4 |\n",
    "\n",
    "Ok, the fact that there are 0s in this table is a problem. It means that if we ever *do* (during testing or inference) see, for example, a small peanut M&M, $P(s | E)$ will \"zero out\" the total even though the peanutness should lend towards edibility. So we use __Laplace smoothing__: for $x_i \\in X$:\n",
    "* instead of $P(Y|x_i) \\sim \\frac{|x_i \\& Y|}{|Y|}$, \n",
    "* we use $P(Y|x_i) \\sim \\frac{|x_i \\& Y|+1}{|Y| + |x_i|}$. \n",
    "\n",
    "In this way, there's some (small!) likelihood for every value of each independent variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42ec8ed9",
   "metadata": {},
   "source": [
    "## Fit\n",
    "\n",
    "* Calculate the prior for E and the prior for D: $P(E) = 4/10$; $P(D) = 6/10$\n",
    "\n",
    "* Calculate the likelihood of each of type of M&M given E and given D. For example, the likelihood of N given E = (3 (peanut M&Ms that were eaten) + 1 (smoothing)) / (4 (ate it) + 3 (unique feature values)). The likelihood of R given D = (3 (raisin M&Ms that were not eaten) + 1 (smoothing)) / (6 (did not eat it) + 3 (unique feature values)).\n",
    " \n",
    "| Y | N | C | R | l | m | s |\n",
    "| -------- | ------- | ---------- | ------------ | -- | -- | -- |\n",
    "| E         | 4/7 | 1/7 | 2/7 | 5/7 | 1/7 | 1/7 |\n",
    "| D         | 2/9 | 3/9 | 4/9 | 1/9 | 3/9 | 5/9 |\n",
    "\n",
    "Store both sets of values.\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    # we will use this to store the likelihoods\n",
    "    likelihoods = []\n",
    "    # cats is all the unique values in the last column of the data; all the unique values for the dependent variable\n",
    "    cats = sorted(np.unique(data[:, -1]))\n",
    "    for cat in cats:\n",
    "        # likelihoods_cat is the likelihoods for this value of the dependent variable\n",
    "        likelihoods_cat = []\n",
    "        # for each feature \n",
    "        for feature in range(data.shape[1]-1):\n",
    "            # values is the value of this feature\n",
    "            values = sorted(np.unique(data[:, feature]))\n",
    "            for value in values:\n",
    "                # add P(value|cat) to likelihoods_cat; don't forget to use Laplace smoothing!\n",
    "                likelihoods_cat.append((len(data[np.where((data[:, feature] == value) & (data[:, -1] == cat))]) + 1) / (len(data[np.where(data[:, -1] == cat)]) + len(values)))\n",
    "        likelihoods.append(likelihoods_cat)\n",
    "    # calculate P(cat) for each cat\n",
    "    priors = [len(data[np.where(data[:, -1] == cat)]) / len(data) for cat in cats]\n",
    "    # a naive Bayes model is a set of likelihoods and a set of priors; we don't need to worry about the evidence because it's always the same\n",
    "    return likelihoods, priors\n",
    "\n",
    "likelihoods, priors = fit(data)\n",
    "print(likelihoods, priors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f36dac31",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Given a new observation, *large raisin M&M*, what is my most likely behavior?\n",
    "\n",
    "* Compare $P(E|l, R)$ and $P(D|l, R)$. \n",
    "\n",
    "Calculate $argmax_Y P(Y)*\\prod_i{P(x_i|Y)}$:\n",
    "* Y=E: $4/10*5/7*2/7$\n",
    "* Y=D: $6/10*4/9*1/9$\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c12b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(datum, likelihoods, priors, mappings):\n",
    "    # we will use this to store the probability of each outcome\n",
    "    res = []\n",
    "    datum = convert_one(datum, mappings)\n",
    "    print(datum)\n",
    "    # for each possible outcome cat\n",
    "    for j, cat in enumerate(likelihoods):\n",
    "        product = 1\n",
    "        # for each possible feature value\n",
    "        for i, value in enumerate(datum):\n",
    "            try:\n",
    "                # get P(value|cat)\n",
    "                # the bug was here!!\n",
    "                likelihood = cat[i*len(mappings[i])+value]\n",
    "            except:\n",
    "                print(\"feature value \" + value + \" is not in likelihoods, returning most frequent category!\")\n",
    "                likelihood = 1\n",
    "            print(cat, value, likelihood)\n",
    "            # all the independent variables are assumed to be conditionally independent of each other\n",
    "            product = product*likelihood\n",
    "        # multiply the likelihood by the prior for cat\n",
    "        res.append(priors[j]*product)\n",
    "    print(res)\n",
    "    # return the most likely outcome\n",
    "    return mappings[-1][np.argmax(res)]\n",
    "\n",
    "def predict(data, likelihoods, priors, mappings):\n",
    "    return np.array([predict_one(datum, likelihoods, priors, mappings) for datum in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cada2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_one(['R', 'l'], likelihoods, priors, mappings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec1e76b",
   "metadata": {},
   "source": [
    "### Score\n",
    "\n",
    "Am I doing better with one more feature? Let's see! Here's my augmented test data:\n",
    "\n",
    "| Candy | Length | Outcome |\n",
    "| ----- | ---- | --- |\n",
    "| peanut M&M (N) | s | did not eat it (D) |\n",
    "| peanut M&M (N) | l | ate it (E) |\n",
    "| raisin M&M (R) | s | did not eat it (D) |\n",
    "| raisin M&M (R) | l | ate it (E) |\n",
    "| chocolate M&M (C) | m | did not eat it (D) |\n",
    "\n",
    "Figuring out the posteriors using the priors and likelihoods:\n",
    "| Y | x1 | x2| P(E)\\*P(x1\\|E)\\*P(x2\\|E) | P(D)\\*P(x1\\|D)\\*P(x2\\|D) | $\\hat{Y}$ |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| D | N | s | .0326 |  .07 |  D |\n",
    "| E | N | l | .163 | .015  | E  |\n",
    "| D | R | s | .016 | .148  | D  |\n",
    "| E | R | l | .082 |  .030 | E  |\n",
    "| D | C | m | .008 | .067  | D  |\n",
    "\n",
    "Based on this test data:\n",
    "* What is the accuracy?\n",
    "* What is the confusion matrix?\n",
    "\n",
    "| Guess/Truth | E | D |\n",
    "| -- | --- | ---- |\n",
    "| E | 2 |0 |\n",
    "| D | 0 | 3 |\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4991ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([['N', 's', 'D'], ['N', 'l', 'E'], ['R', 's', 'D'], ['R', 'l', 'E'], ['C', 'm', 'D']])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc438dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predict(test[:, :-1], likelihoods, priors, mappings)\n",
    "print(test[:, -1], yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22576e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from day 24\n",
    "def accuracy(y, yhat):\n",
    "    assert len(y) == len(yhat)\n",
    "    diffs = y == yhat\n",
    "    vals, counts = np.unique(diffs, return_counts=True)\n",
    "    return counts[np.where(vals == True)] / (np.sum(counts))\n",
    "    \n",
    "def confusion_matrix(y, yhat, cats):\n",
    "    \"Thanks to https://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python\"\n",
    "    assert len(y) == len(yhat)\n",
    "    # make a matrix of all zeros\n",
    "    result = np.zeros((len(cats), len(cats)))\n",
    "    # update the confusion matrix for each pair of y, yhat\n",
    "    for i in range(len(y)):\n",
    "        result[cats.index(y[i])][cats.index(yhat[i])] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d62322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(test[:, -1], yhat))\n",
    "print(confusion_matrix(test[:, -1], yhat, mappings[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9128296",
   "metadata": {},
   "source": [
    "## How can I get more robust estimates?\n",
    "\n",
    "Right now, our estimates are literally count-and-divide (and smooth!) on our training data. Can we do better?\n",
    "\n",
    "Yes, we can, but not without doing some *approximation*.\n",
    "\n",
    "If we __look at our data__, in particular if we plot it, we can often identify the data distribution. For example, the data may follow a Gaussian distribution. Then instead of counting-and-dividing the *fit* can be to use the training data to fit that distribution. This may be more robust (give better generalization) than just count-and-divide.\n",
    "\n",
    "For more information please refer to the scikit-learn documentation link below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20493214",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn?msclkid=515ef324bb4111ec8b8708b8e0e41101\n",
    "* https://scikit-learn.org/stable/modules/naive_bayes.html?msclkid=2a4b7189bb4111ec80c80f36bbb9a228"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
