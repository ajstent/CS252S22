{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16737ff",
   "metadata": {},
   "source": [
    "# RBF Networks for Clustering\n",
    "\n",
    "\n",
    "What if we don't have labels, and we just want to uncover structure in the data?\n",
    "\n",
    "Well of course we can do that using k-means clustering, but we can also do it using a biologically-inspired method called Hebbian learning that in some ways resembles RBF networks.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "This whole lecture is courtesy of Oliver Layton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4591c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e59f1d",
   "metadata": {},
   "source": [
    "# Do Hebbian Learning \n",
    "\n",
    "We will do this with a no-hidden-layer network.\n",
    "* The input layer has one node for each variable (feature) in the input data\n",
    "* The output layer has one node, which outputs its activation based on the input data and the edge weights\n",
    "* There is no bias node\n",
    "* We initialize the edge weights at random\n",
    "* We update the edge weights $w_i$ using Hebb's rule: $w_j(T) = w_j(T-1) + \\eta x_{ij} z_i$, where $z_i$ is the activation output for data point $\\vec{x_i}$ and $\\eta$ is a small update factor (learning rate)\n",
    "  * $x_{ij} z_i$ updates the weights based on the correlation between $x_{ij}$ and $z_i$; if they are both positive / negative, then $w_j$ increases from time $T-1$ to time $T$; else it decreases\n",
    "  * Hebb's rule applied naively leads to weights that increase without bound\n",
    "  * We use Oja's rule to correct: $w_j(T) = w_j(T-1) + \\eta x_{ij} z_i - \\eta x_{ij} {z_i}^2 = w_j(T-1) + \\eta z_i(x_{ij} - {z_i})$\n",
    "\n",
    "The goal in Hebbian learning is to learn weights based on training samples that represent key patterns in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ee397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data, eta=0.01, n_epochs=150):\n",
    "    '''Do Hebbian learning on the data samples (using Oja's Rule) and learning rate of `eta`.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: ndarray. shape=(N, M)\n",
    "        Data samples\n",
    "    eta: float.\n",
    "        Learning rate for weight update\n",
    "    n_epochs: int.\n",
    "        Number of epochs to train (i.e. number of passes/presentations of data the the network)\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    wts. ndarray. shape=(M,).\n",
    "        The learned network weights\n",
    "    '''\n",
    "    # Number of data pts and features\n",
    "    N, M = data.shape\n",
    "\n",
    "    # Initialize weights randomly, centered at 0\n",
    "    wts = 0.1*np.random.rand(M) - 0.1/2\n",
    "    prev_wts = wts + 1\n",
    "\n",
    "    # Do `n_epochs` passes through the data\n",
    "    for j in range(n_epochs):\n",
    "        print('weights at epoch', str(j), ':\\n', wts)\n",
    "        if np.array_equal(wts, prev_wts):\n",
    "            return wts\n",
    "        prev_wts = wts\n",
    "        # Train by processing each sample\n",
    "        for i in range(N):\n",
    "            # Sample x_i\n",
    "            xi = data[i]\n",
    "\n",
    "            # Network output\n",
    "            zi = xi @ wts\n",
    "\n",
    "            # Update wts via Hebbian Learning\n",
    "            wts = wts + eta*zi*(xi - zi*wts)\n",
    "    return wts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68e75f",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "To predict, we would just multiply new data points by the learned weights to get the activation of this network for those new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, weights):\n",
    "    return data @ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58072159",
   "metadata": {},
   "source": [
    "Let's do Hebbian learning on a random data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499abcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Set random seed for reproduceability\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Define data as multivariate Gaussian blob\n",
    "    mu = [0, 0]\n",
    "    sigma = np.array([[3, 1.5],\n",
    "                      [1.5, 3]])\n",
    "    data = np.random.multivariate_normal(mu, sigma, size=100)\n",
    "    plt.plot(data[:, 0], data[:, 1], 'o')\n",
    "\n",
    "    # Normalize globally to range [-0.5, 0.5]\n",
    "    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "    data = data - 0.5\n",
    "\n",
    "    # Train the Hebbian neural network, get the final weights\n",
    "    wts = fit(data, n_epochs=150)\n",
    "    print('Learned wts\\n', wts)\n",
    "\n",
    "    # Draw learned wts vector\n",
    "    vectorScale = 3\n",
    "    ax = plt.gca()\n",
    "    ax.annotate('', vectorScale*wts, [0, 0],\n",
    "                arrowprops=dict(arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "    plt.show()\n",
    "    return data, wts\n",
    "    \n",
    "data, weights = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e70deb",
   "metadata": {},
   "source": [
    "# Comparison with PCA\n",
    "\n",
    "Let's compare this weight vector with the first principal component for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(data):\n",
    "    covariance_matrix = (data.T @ data) / (len(data) - 1)\n",
    "    (evals, evectors) = np.linalg.eig(covariance_matrix)\n",
    "    evals_order = np.argsort(evals)[::-1]\n",
    "    evals_sorted = evals[evals_order]\n",
    "    evectors_sorted = evectors[:, evals_order]\n",
    "    return evals_sorted, evectors_sorted\n",
    "\n",
    "# Why do we do this?\n",
    "centered_data = data - np.mean(data, axis=0)\n",
    "\n",
    "evals, evecs = PCA(centered_data)\n",
    "print(evecs[0])\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9fbd8",
   "metadata": {},
   "source": [
    "# Let's use Hebbian learning on a real dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba73819",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_converter(x):\n",
    "    values = ['setosa', 'versicolor', 'virginica']\n",
    "    return float(values.index(x))\n",
    "\n",
    "def inverse_type_converter(x):\n",
    "    values = ['setosa', 'versicolor', 'virginica']\n",
    "    return values[x]\n",
    "\n",
    "\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "iris = np.array(np.genfromtxt('data/iris.csv', delimiter=',', converters={4: type_converter}, skip_header=2, dtype=float, encoding='utf-8'))\n",
    "print(iris.shape, iris.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6593f0",
   "metadata": {},
   "source": [
    "## Look at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_statistics(data):\n",
    "    \"Get the max, min, mean, var for each variable in the data.\"\n",
    "    return pd.DataFrame(np.array([data.max(axis=0), data.min(axis=0), data.mean(axis=0), data.var(axis=0)]))\n",
    "\n",
    "print(get_summary_statistics(iris))\n",
    "\n",
    "df = pd.DataFrame(iris, columns=columns)\n",
    "sns.pairplot(df, y_vars = [\"class\"], kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ea731",
   "metadata": {},
   "source": [
    "## Split the Data \n",
    "\n",
    "Not for clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d109eb",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "\n",
    "Nothing to see here for the iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841e4e6",
   "metadata": {},
   "source": [
    "## Consider Dimensionality Reduction\n",
    "\n",
    "Nothing to see here for the iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8149c",
   "metadata": {},
   "source": [
    "## Consider Transforming/Normalizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogenizeData(data):\n",
    "    return np.append(data, np.array([np.ones(data.shape[0], dtype=float)]).T, axis=1)\n",
    "   \n",
    "def zScore(data, translateTransform=None, scaleTransform=None):\n",
    "    \"z score.\"\n",
    "    homogenizedData = np.append(data, np.array([np.ones(data.shape[0], dtype=float)]).T, axis=1)\n",
    "    if translateTransform is None:\n",
    "        translateTransform = np.eye(homogenizedData.shape[1])\n",
    "        for i in range(homogenizedData.shape[1]):\n",
    "            translateTransform[i, homogenizedData.shape[1]-1] = -homogenizedData[:, i].mean()\n",
    "    if scaleTransform is None:\n",
    "        diagonal = [1 / homogenizedData[:, i].std() if homogenizedData[:, i].std() != 0 else 1 for i in range(homogenizedData.shape[1])]\n",
    "        scaleTransform = np.eye(homogenizedData.shape[1], dtype=float) * diagonal\n",
    "    data = (scaleTransform@translateTransform@homogenizedData.T).T\n",
    "    return translateTransform, scaleTransform, data[:, :data.shape[1]-1]\n",
    "\n",
    "translateTransform, scaleTransform, data_transformed = zScore(iris)\n",
    "print(\"training data\", \"\\n\", data_transformed.shape, \"\\n\", get_summary_statistics(data_transformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a39ea17",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21240a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = fit(data_transformed)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e118681",
   "metadata": {},
   "source": [
    "## Comparison with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebbf77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = PCA(data_transformed)\n",
    "print(evecs[0])\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a1b8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
