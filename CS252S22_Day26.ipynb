{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70007ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f10b3a",
   "metadata": {},
   "source": [
    "## Bayes' Rule reviewed \n",
    "\n",
    "$P(Y|X) = \\frac{P(Y)*P(X|Y)}{P(X)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c4a9b",
   "metadata": {},
   "source": [
    "## Fitting and predicting using Naive Bayes\n",
    "\n",
    "Let's imagine I have the dataset below. For the dependent variable (first column), I use \"1\" to represent *ate it* and \"0\" to represent *did not eat it*. For the independent variable (zeroth column), I use \"0\" to represent *peanut M&M*, \"1\" to represent *regular M&M* and \"2\" to represent *raisin M&M*.\n",
    "\n",
    "Side note: we typically convert qualitative values to numbers for machine learning; it enables us to use all the power of numpy, at some cost in readability to humans. I'm not going to do this today, for readability to us, but on Friday we'll be back to 'numeric' representation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddcfa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([['peanut M&M', 'ate it'], ['peanut M&M', 'ate it'], ['peanut M&M', 'ate it'], ['raisin M&M', 'did not eat it'], ['raisin M&M', 'ate it'], ['regular M&M', 'did not eat it'], ['peanut M&M', 'did not eat it'], ['regular M&M', 'did not eat it'], ['raisin M&M', 'did not eat it'], ['raisin M&M', 'did not eat it']])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f26914",
   "metadata": {},
   "source": [
    "__Fit__\n",
    "\n",
    "* Calculate the likelihood of *ate it* given *peanut M&M*, of *ate it* given *regular M&M*, of *ate it* given *raisin M&M* and of *did not eat it* given each type of M&M.\n",
    "* Calculate the prior for *ate it* and the prior for *did not eat it*.\n",
    "\n",
    "Store both sets of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    likelihoods = {}\n",
    "    cats = sorted(np.unique(data[:, 1]))\n",
    "    values = sorted(np.unique(data[:, 0]))\n",
    "    for cat in cats:\n",
    "        likelihoods[cat] = {}\n",
    "        for value in values:\n",
    "            likelihoods[cat][value] = len(data[np.where((data[:, 0] == value) & (data[:, -1] == cat))]) / len(data[np.where(data[:, 0] == value)])\n",
    " \n",
    "    priors = {cat : len(data[np.where(data[:, -1] == cat)]) / len(data) for cat in cats}\n",
    "    return likelihoods, priors\n",
    "\n",
    "likelihoods, priors = fit(data)\n",
    "print(likelihoods, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84203d0a",
   "metadata": {},
   "source": [
    "__Predict__\n",
    "\n",
    "Given a new observation, *peanut M&M*, what is my most likely behavior?\n",
    "\n",
    "* Compare $P(ate it|peanut M\\&M)$ and $P(did not eat it|peanut M\\&M)$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f227f",
   "metadata": {},
   "source": [
    "$P(ate it|peanut M\\&M) = \\frac{P(ate it)*P(peanut M\\&M|ate it)}{P(peanut M\\&M)}$\n",
    "\n",
    "$P(did not eat it|peanut M\\&M) = \\frac{P(did not eat it)*P(peanut M\\&M|did not eat it))}{P(peanut M\\&M)}$\n",
    "\n",
    "Note that since $P(peanut M\\&M)$ is in the denominator in both cases, we can ignore it completely for the comparison. So we only need the prior and the likelihood, both of which we calculated during __fit__.\n",
    "\n",
    "This means that the Naive Bayes \"formula\" is $argmax_Y P(Y)*P(X|Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(datum, likelihoods, priors):\n",
    "    res = []\n",
    "    for cat in likelihoods:\n",
    "        try:\n",
    "            likelihood = likelihoods[cat][datum]\n",
    "        except:\n",
    "            print(\"feature value \" + datum + \" is not in likelihoods, returning most frequent category!\")\n",
    "            likelihood = 1\n",
    "        res.append(priors[cat]*likelihood)\n",
    "    print(res)\n",
    "    return list(likelihoods.keys())[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2dec0",
   "metadata": {},
   "source": [
    "* Which is higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fcccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_one('peanut M&M', likelihoods, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f77bb2",
   "metadata": {},
   "source": [
    "So, my classifier will say that I ate the peanut M&M."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893553b7",
   "metadata": {},
   "source": [
    "__What if we got a new observation, *raisin M&M*?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c122d3",
   "metadata": {},
   "source": [
    "### Score\n",
    "\n",
    "Let's say I have the test data below. \n",
    "* Does this data include all my labels?\n",
    "* Does this data include all possible values for my one independent variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([['peanut M&M', 'did not eat it'], ['peanut M&M', 'ate it'], ['raisin M&M', 'did not eat it'], ['raisin M&M', 'ate it'], ['regular M&M', 'did not eat it']])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6faa9",
   "metadata": {},
   "source": [
    "How well does my Naive Bayes model perform on this test data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, likelihoods, priors):\n",
    "    return np.array([predict_one(datum, likelihoods, priors) for datum in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01dae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predict(test[:, 0], likelihoods, priors)\n",
    "print(test[:, -1], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ef828",
   "metadata": {},
   "source": [
    "* What is the accuracy?\n",
    "* What is the confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These come directly from day 24\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    assert len(y) == len(yhat)\n",
    "    diffs = y == yhat\n",
    "    vals, counts = np.unique(diffs, return_counts=True)\n",
    "    return counts[np.where(vals == True)] / (np.sum(counts))\n",
    "    \n",
    "def confusion_matrix(y, yhat, cats):\n",
    "    \"Thanks to https://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python\"\n",
    "    assert len(y) == len(yhat)\n",
    "    result = np.zeros((len(cats), len(cats)))\n",
    "    for i in range(len(y)):\n",
    "        result[cats.index(y[i])][cats.index(yhat[i])] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(test[:, 1], yhat))\n",
    "print(cats, confusion_matrix(test[:, 1], yhat, cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce0c4f",
   "metadata": {},
   "source": [
    "## What if I have multiple independent variables?\n",
    "\n",
    "Let's take the same training data as before, but add one more features: the length of each piece of candy. \n",
    "\n",
    "Because numpy won't let me have arrays of heterogeneous types, I'm going to express length and width as 's', 'm' or 'l'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60098a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    ['peanut M&M', 'l', 'ate it'], \n",
    "    ['peanut M&M', 'l', 'ate it'], \n",
    "    ['peanut M&M', 'l', 'ate it'], \n",
    "    ['raisin M&M', 'm', 'did not eat it'], \n",
    "    ['raisin M&M', 'l', 'ate it'], \n",
    "    ['regular M&M', 's', 'did not eat it'], \n",
    "    ['peanut M&M', 'm', 'did not eat it'], \n",
    "    ['regular M&M', 's', 'did not eat it'], \n",
    "    ['raisin M&M', 's', 'did not eat it'], \n",
    "    ['raisin M&M', 'm', 'did not eat it']])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69cd989",
   "metadata": {},
   "source": [
    "Looking at the data, is the length feature independent of the type feature?\n",
    "* How many large, medium and small peanut M&Ms are there?\n",
    "* How many large, medium and small raisin M&Ms are there?\n",
    "* How many large, medium and small regular M&Ms are there?\n",
    "\n",
    "### Conditional independence\n",
    "\n",
    "When we fit a Naive Bayes model across multiple independent features, we assume those features are *conditionally independent*, given $Y$. In other words, the effect of the value of a feature  on the label is independent of the values of other features.\n",
    "\n",
    "The Naive Bayes formula in this case, where $X = {x_1, x_2, ...}$ looks like:\n",
    "$P(Y|X) = argmax_Y P(Y)*P(X|Y) = argmax_Y P(Y)*\\prod_i{P(x_i|Y)}$.\n",
    "\n",
    "We can do the multiplication because of this *conditional independence*. This *conditional independence* assumption is the \"naive\" in Naive Bayes.\n",
    "\n",
    "Even if the independent variables aren't really conditionally independent, Naive Bayes is still a surprisingly strong baseline model for many tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffabd86",
   "metadata": {},
   "source": [
    "### Missing feature values\n",
    "\n",
    "Ok, the fact that there are 0 medium or large regular M&Ms is a problem. It means that if we ever *do* (during testing or inference) see a large regular M&M, the $P(l|regular M\\&M) = 0$ will \"zero out\" the total, but that's not really safe (especially given we are looking at a small data sample). So we use __Laplace smoothing__: for $x_i \\in X$ and $y_j \\in Y$:\n",
    "* instead of $P(x_i|y_j) \\sim \\frac{count(x_i)}{\\sum_i x_i}$, \n",
    "* we use $P(x_i|y_j) \\sim \\frac{count(x_i)+1}{\\sum_i x_i + |Y|}$. \n",
    "\n",
    "In this way, there's some (small!) likelihood for every value of each independent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec8ed9",
   "metadata": {},
   "source": [
    "__Fit__\n",
    "\n",
    "* Calculate the likelihood of each of *ate it* and *did not eat it* given:\n",
    "  * *peanut M&M*, *regular M&M* and *raisin M&M*\n",
    "  * *s*, *m* and *l*\n",
    "* Calculate the prior for *ate it* and the prior for *did not eat it*.\n",
    "\n",
    "Store both sets of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data):\n",
    "    likelihoods = {}\n",
    "    cats = sorted(np.unique(data[:, -1]))\n",
    "    for cat in cats:\n",
    "        likelihoods[cat] = {}\n",
    "        for feature in range(data.shape[1]-1):\n",
    "            values = sorted(np.unique(data[:, feature]))\n",
    "            for value in values:\n",
    "                print(value, len(data[np.where((data[:, feature] == value) & (data[:, -1] == cat))]), len(data[np.where(data[:, feature] == value)]), len(cats))\n",
    "                likelihoods[cat][value] = (len(data[np.where((data[:, feature] == value) & (data[:, -1] == cat))]) + 1) / (len(data[np.where(data[:, feature] == value)]) + len(cats))\n",
    " \n",
    "    priors = {cat : len(data[np.where(data[:, -1] == cat)]) / len(data) for cat in cats}\n",
    "    return likelihoods, priors\n",
    "\n",
    "likelihoods, priors = fit(data)\n",
    "print(likelihoods, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dac31",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Calculate $argmax_Y P(Y)*\\prod_i{P(x_i|Y)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c12b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(datum, likelihoods, priors):\n",
    "    res = []\n",
    "    for cat in likelihoods:\n",
    "        product = 1\n",
    "        for value in datum:\n",
    "            try:\n",
    "                likelihood = likelihoods[cat][value]\n",
    "            except:\n",
    "                print(\"feature value \" + value + \" is not in likelihoods, returning most frequent category!\")\n",
    "                likelihood = 1\n",
    "            product = product*likelihood\n",
    "        res.append(priors[cat]*product)\n",
    "    return list(likelihoods.keys())[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8199e",
   "metadata": {},
   "source": [
    "Did I eat the large peanut M&M?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_one(np.array(['peanut M&M', 'l']), likelihoods, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45ffac",
   "metadata": {},
   "source": [
    "Did I eat the large raisin M&M?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e96da",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_one(np.array(['raisin M&M', 'l']), likelihoods, priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1e76b",
   "metadata": {},
   "source": [
    "### Score\n",
    "\n",
    "Am I doing better with one more feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4991ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([['peanut M&M', 's', 'did not eat it'], ['peanut M&M', 'l', 'ate it'], ['raisin M&M', 's', 'did not eat it'], ['raisin M&M', 'l', 'ate it'], ['regular M&M', 'm', 'did not eat it']])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc438dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predict(test[:, :-1], likelihoods, priors)\n",
    "print(test[:, -1], yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d62322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(test[:, -1], yhat))\n",
    "print(cats, confusion_matrix(test[:, -1], yhat, cats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
