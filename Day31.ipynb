{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6920b4a",
   "metadata": {},
   "source": [
    "# The One Goal for Today\n",
    "\n",
    "Understand how to fit a naive Bayes model using text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29860ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d26bd31f",
   "metadata": {},
   "source": [
    "# Naive Bayes for Spam\n",
    "\n",
    "Text data, *without preprocessing*, is qualitative data. Let's use Naive Bayes to classify some text data! \n",
    "\n",
    "I'm going to be using the SMS dataset from [here](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection).\n",
    "\n",
    "## I. Load and Look at our data\n",
    "\n",
    "Let's load and __look at our data__. Is our independent variable at the *start* or the *end*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773a1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(np.genfromtxt('data/SMSSpamCollection', delimiter='\\t', encoding='utf-8', dtype=str))  \n",
    "print(data.shape)\n",
    "print(data[2], \"\\n\", data[4], \"\\n\", data[9], \"\\n\", data[10], \"\\n\", data[11], \"\\n\", data[12], \"\\n\", data[14], \"\\n\", data[20], \"\\n\", data[29], \"\\n\", data[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10895987",
   "metadata": {},
   "source": [
    "How many labels do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644772da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(data[:, 0], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f4e1ee",
   "metadata": {},
   "source": [
    "## II. Split the data\n",
    "\n",
    "Let's split it into train, dev, test and make sure we have some of each label in each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ebca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = np.split(data, [int(.8 * len(data))])\n",
    "print(train.shape, test.shape)\n",
    "print(np.unique(train[:, 0], return_counts=True))\n",
    "print(np.unique(test[:, 0], return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d461abc1",
   "metadata": {},
   "source": [
    "So far, so good!\n",
    "\n",
    "## III. Preprocess the data\n",
    "\n",
    "*But*, how do I deal with the SMS texts themselves? Well, a SMS is composed of word-type things (let's call them \"tokens\") and if I *completely ignore* the sequence information then I could say each SMS is composed of a *bag* of tokens. \n",
    "\n",
    "NOTE that to do this hurts me as a computational linguist. Of *course* word order matters! In particular, in this context it means that the token occurrences are not *independent* of each other. But we are going to pretend they are, thus lacerating the sensibilities of both linguists and statisticians in search of a computationally efficient approximation.\n",
    "\n",
    "__Our independent variables will be what, now?__\n",
    "\n",
    "### Let's define a function to get the bag of tokens from the text of a SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec04449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split the text on any character(s) that are not letters or numbers,\n",
    "# and then keep anything that is not whitespace\n",
    "def tokenize(text):\n",
    "    # split on whitespace - \\s+ - and strip off any whitespace\n",
    "    return [token.strip() for token in re.split(r'(\\s+)', text) if not re.match(r'^\\s*$', token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9259a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it on one SMS\n",
    "tokenize(train[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20fd6875",
   "metadata": {},
   "source": [
    "### Let's get the unique tokens\n",
    "\n",
    "We do this on **train only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the tokens\n",
    "tokens = []\n",
    "for i in range(len(train)):\n",
    "    # add the tokens from this training data point to the list of all the tokens\n",
    "    tokens.extend(tokenize(train[i][1]))\n",
    "# get the unique tokens\n",
    "unique_tokens, token_counts = np.unique(tokens, return_counts=True)\n",
    "# how many do we have?\n",
    "print(len(unique_tokens))\n",
    "# what are some of them?\n",
    "print(list(unique_tokens)[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8de881",
   "metadata": {},
   "source": [
    "That's entirely too many tokens! Let's keep just from the $a$ to the $b$ most frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 75\n",
    "b = 2000\n",
    "# all the tokens\n",
    "tokens = []\n",
    "for i in range(len(train)):\n",
    "    # add the tokens from this training data point to the list of all the tokens\n",
    "    tokens.extend(tokenize(train[i][1]))\n",
    "# get the unique tokens\n",
    "unique_tokens, token_counts = np.unique(tokens, return_counts=True)\n",
    "# sort them by the counts; reverse the sort so it goes from largest to smallest; keep only the ones between a and b\n",
    "top_unique_tokens = unique_tokens[np.argsort(token_counts)[::-1]][a:b]\n",
    "print(len(top_unique_tokens))\n",
    "print(top_unique_tokens[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a8c8b2",
   "metadata": {},
   "source": [
    "### Let's collect token counts across all the 'ham' SMSs, and separately across all the 'spam' SMSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ebaba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_of_data means all the data points in a class / with the same value for the dependent variable\n",
    "# token_vocab is the set of most frequent unique tokens we care about\n",
    "def preprocess_category(subset_of_data, token_vocab):\n",
    "    # the tokens in the data points in this class\n",
    "    tokens = []\n",
    "    for i in range(len(subset_of_data)):\n",
    "        # add the tokens from this data point to tokens\n",
    "        tokens.extend(tokenize(subset_of_data[i][1]))\n",
    "    # get the unique tokens\n",
    "    unique_tokens, token_counts = np.unique(tokens, return_counts=True)\n",
    "    # we only want to keep those in token_vocab\n",
    "    res = dict.fromkeys(token_vocab, 0)\n",
    "    # add the count for each token that is in this class and that is a token we care about to res\n",
    "    for token, count in zip(unique_tokens, token_counts):\n",
    "        if token in res:\n",
    "            res[token] += count\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f14601",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = {}\n",
    "# get token counts for ham\n",
    "token_counts['ham'] = preprocess_category(train[np.where(train[:, 0] == 'ham')], top_unique_tokens)\n",
    "# get token counts for spam\n",
    "token_counts['spam'] = preprocess_category(train[np.where(train[:, 0] == 'spam')], top_unique_tokens)\n",
    "# We keep this around for indexing into the likelihoods\n",
    "sorted_tokens = sorted(top_unique_tokens)\n",
    "print(len(sorted_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecca3b6",
   "metadata": {},
   "source": [
    "Pay attention to the sleight of hand here:\n",
    "* Our labels are 'ham' and 'spam'.\n",
    "* Our features are individual token frequencies calculated across all of 'ham' and all of 'spam'. We *assume independence in the features*. That's the naive in Naive Bayes.\n",
    "\n",
    "## Fit\n",
    "\n",
    "So now let's fit a Naive Bayes model for spam detection\n",
    "\n",
    "To fit, we need to calculate:\n",
    "* priors - $P(ham)$ and $P(spam)$\n",
    "* likelihoods - for any token $x$, $P(x|ham)$ and $P(x|spam)$\n",
    "\n",
    "### Priors\n",
    "\n",
    "Calculating priors will be easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62578037",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = {'ham': len(train[np.where(train[:, 0] == 'ham')]) / len(train), 'spam': len(train[np.where(train[:, 0] == 'spam')]) / len(train)}\n",
    "print(priors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f80a8553",
   "metadata": {},
   "source": [
    "### Likelihoods\n",
    "\n",
    "Calculating likelihoods will be tricky, because:.\n",
    "* Sometimes the likelihoods will be 0, as a token will occur *no* times in one of 'ham' or 'spam'.\n",
    "* A lot of the likelihoods will be veerrrry small, as most of the tokens occur only once.\n",
    "\n",
    "To deal with these, we:\n",
    "* 0s - use *Laplace smoothing*.\n",
    "* veerrrry small - move from regular count space to log space (see plot).\n",
    "  * in 'regular count space' $P(I, like, candy) | ham) = P(I|ham)*P(like|ham)*P(candy|ham)$.\n",
    "  * in 'log space' $P(I, like, candy) | ham) = log(P(I|ham)) + log(P(like|ham)) + log(P(candy|ham))$, because the logarithm of a product is the sum of the logarithms.\n",
    "\n",
    "Let's visualize the difference between count space and log space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array([[i, 1/i, np.log(1/i)] for i in range(1, 1000)])\n",
    "plt.plot(res[:, 0], res[:, 1])\n",
    "plt.plot(res[:, 0], res[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9631038f",
   "metadata": {},
   "source": [
    "Let's calculate the log likelihoods using Laplace smoothing of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihoods(data, sorted_tokens, token_counts):\n",
    "    likelihoods = {}\n",
    "    # assume we have all the possible values for the dependent variable in this data subset.\n",
    "    cats = np.unique(data[:, 0])\n",
    "    # for each possible outcome\n",
    "    for cat in cats:\n",
    "        likelihoods[cat] = {}\n",
    "        # for each possible value of the independent variable\n",
    "        for token in sorted_tokens:\n",
    "            # add 1 in the numerator for Laplace smoothing; add the number of possible values of the independent variable to the denominator for Laplace smoothing\n",
    "            # take the log of the likelihood\n",
    "            likelihoods[cat][token] = np.log((token_counts[cat][token] + 1) / \n",
    "                                             (len(np.where([data[:, 0] == cat])) + len(sorted_tokens)))\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fef7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = calculate_likelihoods(train, sorted_tokens, token_counts)\n",
    "# sanity check!\n",
    "print(len(likelihoods))\n",
    "print(len(likelihoods['ham']))\n",
    "print(len(likelihoods['spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2327924",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Let's use the Naive Bayes model we fit to predict the class for each item in our test data.\n",
    "\n",
    "We will have to tokenize each SMS, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf24e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_text_given_category(text, cat, likelihoods):\n",
    "    tokens = tokenize(text)\n",
    "    sum = 0\n",
    "    for token in tokens:\n",
    "        # if the token is not in our vocab from train, it just gets dropped\n",
    "        try:\n",
    "            likelihood = likelihoods[cat][token]\n",
    "        except:\n",
    "            likelihood = 0\n",
    "        # we add instead of multiply, because we are in log space\n",
    "        sum += likelihood\n",
    "    return sum\n",
    "        \n",
    "def predict(data, priors, likelihoods):\n",
    "    labels = []\n",
    "    cats = np.unique(data[:, 0])\n",
    "    for i in range(len(data)):    \n",
    "        res = []\n",
    "        for cat in cats:\n",
    "            # we add instead of multiply, because we are in log space\n",
    "            res.append(np.log(priors[cat]) + p_text_given_category(dev[i][1], cat, likelihoods))\n",
    "        # take the argmax, and also its probability - take the exponent to get us out of log space\n",
    "        labels.append((cats[np.argmax(res)], np.exp(res[np.argmax(res)])))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19530f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = predict(dev, priors, likelihoods)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e79aa",
   "metadata": {},
   "source": [
    "## Score\n",
    "\n",
    "Let's get accuracy and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These come directly from day 24\n",
    "\n",
    "def accuracy(y, yhat):\n",
    "    assert len(y) == len(yhat)\n",
    "    diffs = y == yhat\n",
    "    vals, counts = np.unique(diffs, return_counts=True)\n",
    "    return counts[np.where(vals == True)] / (np.sum(counts))\n",
    "    \n",
    "def confusion_matrix(y, yhat):\n",
    "    \"Thanks to https://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python\"\n",
    "    allLabels = sorted(list(np.union1d(y, yhat)))\n",
    "    print(allLabels)\n",
    "    assert len(y) == len(yhat)\n",
    "    result = np.zeros((len(allLabels), len(allLabels)))\n",
    "    for i in range(len(y)):\n",
    "        result[allLabels.index(y[i])][allLabels.index(yhat[i])] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy([x[0] for x in labels], dev[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93261504",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix([x[0] for x in labels], dev[:, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e6c82ce",
   "metadata": {},
   "source": [
    "Note that accuracy isn't super helpful here, since there are so many 'ham' emails. \n",
    "\n",
    "Question: *What would the accuracy be if the model just labeled __everything__ as 'ham'?*\n",
    "\n",
    "## TPR, FPR, Precision, Recall, F\n",
    "\n",
    "Let's make a different version of the confusion matrix focusing on just the 'spam' class, which is the class we really want to do well on:\n",
    "\n",
    "| | Predict not in 'spam' | Predict in 'spam' | Rates |\n",
    "| -- | --- | --- | -- |\n",
    "| Actual not in 'spam' | TN | FP | FPR = FP/(FP+TN) |\n",
    "| Actual in 'spam' | FN | TP | TPR = TP/(TP+FN) |\n",
    "\n",
    "With a table like this, we can also calculate:\n",
    "* Precision (how many of those we guessed were 'spam' were actually 'spam'?): TP / (TP + FP)\n",
    "* Recall (how many actual 'spam' did we guess were 'spam'?): TP / (TP + FN)\n",
    "\n",
    "Then, to get an assessment of recall and precision together, we can calculate F1: (2\\*Precision\\*Recall)/(Precision+Recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f72cb",
   "metadata": {},
   "source": [
    "## Are we happy?\n",
    "\n",
    "We could potentially do better:\n",
    "* We could add information from *outside the text*. Modern spam filters use features like whether the sender's email address is the same as the reply-to email address, whether the IP address the email comes from matches that of the domain of the sender's email address, whether this same email is being sent to a lot of people at the same time, whether this sender has ever emailed this recipient before (and how often), and all kinds of behavioral features.\n",
    "* We have a *class imbalance*: there are a lot more 'ham' emails than there are 'spam' emails. We can deal with this by *changing the decision threshold*, *downsampling* the 'ham' emails, *upsampling* (repeating) the 'spam' emails, or [other techniques](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/?msclkid=33d6b961b9df11ec98e519046a973ddf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4574cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_texts = np.array([['ham', 'Thank you!'], ['spam', 'hey how are things'], ['ham', 'How is second puppy training going?'], ['ham', 'Your Clear Enrollment verification code is xxxx']])\n",
    "pred = predict(my_texts, priors, likelihoods)\n",
    "print(pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27ea3f3d",
   "metadata": {},
   "source": [
    "# For the project\n",
    "\n",
    "For the project, you will need to load all the files in the ham folder, and all the files in the spam folder. You may use the python **glob** functionality for this. \n",
    "* For each directory in the enron directory (ham, spam):\n",
    "  * For each .txt file in the directory:\n",
    "    * Read the text in the file\n",
    "    * Tokenize the text\n",
    "    * For each token:\n",
    "      * If it's not in the dictionary of token counts, add it there\n",
    "      * Increment the count\n",
    "\n",
    "For the project, you will keep only the 200 most frequent tokens.\n",
    "\n",
    "Then, you will encode each headline uas a 200 dimensional vector (one column per token), and the labels as 0 or 1 for each headline (0 for spam, 1 for ham).\n",
    "\n",
    "Then, you will want to split the data into train and test, and save the training and test data into separate files.\n",
    "\n",
    "The whole process will look very similar to what we have just done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53ad16",
   "metadata": {},
   "source": [
    "Resources:\n",
    "* https://towardsdatascience.com/roc-curve-and-auc-from-scratch-in-numpy-visualized-2612bb9459ab\n",
    "* https://towardsdatascience.com/roc-and-auc-how-to-evaluate-machine-learning-models-in-no-time-fb2304c83a7f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
