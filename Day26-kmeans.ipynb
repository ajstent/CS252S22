{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0e7ca52",
   "metadata": {},
   "source": [
    "# The One Goal For Today\n",
    "\n",
    "Understand how normalization first can lead to better or more efficient clustering and classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811722d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6243b190",
   "metadata": {},
   "source": [
    "# Load and Look at Your Data\n",
    "\n",
    "The data set we wil be analyzing is our usual car dataset from Craigslist. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8571c1f",
   "metadata": {},
   "source": [
    "First we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca359103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be our columns\n",
    "columns = [\"price\", \"year\", \"manufacturer\", \"model\", \"condition\", \"fuel\", \"odometer\", \"title_status\", \"transmission\"]\n",
    "# this will contain our converters\n",
    "colValues = {}\n",
    "\n",
    "# first we load our data as strings so we can define the converters\n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), skip_header=1, dtype=str, encoding='utf-8'))  \n",
    "\n",
    "# make a list of the unique values in each column of our data\n",
    "for colIndex in range(data.shape[1]):\n",
    "    colValues[colIndex] = np.unique(data[:, colIndex]).tolist()\n",
    "    print(colIndex, colValues[colIndex])\n",
    "\n",
    "# map values to their indices in the list of unique values\n",
    "def converter(x, colIndex):\n",
    "    return colValues[colIndex].index(x)\n",
    "    \n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), converters={3: lambda x: converter(x, 2), 4: lambda x: converter(x, 3), 5: lambda x: converter(x, 4), 7: lambda x: converter(x,5), 9: lambda x: converter(x, 7), 11: lambda x: converter(x, 8)}, skip_header=1, dtype=int, encoding='utf-8'))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68a5a5ca",
   "metadata": {},
   "source": [
    "Then we get summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0dcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStatistics(data):\n",
    "    print(\"min, max, mean, std per variable\")\n",
    "    return pd.DataFrame([data.min(axis=0), data.max(axis=0), data.mean(axis=0), data.std(axis=0)])\n",
    "\n",
    "def getShapeType(data):\n",
    "    print(\"shape\")\n",
    "    return (data.shape, data.dtype)\n",
    "\n",
    "print(getSummaryStatistics(data))\n",
    "print(getShapeType(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef7fcc85",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "\n",
    "If we are doing supervised machine learning, we split the data into train and test. \n",
    "\n",
    "However, here we are doing clustering, so we don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "283ad7b8",
   "metadata": {},
   "source": [
    "If we had a clear dependent variable (as we do with the car logo dataset) we'd strip it off. However, here we don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = train[:, -1]\n",
    "#x_train = train[:, 0:-1]\n",
    "#y_test = test[:, -1]\n",
    "#x_test = test[:, 0:-1]\n",
    "x_train = train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0695d83",
   "metadata": {},
   "source": [
    "# Normalization Review\n",
    "\n",
    "Here we implement max-min global, max-min local, z-score and center. This code comes from day 20.\n",
    "\n",
    "This code you can use as a **tool**.\n",
    "\n",
    "**If you are using separate training and test data, you want to normalize to the mean (min, max, std) of the _training data_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa19fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, min, max, mean, std, method='center'):\n",
    "    if method == 'center':\n",
    "        return data - mean\n",
    "    elif method == 'max-min-global':\n",
    "        return (data - min) / (max - min)\n",
    "    elif method == 'max-min-local':\n",
    "        return (data - min) / (max - min)\n",
    "    elif method == 'zscore':\n",
    "        return (data - mean) / std\n",
    "    else:\n",
    "        raise Exception(\"I can't do \" + method)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5352ca4",
   "metadata": {},
   "source": [
    "Let's try it!\n",
    "\n",
    "**When you are doing supervised machine learning, you always want to normalize using statistics (mean, min, max) from your training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_g = np.min(x_train)\n",
    "max_g = np.max(x_train)\n",
    "min_l = np.min(x_train, axis=0)\n",
    "max_l = np.max(x_train, axis=0)\n",
    "mean = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)\n",
    "normalized_train = normalize(x_train, min_l, max_l, mean, std, method='max-min-local')\n",
    "# normalized_train = normalize(train, min_g, max_g, mean, std, method='center')\n",
    "# normalized_train = normalize(train, min_g, max_g, mean, std, method='max-min-global')\n",
    "# normalized_train = normalize(train, min_g, max_g, mean, std, method='zscore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad0b841e",
   "metadata": {},
   "source": [
    "# K-means Clustering Review\n",
    "\n",
    "The code below comes from day 22.\n",
    "\n",
    "You can use this code as a **tool**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "def distance(a, b):\n",
    "    subtracted = a-b\n",
    "    return np.sqrt(np.dot(subtracted.T, subtracted))\n",
    "\n",
    "# Calculate the distance from each data point to each centroid\n",
    "def get_distances(item, centroids):\n",
    "    return [distance(item, centroid) for centroid in centroids]\n",
    "\n",
    "# Update cluster assignments given a set of centroids\n",
    "# input: list of data points, initial list of centroids\n",
    "def update_clusters(data, centroids):\n",
    "    # initialize clusters\n",
    "    clusters = {}\n",
    "    for i in range(len(centroids)):\n",
    "        # set its cluster members to the empty list\n",
    "        clusters[i] = []\n",
    "    # initialize mappings\n",
    "    mappings = {}\n",
    "    # for each data point\n",
    "    for j, datum in enumerate(data):\n",
    "        # find the index of the centroid with the smallest distance to this data point\n",
    "        min_cluster_index = np.argmin(get_distances(datum, centroids))\n",
    "        # add this data point to that centroid's cluster\n",
    "        clusters[min_cluster_index].append(datum)\n",
    "        # add mapping\n",
    "        mappings[j] = min_cluster_index\n",
    "    return clusters, mappings\n",
    "\n",
    "# Update the centroids given the data\n",
    "def update_centroids(clusters, oldcentroids):\n",
    "    # set centroids to empty list\n",
    "    centroids = []\n",
    "    # for each set of data points in a cluster around a single centroid\n",
    "    for centroidid, data_in_cluster in clusters.items():\n",
    "        # graciously handle case where no data ended up in a cluster\n",
    "        if len(data_in_cluster) > 0:\n",
    "            # new centroid is the mean of that cluster\n",
    "            centroids.append(np.mean(data_in_cluster, axis=0))\n",
    "        else:\n",
    "            centroids.append(oldcentroids[centroidid])\n",
    "    return centroids\n",
    "\n",
    "# Measure the inertia\n",
    "def inertia(data, centroids, clusters):\n",
    "    sum = 0\n",
    "    for i in clusters.keys():\n",
    "        for datum in clusters[i]:\n",
    "            # calculate the distance squared between each data point and its centroid\n",
    "            sum += distance(datum, centroids[i])**2\n",
    "    # average over the data\n",
    "    return sum / len(data)\n",
    "\n",
    "def fit_kmeans(data, k, cutoff=1):\n",
    "    # make some initial centroids\n",
    "    centroids = np.array([data[x] for x in np.random.choice(np.arange(len(data)), size=k, replace=False)])\n",
    "    # initialize last_inertia\n",
    "    last_inertia = -1\n",
    "    while True:\n",
    "        # get the clusters for these centroids\n",
    "        clusters, mappings = update_clusters(data, centroids)\n",
    "        # calculate the inertia for this clustering\n",
    "        this_inertia = inertia(data, centroids, clusters)\n",
    "        # stop when the inertia stops changing very much\n",
    "        if last_inertia > 0 and abs(last_inertia - this_inertia) < cutoff:\n",
    "            break\n",
    "        last_inertia = this_inertia\n",
    "        # update the centroids\n",
    "        centroids = update_centroids(clusters, centroids)\n",
    "    return centroids, clusters, mappings, this_inertia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78e7adea",
   "metadata": {},
   "source": [
    "On Wednesday we talked about the Silhouette coefficient as a way to evaluate the goodness of a clustering. We will use the scikit-learn implementation today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "#s = silhouette_score(data, mappings/labels, metric='euclidean')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "064c2fbf",
   "metadata": {},
   "source": [
    "# Impact of normalization on K-Means clustering\n",
    "\n",
    "Fill in this table.\n",
    "1. Try all the types of normalization plus k-means clustering. Use a reasonable value for $k$ in k-means clustering, like 6 (maybe it will cluster them by condition!).\n",
    "2. Try at least one type of normalization (centering!) plus PCA plus k-means clustering. Use the same value of $k$ for k-means clustering as you have so far. Pick a number of principal components that lets you keep at least 80% of the cumulative sum of variance.\n",
    "\n",
    "| Normalization | PCA (None or k) | K-means k | Silhouette score | Time |\n",
    "| ------------- | --------------- | --------- | ---------------- | ---- |\n",
    "| None | None | ?? | | |\n",
    "| Centering | None | ?? | | |\n",
    "| Max-min global | None | ?? | | |\n",
    "| Max-min local | None | ?? | | |\n",
    "| Z-score | None | ?? | | |\n",
    "| ?? | ?? | ?? | | |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a45aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22199c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_g = np.min(x_train)\n",
    "max_g = np.max(x_train)\n",
    "min_l = np.min(x_train, axis=0)\n",
    "max_l = np.max(x_train, axis=0)\n",
    "mean = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = normalize(x_train, min_g, max_g, mean, std, method='center')\n",
    "centroids, clusters, mappings, _ = fit_kmeans(normalized_train, k)\n",
    "silhouette_score(normalized_train, [x[1] for x in sorted(mappings.items())], metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9546c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = normalize(x_train, min_g, max_g, mean, std, method='max-min-global')\n",
    "centroids, clusters, mappings, _ = fit_kmeans(normalized_train, k)\n",
    "silhouette_score(normalized_train, [x[1] for x in sorted(mappings.items())], metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58396e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = normalize(x_train, min_g, max_g, mean, std, method='max-min-local')\n",
    "centroids, clusters, mappings, _ = fit_kmeans(normalized_train, k)\n",
    "silhouette_score(normalized_train, [x[1] for x in sorted(mappings.items())], metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = normalize(x_train, min_g, max_g, mean, std, method='zscore')\n",
    "centroids, clusters, mappings, _ = fit_kmeans(normalized_train, k)\n",
    "silhouette_score(normalized_train, [x[1] for x in sorted(mappings.items())], metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = train\n",
    "centroids, clusters, mappings, _ = fit_kmeans(normalized_train, k)\n",
    "silhouette_score(normalized_train, [x[1] for x in sorted(mappings.items())], metric='euclidean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f6384d4",
   "metadata": {},
   "source": [
    "**Bonus**: Now think about PCA. If we had a dataset with 1000 independent variables (like our car logo data), what do you think might be the impact of PCA-first on silhouette coefficient, and on time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
