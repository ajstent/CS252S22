{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c680f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import utilityfunctions as uf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e774a1",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "The steps to calculate PCA are:\n",
    "* (If appropriate) normalize the variables to be in the range 0-1\n",
    "* Center the data\n",
    "* Compute the covariance matrix\n",
    "* Compute the eigenvectors and eigenvalues; the eigenvectors tell us the direction of variance, and the eigenvalues tell us the amount of variance\n",
    "* Get an ordering over the eigenvalues\n",
    "* Sort the eigenvalues and eigenvectors accordingly\n",
    "* Compute the proportional variance (how much bigger?) accounted for by each principal component\n",
    "* Compute the cumulative sum of the proportional variance (tells us how many eigenvectors we need to explain a desired amount of variance)\n",
    "* Examine the principal components. Select v' of them.\n",
    "* Project the data into PCA space\n",
    "* Reconstruct the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c3a77c",
   "metadata": {},
   "source": [
    "## Today\n",
    "\n",
    "Today, we are going to look at what happens when we preprocess the data in different ways before fitting a PCA.\n",
    "\n",
    "So we will look again at the summary statistics for the data.\n",
    "\n",
    "We will preprocess the data in four ways:\n",
    "* center only\n",
    "* minmax global\n",
    "* minmax local\n",
    "* zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things we need from Day 19\n",
    "\n",
    "# Let's split off the labels\n",
    "def split(data, ycol):\n",
    "    y = data[:, ycol]\n",
    "    xfirst = data[:, 0:ycol]\n",
    "    xsecond = data[:, ycol+1:data.shape[1]]\n",
    "    return (np.hstack((xfirst, xsecond)), y)\n",
    "\n",
    "# center\n",
    "def center(data):\n",
    "    centered = data - np.mean(data, axis=0)\n",
    "    return centered\n",
    "\n",
    "# preprocess\n",
    "def preprocess(data, minmax=False, local=False, zscore=False):\n",
    "    if minmax == True and zscore == True:\n",
    "        print(\"Nope, won't do that!\")\n",
    "        return data\n",
    "    elif minmax == True:\n",
    "        if local == False:\n",
    "            data = uf.minmaxGlobal(data)\n",
    "        else:\n",
    "            data = uf.minmaxLocal(data)\n",
    "        return center(data)\n",
    "    elif zscore == True:\n",
    "        return uf.zScore(data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is most of the code from Day 19 in one function; it fits a PCA and prints out all kinds of things along the way\n",
    "def pca_with_plots(data):\n",
    "    # covariance\n",
    "    covariance_matrix = (data.T @ data) / (data.shape[0] - 1)\n",
    "    print(\"covariance matrix\")\n",
    "    print(covariance_matrix.shape)\n",
    "\n",
    "    # Let's look at the covariance matrix\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    sns.heatmap(pd.DataFrame(covariance_matrix), annot=False, cmap='PuOr')\n",
    "    plt.show()\n",
    "\n",
    "    # svd\n",
    "    (evals, evectors) = np.linalg.eig(covariance_matrix)\n",
    "    print(\"eigenvalues, eigenvectors\")\n",
    "    print(evals.shape)\n",
    "    print(evectors.shape)\n",
    "\n",
    "    # sort\n",
    "    evals_order = np.argsort(evals)[::-1]\n",
    "    evals_sorted = evals[evals_order]\n",
    "    evectors_sorted = evectors[:, evals_order]\n",
    "\n",
    "    # proportional variance\n",
    "    evals_sum = np.sum(evals_sorted)\n",
    "    proportional_vars = [e / evals_sum for e in evals_sorted]\n",
    "\n",
    "    # cumulative sum of proportional variance\n",
    "    print(\"cum sum prop var\")\n",
    "    cumulative_sum = np.cumsum(proportional_vars)\n",
    "\n",
    "    # Let's look at the proportional variance\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(len(proportional_vars)), proportional_vars, alpha=0.5, align='center',\n",
    "            label='Proportional variance')\n",
    "    plt.ylabel('Proportional variance ratio')\n",
    "    plt.xlabel('Ranked Principal Components')\n",
    "    plt.title(\"Scree Graph\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(cumulative_sum)\n",
    "    ax1.set_ylim([0,1.0])\n",
    "    ax1.set_xlabel('Number of Principal Components')\n",
    "    ax1.set_ylabel('Cumulative explained variance')\n",
    "    ax1.set_title('Elbow Plot')\n",
    "    plt.show()\n",
    "    \n",
    "    return evals_sorted, evectors_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68deb227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still our digits data!\n",
    "data = np.array(np.genfromtxt('data/optdigits/optdigits.tra', delimiter=',', dtype=int)) \n",
    "(data, y) = split(data, data.shape[1]-1)\n",
    "\n",
    "# What are we doing here??\n",
    "def sums(data):\n",
    "    y = np.array([np.sum(data[i]) for i in range(data.shape[0])])\n",
    "    return y\n",
    "\n",
    "def nonzeros(data):\n",
    "    y = np.array([(x.shape[0] - np.count_nonzero(x)) / x.shape[0] for x in data])\n",
    "    return y\n",
    "\n",
    "data = np.hstack((data, np.array([sums(data), nonzeros(data)]).T))\n",
    "print(\"data\")\n",
    "print(data.shape)\n",
    "print(uf.getSummaryStatistics(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9057e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now center the data, print summary statistics, and then fit a PCA\n",
    "# What do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd01b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now preprocess the data via minmax global and center, print summary statistics, and then fit a PCA\n",
    "# What do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40784a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now preprocess the data via minmax local and center, print summary statistics, and then fit a PCA\n",
    "# What do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now preprocess the data via zscoring, print summary statistics, and then fit a PCA\n",
    "# What do we observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e5284",
   "metadata": {},
   "source": [
    "Some interesting ties between QR decomposition, SVD and PCA:\n",
    "* https://python.quantecon.org/qr_decomp.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
