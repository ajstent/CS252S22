{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0e7ca52",
   "metadata": {},
   "source": [
    "# The One Goal For Today\n",
    "\n",
    "Understand how normalization first can lead to better or more efficient clustering and classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811722d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6243b190",
   "metadata": {},
   "source": [
    "# Load and Look at Your Data\n",
    "\n",
    "The data set we wil be analyzing is the dataset of car logos from https://github.com/GeneralBlockchain/vehicle-logos-dataset. I converted each logo to greyscale and downscaled them to a consistent size. I also converted the dependent variable (manufacturer name) to an int; it is the last column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8571c1f",
   "metadata": {},
   "source": [
    "First we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca359103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be our columns\n",
    "columns = [\"price\", \"year\", \"manufacturer\", \"model\", \"condition\", \"fuel\", \"odometer\", \"title_status\", \"transmission\"]\n",
    "# this will contain our converters\n",
    "colValues = {}\n",
    "\n",
    "# first we load our data as strings so we can define the converters\n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), skip_header=1, dtype=str, encoding='utf-8'))  \n",
    "\n",
    "# make a list of the unique values in each column of our data\n",
    "for colIndex in range(data.shape[1]):\n",
    "    colValues[colIndex] = np.unique(data[:, colIndex]).tolist()\n",
    "    print(colIndex, colValues[colIndex])\n",
    "\n",
    "# map values to their indices in the list of unique values\n",
    "def converter(x, colIndex):\n",
    "    return colValues[colIndex].index(x)\n",
    "    \n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), converters={3: lambda x: converter(x, 2), 4: lambda x: converter(x, 3), 5: lambda x: converter(x, 4), 7: lambda x: converter(x,5), 9: lambda x: converter(x, 7), 11: lambda x: converter(x, 8)}, skip_header=1, dtype=int, encoding='utf-8'))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68a5a5ca",
   "metadata": {},
   "source": [
    "Then we get summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0dcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStatistics(data):\n",
    "    print(\"min, max, mean, std per variable\")\n",
    "    return pd.DataFrame([data.min(axis=0), data.max(axis=0), data.mean(axis=0), data.std(axis=0)])\n",
    "\n",
    "def getShapeType(data):\n",
    "    print(\"shape\")\n",
    "    return (data.shape, data.dtype)\n",
    "\n",
    "print(getSummaryStatistics(data))\n",
    "print(getShapeType(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef7fcc85",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "\n",
    "If we are doing supervised machine learning, we split the data into train and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8431e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# split the data into train and test\n",
    "(train, test) = np.split(data, [int(len(data) / 10 * 8)])\n",
    "print(\"train, test: \", train.shape, test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "283ad7b8",
   "metadata": {},
   "source": [
    "Strip off the dependent variable (the labels, the classes). Let's go with trying to predict the car's **drive train**. That's the last variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[:, -1]\n",
    "x_train = train[:, 0:-1]\n",
    "y_test = test[:, -1]\n",
    "x_test = test[:, 0:-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0695d83",
   "metadata": {},
   "source": [
    "# Normalization Review\n",
    "\n",
    "Here we implement max-min global, max-min local, z-score and center. This code comes from day 20.\n",
    "\n",
    "This code you can use as a **tool**.\n",
    "\n",
    "**If you are using separate training and test data, you want to normalize to the mean (min, max, std) of the _training data_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa19fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, min, max, mean, std, method='center'):\n",
    "    if method == 'center':\n",
    "        return data - mean\n",
    "    elif method == 'max-min-global':\n",
    "        return (data - min) / (max - min)\n",
    "    elif method == 'max-min-local':\n",
    "        return (data - min) / (max - min)\n",
    "    elif method == 'zscore':\n",
    "        return (data - mean) / std\n",
    "    else:\n",
    "        raise Exception(\"I can't do \" + method)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5352ca4",
   "metadata": {},
   "source": [
    "Let's try it!\n",
    "\n",
    "**When you are doing supervised machine learning, you always want to normalize using statistics (mean, min, max) from your training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_g = np.min(x_train)\n",
    "max_g = np.max(x_train)\n",
    "min_l = np.min(x_train, axis=0)\n",
    "max_l = np.max(x_train, axis=0)\n",
    "mean = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)\n",
    "normalized_train = normalize(x_train, min_l, max_l, mean, std, method='max-min-local')\n",
    "# normalized_train = normalize(train, min_g, max_g, mean, std, method='center')\n",
    "# normalized_train = normalize(train, min_g, max_g, mean, std, method='max-min-global')\n",
    "# normalized_train = normalize(train, min_g, max_g, mean, std, method='zscore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad0b841e",
   "metadata": {},
   "source": [
    "# kNN Review\n",
    "\n",
    "The code below comes from day 24.\n",
    "\n",
    "You can use this code as a **tool**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy over euclidean distance from Friday\n",
    "def distance(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2))\n",
    "\n",
    "# \"fits\" a model to the data\n",
    "def fit_knn(data, labels, k):\n",
    "    assert len(data) == len(labels)\n",
    "    # \"store\" or return the model which is the combination of data, labels and k\n",
    "    # see predict_one_knn for what it should look like\n",
    "    return (data, labels, k)\n",
    "\n",
    "# predict the label for one datapoint\n",
    "def predict_one_knn(element, model):\n",
    "    training_data = model[0]\n",
    "    labels = model[1]\n",
    "    k = model[2]\n",
    "    # let's look up this argpartition thing\n",
    "    neighbors_by_distance = np.argpartition([distance(element, datapoint) for datapoint in training_data], k)\n",
    "    neighbor_labels = [labels[neighbors_by_distance[x]] for x in range(k)]\n",
    "    vals, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "    # print(\"neighbor labels by counts: \", vals, counts)\n",
    "    return vals[np.argwhere(counts == np.max(counts))][0,0]\n",
    "\n",
    "# predict the label for a set of data points\n",
    "def predict_knn(data, model):\n",
    "    return np.array([predict_one_knn(datapoint, model) for datapoint in data])\n",
    "    \n",
    "# calculate accuracy given actual labels y and predicted labels yhat\n",
    "def accuracy(y, yhat):\n",
    "    assert len(y) == len(yhat)\n",
    "    diffs = y == yhat\n",
    "    vals, counts = np.unique(diffs, return_counts=True)\n",
    "    return (counts[np.where(vals == True)] / (np.sum(counts)))[0]\n",
    "\n",
    "# score a model using test data\n",
    "def score(model, testing_data, test_labels):\n",
    "    predicted_labels = predict_knn(testing_data, model)\n",
    "    return accuracy(test_labels, predicted_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "064c2fbf",
   "metadata": {},
   "source": [
    "# Impact of normalization on kNN\n",
    "\n",
    "Fill in this table.\n",
    "1. Try all the types of normalization plus kNN classification. Use a reasonable value for $k$ in kNN classification, like 5.\n",
    "2. Try at least one type of normalization (centering!) plus PCA plus kNN classification. Use the same value of $k$ for kNN classification as you have so far. Pick a number of principal components that lets you keep at least 80% of the cumulative sum of variance.\n",
    "\n",
    "| Normalization | PCA (None or k) | kNN k | Accuracy | Time |\n",
    "| ------------- | --------------- | --------- | ---------------- | ---- |\n",
    "| None | None | ?? | | |\n",
    "| Centering | None | ?? | | |\n",
    "| Max-min global | None | ?? | | |\n",
    "| Max-min local | None | ?? | | |\n",
    "| Z-score | None | ?? | | |\n",
    "| ?? | ?? | ?? | | |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a45aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22199c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_g = np.min(x_train)\n",
    "max_g = np.max(x_train)\n",
    "min_l = np.min(x_train, axis=0)\n",
    "max_l = np.max(x_train, axis=0)\n",
    "mean = np.mean(x_train, axis=0)\n",
    "std = np.std(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21680af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = normalize(x_train, min_g, max_g, mean, std, method='center')\n",
    "model = fit_knn(normalized_train, y_train, k)\n",
    "normalized_test = normalize(x_test,  min_g, max_g, mean, std, method='center')\n",
    "score(model, normalized_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9546c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = normalize(x_train, min_g, max_g, mean, std, method='max-min-global')\n",
    "model = fit_knn(normalized_train, y_train, k)\n",
    "normalized_test = normalize(x_test,  min_g, max_g, mean, std, method='max-min-global')\n",
    "score(model, normalized_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58396e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = normalize(x_train, min_l, max_l, mean, std, method='max-min-local')\n",
    "model = fit_knn(normalized_train, y_train, k)\n",
    "normalized_test = normalize(x_test,  min_l, max_l, mean, std, method='max-min-local')\n",
    "score(model, normalized_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = normalize(x_train, min_g, max_g, mean, std, method='zscore')\n",
    "model = fit_knn(normalized_train, y_train, k)\n",
    "normalized_test = normalize(x_test,  min_g, max_g, mean, std, method='zscore')\n",
    "score(model, normalized_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "normalized_train = x_train\n",
    "model = fit_knn(normalized_train, y_train, k)\n",
    "normalized_test = x_test\n",
    "score(model, normalized_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c56b7e24",
   "metadata": {},
   "source": [
    "**Bonus**: Now think about PCA. If we had a dataset with 1000 independent variables (like our car logo data), what do you think might be the impact of PCA-first on accuracy, and on time?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
