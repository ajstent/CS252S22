{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0e7ca52",
   "metadata": {},
   "source": [
    "# Feature selection and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811722d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy.linalg as sp_la"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6243b190",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Today we will keep working with the set of Craigslist listings for used cars."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd3775ce",
   "metadata": {},
   "source": [
    "All of this section is *exactly the same* as Wednesday and Friday.\n",
    "\n",
    "First I make my converters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be our columns\n",
    "columns = [\"price\", \"year\", \"manufacturer\", \"model\", \"condition\", \"fuel\", \"odometer\", \"title_status\", \"transmission\"]\n",
    "# this will contain our converters\n",
    "colValues = {}\n",
    "\n",
    "# first we load our data as strings so we can define the converters\n",
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), skip_header=1, dtype=str, encoding='utf-8'))  \n",
    "\n",
    "# make a list of the unique values in each column of our data\n",
    "for colIndex in range(data.shape[1]):\n",
    "    colValues[colIndex] = np.unique(data[:, colIndex]).tolist()\n",
    "    print(colIndex, colValues[colIndex])\n",
    "\n",
    "# fix up some of these ones we know are ordered\n",
    "colValues[columns.index('condition')] = ['new', 'like new', 'excellent', 'good', 'fair', 'salvage']\n",
    "colValues[columns.index('title_status')] = ['clean', 'lien', 'rebuilt', 'salvage', 'parts only', 'missing']\n",
    "\n",
    "# map values to their indices in the list of unique values\n",
    "def converter(x, colIndex):\n",
    "    return colValues[colIndex].index(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8571c1f",
   "metadata": {},
   "source": [
    "Now we actually load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca359103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(np.genfromtxt('data/vehicles.csv', delimiter=',', usecols=(1,2,3,4,5,7,8,9,11), converters={3: lambda x: converter(x, 2), 4: lambda x: converter(x, 3), 5: lambda x: converter(x, 4), 7: lambda x: converter(x,5), 9: lambda x: converter(x, 7), 11: lambda x: converter(x, 8)}, skip_header=1, dtype=int, encoding='utf-8'))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68a5a5ca",
   "metadata": {},
   "source": [
    "Let's get some summary statistics and do a **pairplot** so we can see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0dcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStatistics(data):\n",
    "    print(\"min, max, mean, std per variable\")\n",
    "    return pd.DataFrame([data.min(axis=0), data.max(axis=0), data.mean(axis=0), data.std(axis=0)])\n",
    "\n",
    "def getShapeType(data):\n",
    "    print(\"shape\")\n",
    "    return (data.shape, data.dtype)\n",
    "\n",
    "print(getSummaryStatistics(data))\n",
    "print(getShapeType(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ceb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=columns)\n",
    "seaborn.pairplot(df, y_vars = columns[0], x_vars = columns[1:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fc62d6b",
   "metadata": {},
   "source": [
    "Let's calculate *correlations* between price and the other variables. (Remind me what correlation values vary between?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(columns)):\n",
    "    print(columns[i], np.corrcoef(data[:, 0], data[:, i], rowvar=True)[0,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad0b841e",
   "metadata": {},
   "source": [
    "# Which model is best?\n",
    "\n",
    "## Stepwise regression\n",
    "\n",
    "We can do **feature selection**. This is useful for dealing with data that has many variables (features). How do we know which ones to *use*?\n",
    "Here we do additive feature selection:\n",
    "* repeatedly add an independent variable, train, and report $R^2$\n",
    "\n",
    "For stepwise regression we use a modification of $R^2$, $${R^2}_{adj} = 1 - \\frac{(1-R^2)(N-1)}{N-k-1}$$\n",
    "where $N$ is the number of variables, and $k$ is the number of variables in $A$.\n",
    "\n",
    "Stepwise regression works like this:\n",
    "\n",
    "1. Initialize $A$ to be just the leading column of 1s (because we know we will have an intercept).\n",
    "\n",
    "2. Then while the improvements in ${R^2}_{adj}$ are > 0 and there remain independent variables not yet added:\n",
    "  * calculate a regression using $A$ and each variable not yet in $A$, and \n",
    "  * add the one with the highest ${R^2}_{adj}$ to $A$.\n",
    "\n",
    "We could also do a variant of additive feature selection using the correlations:\n",
    "* sort independent variables by size of correlation (positive or negative!) with the dependent variable\n",
    "* repeatedly add the independent variable with the next biggest correlation; if it leads to higher $R^2$, keep it; if it doesn't, drop it again\n",
    "\n",
    "And we could also go from most features to fewest:\n",
    "* start with a model fit using *all* independent variables\n",
    "* repeatedly take an independent variable out; if the resulting model has higher $R^2$, leave that variable out going forward\n",
    "\n",
    "There are many other options for feature selection!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c2ea780",
   "metadata": {},
   "source": [
    "In this code block, I calculate the *powerset* of all the independent variables. Then, for each subset of the independent variables I train a model and calculate MSSE (on the training data) and $R^2$ (on the test data). Then, I report the ten worst and ten best performing sets of independent variables by MSSE and by $R^2$.\n",
    "\n",
    "Note:\n",
    "* sometimes models with fewer variables work better than models with more\n",
    "* sometimes a model may fit the training data better but the test data worse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccdae3d3",
   "metadata": {},
   "source": [
    "### First, split our data\n",
    "\n",
    "Let's split our data into **train** and **test**. Let's make sure and sort by time first, because we don't want to let the future predict the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[:, 1].argsort()]\n",
    "print(getSummaryStatistics(data))\n",
    "print(getShapeType(data))\n",
    "\n",
    "(train, test) = np.split(data, [int(len(data) / 10 * 8)])\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "064c2fbf",
   "metadata": {},
   "source": [
    "This chunk of code below we copied over verbatim from Monday's notebook in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22199c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x a matrix of multiple independent variables\n",
    "# poly -> polys, a matrix of multiple polynomial degrees for each column in x in order\n",
    "def makePoly(x, polys):\n",
    "    # make an empty array of size A\n",
    "    A = np.zeros([x.shape[0], np.sum(polys)+1])\n",
    "    # left most column of 1s for the intercept\n",
    "    # notice this is also a third way to get that leading column of ones!\n",
    "    A[:, 0] = np.squeeze(x[:, 0]**0)\n",
    "    k = 1\n",
    "    # for each variable\n",
    "    for (j, poly) in enumerate(polys):\n",
    "        # for up to and including! poly\n",
    "        for i in range(1, poly+1):\n",
    "            A[:, k] = np.squeeze(x[:, j]**i)\n",
    "            k += 1\n",
    "    return A\n",
    "\n",
    "def fit(data, independent, dependent, polys):\n",
    "    # This is our independent variable, just one for now\n",
    "    x = data[np.ix_(np.arange(data.shape[0]), independent)]\n",
    "\n",
    "    # We add the polynomials, and a column of 1s for the intercept\n",
    "    A = makePoly(x, polys)\n",
    "\n",
    "    # This is the dependent variable \n",
    "    y = data[:, dependent]\n",
    "\n",
    "    # This is the regression coefficients that were fit, plus some other results\n",
    "    # We use _ when we don't want to remember something a function returns\n",
    "    c, _, _, _ = sp_la.lstsq(A, y)\n",
    "    return c\n",
    "\n",
    "def predict(data, independent, polys, c):\n",
    "    # These are our independent variable(s)\n",
    "    x = data[np.ix_(np.arange(data.shape[0]), independent)]\n",
    "\n",
    "    # We add the polynomials, and a column of 1s for the intercept\n",
    "    A = makePoly(x, polys)\n",
    "\n",
    "    return np.dot(A, c)\n",
    "\n",
    "def rsquared(y, yhat):\n",
    "    if len(y) != len(yhat):\n",
    "        print(\"Need y and yhat to be the same length!\")\n",
    "        return 0\n",
    "    return 1 - (((y - yhat)**2).sum() / ((y - y.mean())**2).sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06d2cb29",
   "metadata": {},
   "source": [
    "This code is new for today. We updated it a little based on the code we copied over from Monday's class. Also, after class I added logging with weights and biases.\n",
    "\n",
    "For this to work now you need a (free!) weights and biases account. Get one from https://wandb.ai. Then copy the API key. On the terminal, type \"wandb login\". Paste in your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "import wandb\n",
    "\n",
    "def powerset(variables):\n",
    "    return chain.from_iterable(combinations(variables, r) for r in range(len(variables)+1))\n",
    "\n",
    "def msse(y, yhat):\n",
    "    r = (np.square(y - yhat)).mean()\n",
    "    return r\n",
    "\n",
    "res = {}\n",
    "for variableset in powerset(range(1, train.shape[1])):\n",
    "    if len(variableset) > 0:\n",
    "        name = '+'.join([str(x) for x in variableset])\n",
    "        # start a new wandb run to track this run\n",
    "        wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"cars-regression\",\n",
    "            name=f\"experiment_{name}\",\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "                \"architecture\": \"regression\",\n",
    "                \"dataset\": \"hyundaikia\",\n",
    "                \"split\": 20,\n",
    "                \"features\": variableset\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # fit the multiple linear regression\n",
    "        polys = [1 for x in range(len(variableset))]\n",
    "        c = fit(train, list(variableset), 0, polys)\n",
    "        # calculate MSSE and R^2\n",
    "        res[variableset] = (msse(train[:, 0], predict(train, variableset, polys, c)), \n",
    "                            rsquared(test[:, 0], predict(test, variableset, polys, c)))\n",
    "        wandb.log({\"msse\":res[variableset][0], \"rsquared\": res[variableset][1]})\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by R^2\n",
    "byrsquared = sorted(res.items(), key=lambda item: item[1][1])\n",
    "print(\"Worst R^2\")\n",
    "for i in range(10):\n",
    "    print([columns[x] for x in byrsquared[i][0]], byrsquared[i][1])\n",
    "print(\"Best R^2\")\n",
    "for i in range(1, 11):\n",
    "    print([columns[x] for x in byrsquared[-i][0]], byrsquared[-i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d462680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by MSSE\n",
    "bymsse = sorted(res.items(), key=lambda item: item[1][0])\n",
    "print(\"Worst MSSE\")\n",
    "for i in range(1, 11):\n",
    "    print([columns[x] for x in bymsse[-i][0]], bymsse[-i][1])\n",
    "print(\"Best MSSE\")\n",
    "for i in range(10):\n",
    "    print([columns[x] for x in bymsse[i][0]], bymsse[i][1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a4e9ea9",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "When we fiddle with our independent variables to make the models better, we call this **feature engineering**. For example, adding the square of age.\n",
    "\n",
    "So we have two ways to experiment with a single modeling approach:\n",
    "* feature selection\n",
    "* feature engineering\n",
    "\n",
    "I would not call data transformations (like max-min normalization) feature engineering, since you should do them before you start modeling, but you might choose to consider them feature engineering.\n",
    "\n",
    "In real-world projects, you can spend a huge amount of time doing feature selection and feature engineering. It can get overwhelming quickly! Keep good track of your work through either an experiment logbook, or experiment tracking software like [weights and biases](https://wandb.ai/site)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d21c4634",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "1. Find some data!\n",
    "2. Load and look at your data\n",
    "  * thing1\n",
    "  * thing2\n",
    "3. Consider cleaning, transforming and/or normalizing your data\n",
    "  * thing1\n",
    "  * thing2\n",
    "  * thing3\n",
    "  * thing4\n",
    "4. Look at your data some more and consider feature selection, feature engineering, dimensionality reduction\n",
    "  * *correlations*\n",
    "  * *covariance matrix*\n",
    "  * *PCA*\n",
    "5. Model\n",
    "  * thing1 (in three variations!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2d1b0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
