{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16737ff",
   "metadata": {},
   "source": [
    "# RBF Networks with Iris Data: Soup to Nuts (and Beyond!)\n",
    "\n",
    "\n",
    "Review from Monday:\n",
    "\n",
    "Training a RBF consists of:\n",
    "* Finding prototypes\n",
    "* Selecting the activation function for the hidden nodes\n",
    "* Selecting the activation function for the output nodes\n",
    "* Setting the weights for the edges and biases\n",
    "\n",
    "We use:\n",
    "* kmeans to find prototypes\n",
    "* linear regression to fit the edge weights from the hidden layer to the output layer\n",
    "\n",
    "We can use RBF networks for:\n",
    "* classification - by taking the absolute rounded value of the argmax of the outputs from the output layer (one output node per class)\n",
    "* regression - by taking the output from the one node in the output layer\n",
    "\n",
    "Today we will talk about:\n",
    "* Visualizing neural networks and the activations of a neural network layer\n",
    "* Hebbian learning - using RBF networks for clustering\n",
    "\n",
    "As a sanity check when building a model, I suggest you *always print the shape at each step*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba73819",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca93b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def type_converter(x):\n",
    "    values = ['setosa', 'versicolor', 'virginica']\n",
    "    return float(values.index(x))\n",
    "\n",
    "def inverse_type_converter(x):\n",
    "    values = ['setosa', 'versicolor', 'virginica']\n",
    "    return values[x]\n",
    "\n",
    "\n",
    "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "iris = np.array(np.genfromtxt('data/iris.csv', delimiter=',', converters={4: type_converter}, skip_header=2, dtype=float, encoding='utf-8'))\n",
    "print(iris.shape, iris.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6593f0",
   "metadata": {},
   "source": [
    "# Look at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_statistics(data):\n",
    "    \"Get the max, min, mean, var for each variable in the data.\"\n",
    "    return pd.DataFrame(np.array([data.max(axis=0), data.min(axis=0), data.mean(axis=0), data.var(axis=0)]))\n",
    "\n",
    "print(get_summary_statistics(iris))\n",
    "\n",
    "df = pd.DataFrame(iris, columns=columns)\n",
    "sns.pairplot(df, y_vars = [\"class\"], kind = \"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d109eb",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "\n",
    "Nothing to see here for the iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f2800",
   "metadata": {},
   "source": [
    "## Split the Data \n",
    "\n",
    "Into train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3cf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why are we doing this?\n",
    "np.random.shuffle(iris)\n",
    "\n",
    "train_data, dev_data, test_data = np.split(iris, [int(.8 * len(iris)), int(.9 * len(iris))])\n",
    "print(\"training data\", \"\\n\", train_data.shape, \"\\n\", get_summary_statistics(train_data))\n",
    "print(\"dev data\", \"\\n\", dev_data.shape, \"\\n\", get_summary_statistics(dev_data))\n",
    "print(\"test data\", \"\\n\", test_data.shape, \"\\n\", get_summary_statistics(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e0bc8",
   "metadata": {},
   "source": [
    "## Split off the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08814765",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_var = 4\n",
    "ind_vars = list(range(train_data.shape[1]))\n",
    "ind_vars.pop(dep_var)\n",
    "dep_name = columns.pop(dep_var)\n",
    "print(dep_var, ind_vars)\n",
    "\n",
    "train_data, train_y = train_data[np.ix_(np.arange(train_data.shape[0]), ind_vars)], train_data[:, dep_var]\n",
    "dev_data, dev_y = dev_data[np.ix_(np.arange(dev_data.shape[0]), ind_vars)], dev_data[:, dep_var]\n",
    "test_data, test_y = test_data[np.ix_(np.arange(test_data.shape[0]), ind_vars)], test_data[:, dep_var]\n",
    "print(\"training data\", \"\\n\", train_data.shape, \"\\n\", get_summary_statistics(train_data))\n",
    "print(\"dev data\", \"\\n\", dev_data.shape, \"\\n\", get_summary_statistics(dev_data))\n",
    "print(\"test data\", \"\\n\", test_data.shape, \"\\n\", get_summary_statistics(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8149c",
   "metadata": {},
   "source": [
    "## Consider Transforming/Normalizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f2beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogenizeData(data):\n",
    "    return np.append(data, np.array([np.ones(data.shape[0], dtype=float)]).T, axis=1)\n",
    "   \n",
    "def zScore(data, translateTransform=None, scaleTransform=None):\n",
    "    \"z score.\"\n",
    "    homogenizedData = np.append(data, np.array([np.ones(data.shape[0], dtype=float)]).T, axis=1)\n",
    "    if translateTransform is None:\n",
    "        translateTransform = np.eye(homogenizedData.shape[1])\n",
    "        for i in range(homogenizedData.shape[1]):\n",
    "            translateTransform[i, homogenizedData.shape[1]-1] = -homogenizedData[:, i].mean()\n",
    "    if scaleTransform is None:\n",
    "        diagonal = [1 / homogenizedData[:, i].std() if homogenizedData[:, i].std() != 0 else 1 for i in range(homogenizedData.shape[1])]\n",
    "        scaleTransform = np.eye(homogenizedData.shape[1], dtype=float) * diagonal\n",
    "    data = (scaleTransform@translateTransform@homogenizedData.T).T\n",
    "    return translateTransform, scaleTransform, data[:, :data.shape[1]-1]\n",
    "\n",
    "translateTransform, scaleTransform, train_data_transformed = zScore(train_data)\n",
    "print(\"training data\", \"\\n\", train_data_transformed.shape, \"\\n\", get_summary_statistics(train_data_transformed))\n",
    "\n",
    "_, _, dev_data_transformed = zScore(dev_data, translateTransform=translateTransform, scaleTransform=scaleTransform)\n",
    "print(\"dev data\", \"\\n\", dev_data_transformed.shape, \"\\n\", get_summary_statistics(dev_data_transformed))\n",
    "\n",
    "_, _, test_data_transformed = zScore(test_data, translateTransform=translateTransform, scaleTransform=scaleTransform)\n",
    "print(\"test data\", \"\\n\", test_data_transformed.shape, \"\\n\", get_summary_statistics(test_data_transformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc9e15",
   "metadata": {},
   "source": [
    "## Consider Dimensionality Reduction\n",
    "\n",
    "Nothing to see here for the iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e59f1d",
   "metadata": {},
   "source": [
    "# Find Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ee397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertia_by_k = []\n",
    "\n",
    "for k in range(2, 17):\n",
    "    km = KMeans(n_clusters=k, random_state=0).fit(train_data)\n",
    "    inertia_by_k.append([k, km.inertia_])\n",
    "\n",
    "inertia_by_k = np.array(inertia_by_k)\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(inertia_by_k[:, 0], inertia_by_k[:, 1])\n",
    "ax1.set_xlabel('k')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21240a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 11\n",
    "\n",
    "km = KMeans(n_clusters=k, random_state=0).fit(train_data)\n",
    "\n",
    "print(km.cluster_centers_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a4c3e",
   "metadata": {},
   "source": [
    "# Define the Activation Function for the Hidden Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to implement a _stupid activation function_ so that you can implement the right one yourselves for project 7\n",
    "def calculate_activations(data, centroids):\n",
    "    \"I repeat, do not use this activation function directly. This one is exp(-distance / 3); yours is exp(-distance^2 / (2*radius + epsilon))\"\n",
    "    # You can easily fiddle with this numerator to make it calculate the square of the distance\n",
    "    numerator = -np.linalg.norm(data-centroids[:,np.newaxis], axis = 2).T\n",
    "    # The construction of your denominator will be a little more complex than this; the diagonals will be centroid/prototype-specific\n",
    "    denominator = np.eye(centroids.shape[0], dtype=float) * 1/3\n",
    "    print(numerator.shape, denominator.shape)\n",
    "    return np.exp((denominator@numerator.T).T)\n",
    "\n",
    "train_calcs = calculate_activations(train_data_transformed, km.cluster_centers_)\n",
    "print(train_calcs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42151c3",
   "metadata": {},
   "source": [
    "# Define the Activation Function for the Output Nodes\n",
    "\n",
    "In a RBF network, this will be a straight linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca43f5",
   "metadata": {},
   "source": [
    "# Calculate the Values for the Weights on the Edges to the Output Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847597d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fit_classification(A, y):\n",
    "    y = []\n",
    "    for value in np.unique(train_y):\n",
    "        y.append([1 if x == value else 0 for x in train_y])\n",
    "    y = np.vstack(y).T\n",
    "    print(\"y\", \"\\n\", y.shape, \"\\n\", np.vstack((y.T, train_y)).T)\n",
    "    reg = LinearRegression().fit(A, y)\n",
    "    print(\"coefficients\", \"\\n\", reg.coef_.shape)\n",
    "    print(\"intercepts\", \"\\n\", reg.intercept_.shape)\n",
    "    return reg\n",
    "\n",
    "def fit_regression(A, y):\n",
    "    print(\"y\", \"\\n\", y.shape, \"\\n\", np.vstack((y.T, train_y)).T)\n",
    "    reg = LinearRegression().fit(A, y)\n",
    "    print(\"coefficients\", \"\\n\", reg.coef_.shape)\n",
    "    print(\"intercepts\", \"\\n\", reg.intercept_.shape)\n",
    "    return reg\n",
    "\n",
    "if dep_var == 4:\n",
    "    reg = fit_classification(train_calcs, train_y)\n",
    "else:\n",
    "    reg = fit_regression(train_calcs, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68e75f",
   "metadata": {},
   "source": [
    "# Test and Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_calcs = calculate_activations(dev_data_transformed, km.cluster_centers_)\n",
    "print(dev_calcs.shape)\n",
    "\n",
    "dev_yhat = reg.predict(dev_calcs)\n",
    "print(dev_yhat.shape)\n",
    "print(dev_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prediction(input_predictions):\n",
    "    return np.abs(np.round(np.argmax(input_predictions)))\n",
    "\n",
    "if dep_var == 4:\n",
    "    dev_yhat = [final_prediction(dev_yhat[i]) for i in range(dev_yhat.shape[0])]\n",
    "print(dev_yhat)\n",
    "print(dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0968e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, yhat):\n",
    "    return np.sum([1 if y[i]==yhat[i] else 0 for i in range(len(y))]) / len(y)\n",
    "\n",
    "def rsquared(y, yhat):\n",
    "    return 1 - (((y - yhat)**2).sum() / ((y - y.mean())**2).sum())\n",
    " \n",
    "if dep_var == 4:\n",
    "    print(accuracy(dev_y, dev_yhat))\n",
    "else:\n",
    "    print(rsquared(dev_y, dev_yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee7de5",
   "metadata": {},
   "source": [
    "# Visualizing Neural Networks\n",
    "\n",
    "## Visualize the network itself\n",
    "\n",
    "For this, there are many options. We will use *graphviz*. To install graphviz:\n",
    "* First, download and stall the appropriate executable package from https://www.graphviz.org/download/ (for Windows or Mac)\n",
    "* Then, do pip install graphviz or (or conda install graphviz) to install the python bindings\n",
    "You may have to restart your computer after this for the computer to find the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Let's make a graph\n",
    "network = Digraph(format='png', comment='RBF network')\n",
    "# Let's make it draw from left to right\n",
    "network.attr(rankdir='LR')\n",
    "# Let's make it draw straight lines for edges rather than curves\n",
    "#network.attr(splines='line')\n",
    "\n",
    "# Add input nodes\n",
    "with network.subgraph(name='input', node_attr={'shape': 'circle', 'color': 'blue'}) as input:\n",
    "    for i in range(len(columns)):\n",
    "        input.node('i'+str(i), columns[i])\n",
    "\n",
    "# Add hidden layer nodes\n",
    "with network.subgraph(name='hidden', node_attr={'shape': 'circle', 'color': 'black'}) as hidden:\n",
    "    for i in range(k):\n",
    "        hidden.node('h'+str(i))\n",
    "\n",
    "# Add bias nodes\n",
    "o_len = 1\n",
    "if len(reg.coef_.shape) > 1:\n",
    "    o_len = reg.coef_.shape[0]\n",
    "with network.subgraph(name='bias', node_attr={'shape': 'square', 'color': 'black'}) as bias:\n",
    "    for i in range(o_len):\n",
    "        bias.node('b'+str(i))\n",
    "\n",
    "# Add output layer nodes\n",
    "with network.subgraph(name='output', node_attr={'shape': 'circle', 'color': 'red'}) as output:\n",
    "    for i in range(o_len):\n",
    "        output.node('o'+ str(i), inverse_type_converter(i))\n",
    "\n",
    "\n",
    "# Add edges from input to hidden layer\n",
    "for i in range(len(columns)):\n",
    "    for j in range(k):\n",
    "        network.edge('i'+ str(i), 'h'+str(j))\n",
    "\n",
    "# Add edges from hidden to output layer\n",
    "for i in range(k):\n",
    "    for j in range(o_len):\n",
    "        network.edge('h'+str(i), 'o'+str(j), label=str(reg.coef_[j, i]))\n",
    "\n",
    "# Add edges from bias to output layer\n",
    "for i in range(o_len):\n",
    "    network.edge('b'+str(i), 'o'+str(i), label=str(reg.intercept_[i]))\n",
    "\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c02e6",
   "metadata": {},
   "source": [
    "# Visualizing the *activation* of one or more layers of the network for an input\n",
    "\n",
    "One way to do this is with a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4257a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dev_calcs , yticklabels=[inverse_type_converter(int(x)) for x in dev_y])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
