{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf750918",
   "metadata": {},
   "source": [
    "# Dealing with BIG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as sp_la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c1c43",
   "metadata": {},
   "source": [
    "One more day with Boston housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f999d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(np.genfromtxt('data/boston.csv', delimiter=',', skip_header=2, dtype=float, encoding='utf-8'))  \n",
    "split = np.array_split(data, 10)\n",
    "train = np.vstack((split[0], split[1], split[3], split[4], split[5], split[6], split[7], split[8], split[9]))\n",
    "test = split[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ef39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "databig = np.vstack([data]*1000)\n",
    "print(databig.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f4b3d",
   "metadata": {},
   "source": [
    "(From day 15, which in turn was from day 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be204365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSummaryStatistics(data):\n",
    "    return np.array([data.max(axis=0), data.min(axis=0), data.mean(axis=0, dtype=int)])\n",
    "\n",
    "def getShapeType(data):\n",
    "    return (data.shape, data.dtype)\n",
    "\n",
    "def fitlstsq(data, independent, dependent):\n",
    "    # These are our independent variable(s)\n",
    "    x = data[np.ix_(np.arange(data.shape[0]), independent)]\n",
    "    # We add a column of 1s for the intercept\n",
    "    A = np.hstack((np.array([np.ones(x.shape[0])]).T, x))\n",
    "    # This is the dependent variable \n",
    "    y = data[:, dependent]\n",
    "    # This is the regression coefficients that were fit, plus some other results\n",
    "    c, res, _, _ = sp_la.lstsq(A, y)\n",
    "    return c\n",
    "\n",
    "def fitnorm(data, independent, dependent):\n",
    "    # These are our independent variable(s)\n",
    "    x = data[np.ix_(np.arange(data.shape[0]), independent)]\n",
    "    # We add a column of 1s for the intercept\n",
    "    A = np.hstack((np.array([np.ones(x.shape[0])]).T, x))\n",
    "    # This is the dependent variable \n",
    "    y = data[:, dependent]\n",
    "    # This is the regression coefficients that were fit, plus some other results\n",
    "    c = sp_la.inv(A.T.dot(A)).dot(A.T).dot(y)\n",
    "    return c\n",
    "\n",
    "def fitqr(data, independent, dependent):\n",
    "    # These are our independent variable(s)\n",
    "    x = data[np.ix_(np.arange(data.shape[0]), independent)]\n",
    "    # We add a column of 1s for the intercept\n",
    "    A = np.hstack((np.array([np.ones(x.shape[0])]).T, x))\n",
    "    # This is the dependent variable \n",
    "    y = data[:, dependent]\n",
    "    # This is the regression coefficients that were fit, plus some other results\n",
    "    Q, R = sp_la.qr(A)\n",
    "    print(A.shape)\n",
    "    print(Q.shape)\n",
    "    print(R.shape)\n",
    "    c = sp_la.solve_triangular(R, Q.T.dot(y))\n",
    "    return c\n",
    "\n",
    "def predict(data, independent, c):\n",
    "    # These are our independent variable(s)\n",
    "    x = data[np.ix_(np.arange(data.shape[0]), independent)]\n",
    "    # We add a column of 1s for the intercept\n",
    "    A = np.hstack((np.array([np.ones(x.shape[0])]).T, x))\n",
    "    return np.dot(A, c)\n",
    "\n",
    "def plotxyyhat(x, y, c):\n",
    "    plt.plot(x, y, 'o', label='data')\n",
    "    xCurve = np.linspace(np.min(x), np.max(x))\n",
    "    yCurve = c[0]\n",
    "    for i in range(1, len(c)):\n",
    "        yCurve += c[i]*(xCurve**i)\n",
    "    plt.plot(xCurve, yCurve, label='least squares fit, linear')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(framealpha=1, shadow=True)\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d80445",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fitlstsq(data, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e91567",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fitnorm(databig, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fitqr(databig, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf073e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20af97f1",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Let's talk about one more way to calculate a  regression. We already know three *analytical* methods. What are they?\n",
    "\n",
    "This method is an *approximation* method. So it's not guaranteed to give us a correct answer. But it may be handy when the data is so gnarly that the analytical methods fail, or when the data is large (> 10000 data points, > 1000 variables). \n",
    "\n",
    "For this discussion I'm going to refer to linear regression with one independent variable, but the method is general.\n",
    "\n",
    "All the back on day 12 we defined the MSSE as the *loss function* we wanted to *minimize* for regression. \n",
    "$$MSSE = 1 / N \\sum_{i=1}^N (r_i)^2 = 1/N \\sum_{i=1}^N (y_i - \\hat{y}_i)^2$$\n",
    "This method is an iterative step by step method. We take a step, figure out if we are going towards the *minimum loss* and if so we take another step. \n",
    "\n",
    "* At first we pick random values for $m$ and $b$ (our slope and intercept).\n",
    "* At each step we:\n",
    "  1. calculate the partial derivative of $MSSE$ with respect to $m$ and to $b$, and plug in $x$, $y$, $m$ and $b$.\n",
    "    * partial derivative of $MSSE$ ($p$) with respect to $m$: $$\\frac{\\partial p}{\\partial m} = 1/N \\sum_{i=1}^N \\frac{\\partial }{\\partial m} (y_i - \\hat{y}_i)^2 = 1/N \\sum_{i=1}^N \\frac{\\partial }{\\partial m} (y_i - (m x_i + b))^2 = 1 / N \\sum_{i=1}^N -2 x_i (y_i - (m x_i + b)) = -2 / N \\sum_{i=1}^N x_i (y_i - \\hat{y}_i)$$\n",
    "    * partial derivative of $MSSE$ ($p$) with respect to $b$: $$\\frac{\\partial p}{\\partial b} = 1/N \\sum_{i=1}^N \\frac{\\partial }{\\partial b} (y_i - \\hat{y}_i)^2 = 1/N \\sum_{i=1}^N \\frac{\\partial }{\\partial b} (y_i - (m x_i + b))^2 = 1 / N \\sum_{i=1}^N -2 (y_i - (m x_i + b)) = -2 / N \\sum_{i=1}^N (y_i - \\hat{y}_i)$$\n",
    "  2. update $m$ and $b$ as follows:\n",
    "    * $m = m - lr * \\frac{\\partial p}{\\partial m}$\n",
    "    * $b = b - lr * \\frac{\\partial p}{\\partial b}$\n",
    "    where $lr$ is a *learning rate* set, hopefully, big enough that we don't have to step forever, and small enough that we don't overshoot our goal (the minimum loss)\n",
    "* We stop after a preset number of steps (*epochs*) or after the change in loss stops getting smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "def gradient_descent(data, independent, dependent, lr, epochs):\n",
    "    # initialize m and b\n",
    "    rng = default_rng()\n",
    "    c = rng.standard_normal(2)\n",
    "    # set n, x and y for readability of the method\n",
    "    n = data.shape[0]\n",
    "    x = data[np.ix_(np.arange(data.shape[0]), independent)]\n",
    "    y = data[:, dependent]\n",
    "    A = np.hstack((np.array([np.ones(x.shape[0])]).T, x))\n",
    "    for i in range(epochs):\n",
    "        yhat = np.dot(A, c)\n",
    "        # how are we doing on MSSE?\n",
    "        print((1/n) * np.sum(y - yhat)**2)\n",
    "        if ((1/n) * np.sum(y - yhat)**2) < 0.00001:\n",
    "            return c\n",
    "        # fill in the partial derivatives\n",
    "        dpdm = (-2/n) * np.sum(x * (y - yhat))\n",
    "        dpdb = (-2/n) * np.sum(y - yhat)\n",
    "        # update c\n",
    "        c = c - np.array([lr * dpdm, lr * dpdb])\n",
    "#        plotxyyhat(x, data[:, dependent], c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29099377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsquared(y, yhat):\n",
    "    if len(y) != len(yhat):\n",
    "        print(\"Need y and yhat to be the same length!\")\n",
    "        return 0\n",
    "    return 1 - (((y - yhat)**2).sum() / ((y - y.mean())**2).sum())\n",
    "\n",
    "capprox = gradient_descent(data, [7], 12, 0.0001, 1000)\n",
    "cexact = fit(data, [7], 12)\n",
    "\n",
    "# And how does it look on the test data that we held out?\n",
    "print(capprox, cexact)\n",
    "print(rsquared(test[:, 12], predict(test, [7], capprox)))\n",
    "print(rsquared(test[:, 12], predict(test, [7], cexact)))\n",
    "\n",
    "# Homework next week: boston x 50, lstsq, normal, qr, gd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ab8a9",
   "metadata": {},
   "source": [
    "## Stepwise Regression\n",
    "\n",
    "Let's talk about dealing with data that has many variables (features). How do we know which ones to *use*?\n",
    "\n",
    "One way to go about this is stepwise regression.\n",
    "\n",
    "For stepwise regression we use a modification of $R^2$, $${R^2}_{adj} = 1 - \\frac{(1-R^2)(N-1)}{N-k-1}$$\n",
    "where $N$ is the number of variables, and $k$ is the number of variables in $A$.\n",
    "\n",
    "Stepwise regression works like this:\n",
    "\n",
    "1. Initialize $A$ to be just the leading column of 1s (because we know we will have an intercept).\n",
    "\n",
    "2. Then while the improvements in ${R^2}_{adj}$ are > 0 and there remain independent variables not yet added:\n",
    "  * calculate a regression using $A$ and each variable not yet in $A$, and \n",
    "  * add the one with the highest ${R^2}_{adj}$ to $A$.\n",
    "\n",
    "Let's try this on the Boston housing data!\n",
    "\n",
    "For this exercise, I'm going to use linear regression only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0a728",
   "metadata": {},
   "source": [
    "We define our adjusted $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsquared(y, yhat):\n",
    "    if len(y) != len(yhat):\n",
    "        print(\"Need y and yhat to be the same length!\")\n",
    "        return 0\n",
    "    return 1 - (((y - yhat)**2).sum() / ((y - y.mean())**2).sum())\n",
    "\n",
    "def rsquared_adj(y, yhat, n, k):\n",
    "    return 1 - (((1-rsquared(y, yhat))*(n-1)) / (n-k-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7610db9",
   "metadata": {},
   "source": [
    "We define stepwisefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwisefit(data, dependent):\n",
    "    # I'm just going to store the indices of the independent variables we are keeping\n",
    "    colsinA  = []\n",
    "    while(len(colsinA) < data.shape[1]):\n",
    "        print(colsinA)\n",
    "        best_independent = -1\n",
    "        # extension: set best_rsa to be the rsa of A \n",
    "        best_rsa = 0.0\n",
    "        for independent in range(data.shape[1]):\n",
    "            if independent != dependent and independent not in colsinA:\n",
    "                c = fit(data, colsinA + [independent], dependent)\n",
    "                yhat = predict(data, colsinA + [independent], c)\n",
    "                rsa = rsquared_adj(data[:, dependent], yhat, data.shape[1], len(colsinA))\n",
    "                print(rsa, independent, best_rsa, best_independent)\n",
    "                if rsa > best_rsa:\n",
    "                    best_rsa = rsa\n",
    "                    best_independent = independent\n",
    "        if rsa <= 0:\n",
    "            return colsinA\n",
    "        if best_independent > -1:\n",
    "            colsinA.append(best_independent)\n",
    "    return colsinA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2f394",
   "metadata": {},
   "source": [
    "We try it on the Boston data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsinA = stepwisefit(data, 12)\n",
    "print(colsinA)\n",
    "\n",
    "# And how does it look on the test data that we held out?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881d6df",
   "metadata": {},
   "source": [
    "This is not the only way to do stepwise regression: we could start with all the variables, and then iteratively remove one; or we could add *and* remove in each step (see https://en.wikipedia.org/wiki/Stepwise_regression).\n",
    "\n",
    "This is also not the only way to figure out whether variables should be included in a regression. You might have knowledge ahead of time. If you don't:\n",
    "* You might calculate correlations\n",
    "* You might look at a pair plot\n",
    "\n",
    "## The curse of dimensionality\n",
    "\n",
    "Of course when data is very multidimensional, everything becomes slower and more complicated:\n",
    "* How can you look at your data? There's too much there!\n",
    "* How can you normalize your data? Without being able to look at it, what normalizations would you know to apply?\n",
    "* How can you fit a regression to your data? Which variables would you choose?\n",
    "\n",
    "In fact, data in a high dimensional space just is different:\n",
    "* there are more extreme values. In a 2 dimensional unit square, the probability that a random point is within 0.001 of the border is 0.004, but in a 10000 dimensional unit hypercube, the probability is > 0.999.\n",
    "* the distances between points are bigger. In a 2 dimensional unit square, the average distance between two points is about 0.5. In a 3 dimensional unit cube, it's 0.66. But in a 10000 dimensional unit hypercube, it's about 408.25.\n",
    "\n",
    "Here's the thing though: generally, only a small fraction of variables in a data set suffice to model the whole. So if we can identify them, we can *project* our data to a much smaller dimensional space and analyze the data in that space instead. The technique we will use for that is *principal component analysis* (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1ff18",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
